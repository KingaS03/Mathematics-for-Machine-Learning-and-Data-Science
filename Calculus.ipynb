{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/KingaS03/Introduction-to-Python-2020-June/master)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KingaS03/Introduction-to-Python-2020-June)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculus\n",
    "\n",
    "Agenda\n",
    "- differentiation of univariate functions\n",
    "- rules of differentiation\n",
    "- differentiation of multivariate functions (the Jacobian, the Hessian)\n",
    "- chain rule for univariate and multivariate functions \n",
    "- the Taylor approximation\n",
    "- the Newton-Raphson method\n",
    "- gradient descent method\n",
    "- backpropagation\n",
    "\n",
    "\n",
    "## 3.1. Motivation\n",
    "Find the optimal value of the model parameters of a neuronal network.\n",
    "\n",
    "## 3.2. Functions\n",
    "A function $f:A \\to B$ associates to each element of the set $A$ an element of the set $B$.\n",
    "\n",
    "For our future context $A = \\mathbb{R}^n$ and $B = \\mathbb{R}^m$ for some natural numbers $n$ and $m$.\n",
    "\n",
    "### 3.2.1. Differentiation of a univariate function\n",
    "----------------------------\n",
    "**Definition of the derivative of a univariate function**<br>\n",
    "\n",
    "For a function $f:\\mathbb{R} \\to \\mathbb{R}$ we would like to characterise its local linear behavior. Therefore we take two points $x$ and $x+\\Delta x$ and their corresponding values $f(x)$ and $f(x+\\Delta x)$. We are connecting these points by a line and we will calculate the gradient of this line\n",
    "\n",
    "$$m = \\frac{\\Delta f}{\\Delta x} = \\frac{f(x+\\Delta x)-f(x)}{(x+\\Delta x) - x} = \\frac{f(x+\\Delta x)-f(x)}{\\Delta x}$$\n",
    "\n",
    "Now we are going to take smaller and smaller values for the increment $\\Delta x$. We define the derivative of $f$ in point $x$ as the value of the above quotient when $\\Delta x$ is getting infinitesimally small.\n",
    "\n",
    "In mathematical formalism the definition of the first order derivative of $f$ looks in the following way:\n",
    "\n",
    "$$f'(x) = \\lim\\limits_{\\Delta x \\to 0} \\frac{f(x+\\Delta x)-f(x)}{\\Delta x}$$\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1100\"\n",
       "            height=\"900\"\n",
       "            src=\"https://www.geogebra.org/classic/enyhcvgw\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104d6f550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://www.geogebra.org/classic/enyhcvgw\", 1100, 900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Differentiation rules \n",
    "\n",
    "Observe that the above defined derivative satisfies the following properties:\n",
    "\n",
    "0. constant rule: $c' = 0$, for any constant $c \\in \\mathbb{R}$ \n",
    "\n",
    "1. constant mutiple rule: $(cf(x))' = c f'(x)$, where $c \\in \\mathbb{R}$ \n",
    "\n",
    "2. sum and difference rule: $(f(x) \\pm g(x))' = f'(x)\\pm g'(x)$\n",
    "\n",
    "3. product rule: $(f(x) \\cdot g(x))' = f'(x)g(x) + f(x)g'(x)$\n",
    "\n",
    "4. power rule: $\\left(x^r\\right)' = r x^{r-1}$, where $r \\in \\mathbb{R}\\setminus\\{0\\}$\n",
    "\n",
    "5. exponential derivative: $(e^x)' = e^x$, $(a^x)' = ln(a) a^x$, where $a \\in (0,\\infty) \\setminus\\{1\\}$\n",
    "\n",
    "6. logarithm derivative: $(\\ln(x))' = \\frac{1}{x}$, $\\log_a(x) = \\frac{1}{\\ln(a)x}$, where $a \\in (0,\\infty) \\setminus\\{1\\}$ and $x \\neq 0$\n",
    "\n",
    "7. trigonometric derivatives: $(\\sin(x))' = \\cos(x)$, $(\\cos(x))' = -\\sin(x)$, \n",
    "\n",
    "8. chain rule: $\\left(f(g(x))\\right)' = f'(g(x)) \\cdot g'(x)$\n",
    "\n",
    "<!-- <center>\n",
    "<img src=\"Images/DifferentiationRules.png\" width=\"500\"> \n",
    "</center>-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2.3. Differentiation of a multivariate function\n",
    "\n",
    "-------------\n",
    "**Definition of the partial derivative**\n",
    "\n",
    "When the function $f:\\mathbb{R}^n \\to \\mathbb{R}$ depends on more variables $x_1, x_2, \\ldots, x_n$ and it is nice enough, we can calculate its partial derivatives w.r.t. each variable. The partial derivative of the function $f$ in a point $x^* =(x_1^*, x_2*, \\ldots, x_n^*)$ w.r.t. the variable $x_1$ can be calculated by fixing the values of the other parameters to be equal to $x_2^*, \\ldots, x_n^*$ and differentiating the so resulting function by its only parameter $x_1$.\n",
    "\n",
    "To describe the formula in a mathematical exact way let us consider the function $g: \\mathbb{R} \\to \\mathbb{R}$ defined by the formula \n",
    "\n",
    "$$g(x_1) = f(x_1, x_2^*, \\ldots, x_n^*)$$\n",
    "\n",
    "Then the partial derivative of $f$ w.r.t. $x_1$ is denoted by $\\frac{\\partial f}{\\partial x_1}$ and is equal to the derivative of $g$ in the point $x_1^*$, that is\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_1}(x_1^*,, x_2^*, \\ldots, x_n^*) = g'(x_1^*)$$\n",
    "\n",
    "---------------\n",
    "\n",
    "Alternatively we can use for this partial derivative also other notations, like the shorter \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_1}(x^*) \\quad \\mbox{or} \\quad \\partial_{x_1} f(x^*)$$\n",
    "\n",
    "When it clear that we are performing our calculations in the point $x^*$ and there is no source for confusion, we can omit $x^*$ also and work just with \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_1} \\quad \\mbox{or} \\quad \\partial_{x_1} f$$\n",
    "\n",
    "We can proceed similarly in the case of the other variables to calculate all partial derivatives\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_2}(x^*), \\quad \\frac{\\partial f}{\\partial x_3}(x^*), \\quad \\ldots \\quad, \\frac{\\partial f}{\\partial x_n}(x^*)$$\n",
    "\n",
    "-------------\n",
    "**Definition of the Jacobian**\n",
    "\n",
    "The row vector of all partial derivatives is called the **gradient** of the function or the **Jacobian** of it, that is\n",
    "\n",
    "$$ \\nabla f (x^*) = \\left( \\frac{\\partial f}{\\partial x_1} (x^*), \\frac{\\partial f}{\\partial x_2}(x^*), \\ldots \\frac{\\partial f}{\\partial x_n}(x^*)\\right)$$\n",
    "\n",
    "------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further generalisation of the Jacobian**\n",
    "\n",
    "For a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ having also a multivariate output, we can take each output and calculate its partial derivatives w.r.t. each input variable $x_1, x_2, \\ldots, x_n$. For the first output we will have $n$ partial derivatives,i.e.\n",
    "\n",
    "$$\\frac{\\partial f_1}{\\partial x_1}(x^*), \\quad \\frac{\\partial f_1}{\\partial x_1}(x^*), \\quad \\ldots \\quad, \\frac{\\partial f_1}{\\partial x_n}(x^*)$$\n",
    "\n",
    "And for each output the same will happen. We will organise these partial derivatives in a matrix in such a way that in the $i$th row the derivatives of $f_i$ will be enlisted, and at the intersection of $i$th row and $j$th column the derivative \n",
    "\n",
    "$$\\frac{\\partial f_i}{\\partial x_j}(x^*)$$\n",
    "\n",
    "will be stored.\n",
    "\n",
    "This way we obtain the matrix\n",
    "\n",
    "$$\\nabla f (x^*) = \\left(\n",
    "\\begin{array}{cccc}\n",
    "\\frac{\\partial f_1}{\\partial x_1}(x^*) & \\frac{\\partial f_1}{\\partial x_2}(x^*) & \\cdots & \\frac{\\partial f_1}{\\partial x_n}(x^*)\\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1}(x^*) & \\frac{\\partial f_2}{\\partial x_2}(x^*) & \\cdots & \\frac{\\partial f_2}{\\partial x_n}(x^*)\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1}(x^*) & \\frac{\\partial f_m}{\\partial x_2}(x^*) & \\cdots & \\frac{\\partial f_m}{\\partial x_n}(x^*)\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "This matrix is called the Jacobian of the function $f$. \n",
    "\n",
    "Sometimes for the notation of the above Jacobian matrix the $\\frac{\\partial f}{\\partial x}$ or $\\partial_x f$ notations are also used. These latter notations are preferred when the function $f$ might depend on other variables as well and we would like to emphasize w.r.t. which variables do we consider the Jacobian.\n",
    "\n",
    "---------------\n",
    "\n",
    "### 3.2.4. Multivariate chain rule\n",
    "\n",
    "\n",
    "Having introduced the Jacobian, we can formulate the multivariate chain rule. \n",
    "\n",
    "-------------\n",
    "**The multivariate chain rule**\n",
    "\n",
    "For two functions $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ and $g: \\mathbb{R}^m \\to \\mathbb{k}$ the Jacobian of the compound function $f \\circ g$ in the point $x^*$ is:\n",
    "\n",
    "$$(\\nabla f \\circ g) (x^*) = (\\nabla f)(g(x^*)) \\cdot (\\nabla g)(x^*)$$\n",
    "\n",
    "-------------\n",
    "\n",
    "### 3.2.5. Higher order differentials (uni- and multivariate case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "**Definition of higher order differentials / derivatives**\n",
    "\n",
    "For a function $f: \\mathbb{R}\\to \\mathbb{R}$ we can calculate its derivative in each point, this means that the derivative $f'$ of the function is again a function mapping each point $x \\in \\mathbb{R}$ to the derivative $f'(x)$.\n",
    "\n",
    "Now we could differentiate again each the first order derivative $f'$ and as such we get to the second order derivative, i.e.\n",
    "\n",
    "$$f''(x) = \\lim\\limits_{\\Delta x \\to 0}\\frac{f'(x+\\Delta x) - f'(x)}{\\Delta x}$$\n",
    "\n",
    "The second order derivative can be again differentiated and this way we obtain the $3$rd order derivative of a function denoted by $f'''$ or $f^{(3)}$.\n",
    "\n",
    "The $n$th order derivative of a function $f: \\mathbb{R}\\to \\mathbb{R}$ in the point $x$ is denoted by $f^{(n)}(x)$ if it exists.\n",
    "\n",
    "-------------------\n",
    "**Multivariate case**\n",
    "\n",
    "We extend the notion of second order derivative to a function $f: \\mathbb{R}^n \\to \\mathbb{R}$.\n",
    "\n",
    "Consider as starting point the Jacobian of the function (which corresponds to the derivative from the univariate case). Let us calculate all partial derivatives of the first order partial derivatives from \n",
    "\n",
    "$$\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots \\frac{\\partial f}{\\partial x_n}\\right),$$\n",
    "\n",
    "and organize them in the following way in a matrix\n",
    "\n",
    "$$\\nabla^2 f = \\left(\n",
    "\\begin{array}{cccc}\n",
    "\\frac{\\partial^2 f}{\\partial x_1\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1\\partial x_n} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_n\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n\\partial x_n} \n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "then the resulting matrix is called the **Hessian matrix**.\n",
    "\n",
    "-----------\n",
    "\n",
    "The value of the Hessian matrix can be used \n",
    "\n",
    "- to derive better local approximation for a function than the linear one,\n",
    "- to find out whether a critical point is a minimum or maximim point or saddle point (exacly as the second order derivative helps us determine whether a critical point is an extreme point of the function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Applications of the differentials\n",
    "\n",
    "### 3.3.1. The Taylor series approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"800\"\n",
       "            src=\"https://www.geogebra.org/classic/kc2umqak\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10fc7e390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/kc2umqak\", 1000, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "**Definition of the Taylor polynomial of order $n$**\n",
    "\n",
    "The Taylor polynomial of an $n$-times differentiable function $f:\\mathbb{R} \\to \\mathbb{R}$ in a point $x_0$ is the polynomial $p$ of order $n$, for which it holds that\n",
    "\n",
    "$$\\left\\{\n",
    "\\begin{align*}\n",
    "f(x_0) &= p(x_0)\\\\\n",
    "f'(x_0) &= p'(x_0)\\\\\n",
    "f''(x_0) &= p''(x_0)\\\\\n",
    "\\vdots\\\\\n",
    "f^{(n)}(x_0) &= p^{(n)}(x_0)\n",
    "\\end{align*}\\right.$$\n",
    "\n",
    "-------\n",
    "**Remark**<br>\n",
    "1. Observe that the Taylor polynomial is uniquely defined and it is given by the following formula\n",
    "\n",
    "$$p(x) = \\frac{f(x_0)}{0!} + \\frac{f'(x_0)}{1!}(x-x_0) + \\frac{f''(x_0)}{2!} + \\cdot + \\frac{f^{(n)}(x_0)}{n!} (x-x_0)^n$$\n",
    "\n",
    "Where $0! = 0$ by convention.\n",
    "\n",
    "2. If the function is nice enough, then the approximation error $f(x) - p(x)$ is of the magnitude of $(x-x_0)^{n+1}$.\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**<br>\n",
    "The above polinomial has the property that the function value and the first $n$ derivatives of the original function $f$ and the polynomial $p$ are exactly the same in the point $x=x_0$. This polynomial is uniquely defined.\n",
    "\n",
    "--------------------\n",
    "**The Taylor approximation of a multivariate function**<br>\n",
    "For a function $f: \\mathbb{R}^n \\to \\mathbb{R}$ the Taylor approximation of order 1 is\n",
    "\n",
    "$$l(x) = \\frac{f(x_0)}{0!} + \\frac{\\nabla f(x_0)}{1!}\\cdot (x-x_0),$$\n",
    "\n",
    "where $\\nabla f(x_0)$ denotes the Jacobian of the function in point $x_0$ and this row vector is multiplied by the column vector $x-x_0$ in the above formula.\n",
    "\n",
    "For a function $f: \\mathbb{R}^n \\to \\mathbb{R}$ the Taylor approximation of order 2 is\n",
    "\n",
    "$$q(x) = \\frac{f(x_0)}{0!} + \\frac{1}{1!}\\nabla f(x_0)\\cdot (x-x_0) + \\frac{1}{2} (x-x_0)^T \\cdot \\nabla^2 f(x_0) \\cdot (x-x_0),$$\n",
    "\n",
    "where $\\nabla^2 f(x_0)$ denotes the Hessian of the function in point $x_0$ and this matrix  is multiplied from left by the row vector $(x-x_0)^T$ nd from the right by the column vector $x-x_0$ in the above formula.\n",
    "\n",
    "------------------------\n",
    "\n",
    "**Remark**<br>\n",
    "The gradient or Jacobian of the function $f$ has the following two properties, which are crutial for our forthcoming applications:\n",
    "- in a fixed point $x =(x_1, x_2, \\ldots, x_n)$ the gradient/ Jacobian $\\nabla f$ points up the hill along the steepest direction\n",
    "- its length is proportional to the steepness.\n",
    "\n",
    "*Proof*<br>\n",
    "In case of a univariate function $f:\\mathbb{R} \\to \\mathbb{R}$, in the proximity of a chosen point $x$, the best linear approximation of the function $f$ is given by the equation\n",
    "\n",
    "$$y = f(x) + f'(x) \\Delta x $$\n",
    "\n",
    "Therefore the total change of the function $f(x + \\Delta x) - f(x)$ can be approximated by $f'(x) \\Delta x$.\n",
    "\n",
    "Similarly in case of a multivariate function $f: \\mathbb{R}^n \\to \\mathbb{R}$ the total change around a point $x$ can be approximated by $\\langle \\nabla f(x),\\Delta x\\rangle = \\nabla f(x) \\cdot \\Delta x$, where $\\Delta x$ is an element of the tangent space of the surface at the point $x$ expressed w.r.t. the basis $\\frac{\\partial f(x)}{\\partial x_1}, \\frac{\\partial f(x)}{\\partial x_2}, \\ldots, \\frac{\\partial f(x)}{\\partial x_n}$. Its compoenents $\\Delta x_1, \\Delta x_2, \\ldots, \\Delta x_n$ are called the increments.\n",
    "\n",
    "Let us recall that \n",
    "\n",
    "$$\\nabla f(x) \\cdot \\Delta x = ||\\nabla f(x)|| \\cdot ||\\Delta x|| \\cdot \\cos(\\theta),$$\n",
    "\n",
    "where $\\theta$ is the angle of the vectors $\\nabla f(x)$ and $\\Delta x$.\n",
    "\n",
    "The function is the steepest into the direction for which the total change of the function is maximal. Therefore we would like to determine the unit-length vector $\\Delta x$ for which $\\nabla f(x) \\cdot \\Delta x$ is maximal. By the previous formula this will be achieved when $\\cos(\\theta) = 1 \\Leftrightarrow \\theta = 0$, i.e. $\\Delta x$ is the unit vector pointing into the same direction as $\\nabla f(x)$, namely \n",
    "\n",
    "$$ \\Delta x = \\frac{\\nabla f(x)^T}{||\\nabla f(x)||}$$  \n",
    "\n",
    "By this we have shown that the Jacobian $\\nabla f(x)$ is pointing towards the steepest direction up the hill.\n",
    "\n",
    "Furthermore,\n",
    "$$\\begin{align*}\n",
    "&f(x+\\Delta x) - f(x) \\simeq \\nabla f(x) \\cdot \\Delta x\\\\\n",
    "&\\max_{||\\Delta x|| = 1} \\nabla f(x) \\cdot \\Delta x = \\nabla f(x) \\cdot \\frac{\\nabla f(x)^T}{||\\nabla f(x)||} = \\frac{||\\nabla f(x)||^2}{||\\nabla f(x)||} = ||\\nabla f(x)||\n",
    "\\end{align*},$$\n",
    "\n",
    "which shows that its length is proportional to the steepness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**<br>\n",
    "The contour lines are such lines, where the value of the function stays constant. Most probably you have seen contour lines of peaks or of the sea on old maps.\n",
    "\n",
    "In the below animation you can move from one contour line to another by changing the value of $z$ on the slider and you can also move the point $A$ on the active contour line. What do you observe? What is the relation between the contour lines and the Jacobian / gradient?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"https://www.geogebra.org/classic/atrvy2e9\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x107dcf5f8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/atrvy2e9\", 1200, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jacobian vector is ................................. the contour lines.\n",
    "\n",
    "To prove this also formally consider the following setting. Let $f: \\mathbb{R^n} \\to \\mathbb{R}$ be a function, defining an $n$-dimensional surface in $\\mathbb{R}^n$. And let $c:t \\to \\left(\n",
    "\\begin{array}{c}\n",
    "c_1(t)\\\\\n",
    "c_2(t)\\\\\n",
    "\\vdots\\\\\n",
    "c_n(t)\n",
    "\\end{array}\n",
    "\\right)$ be a curve in the parameterspace of $f$ such that the curve $f(c(t))$ is a contourline, i.e. $f(c(t)) = k$ for some constant $k \\in \\mathbb{R}$.\n",
    "\n",
    "The above property follows by differentiating the $f(c(t)) = k$ w.r.t. the variable $t$. By the multivariate chain rule we obtain, that\n",
    "\n",
    "$$\\nabla f(c(t)) \\cdot \\nabla c(t) = 0,$$\n",
    "\n",
    "i.e. the Jacobian / gradient of the function $f$ is perpendicular to the tangent of the contour line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. The Newton-Raphson method \n",
    "\n",
    "The Newton-Raphson method is used to find the approximate a root of a function.\n",
    "\n",
    "Observe how does it work and identify the steps of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.geogebra.org/classic/mm9xvyxr\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10fd609b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/mm9xvyxr\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Raphson method is an iterative method. \n",
    "\n",
    "We cosider a function $f:\\mathbb{R} \\to \\mathbb{R}$\n",
    "    \n",
    "The purpose of this method is to approximate roots of the function, i.e. such $x$ values for which $f(x) = 0$.\n",
    "\n",
    "Let us assume that we know the value of the function in a point $x_0$, i.e we know $f(x_0)$. We approximate the behaviour of the function by the tangent line\n",
    "\n",
    "$$f(x) \\simeq l(x) = f(x_0) + f'(x_0)\\cdot (x-x_0)$$\n",
    "\n",
    "and we solve the equation \n",
    "\n",
    "$$l(x) = 0$$\n",
    "\n",
    "The solution of this will be denoted by $x_1$ and by solving the above linear equation we obtain that\n",
    "\n",
    "$$x_1 = x_0 -  \\frac{f(x_0)}{f'(x_0)}$$\n",
    "\n",
    "$x_1$ is our second approximation for a root of $f$. \n",
    "\n",
    "If we continue the process now by constructing the tangent line in $x_1$ and defining the next point as an intersection of the tangent with the $x$-axis, then \n",
    "\n",
    "$$x_2 = x_1 - \\frac{f(x_0)}{f'(x_0)}$$\n",
    "\n",
    "will be our third approximation for the root.\n",
    "\n",
    "If the function is nice enough, then this method converges to a root of the function.\n",
    "\n",
    "<!--The method can generalised for functions of type $f: \\mathbb{R}^n \\to \\mathbb{R}$. In this setting we start by choosing an $x_0$ and the next value of the sequence that we construct can be determined as\n",
    "\n",
    "$$x_{n+1} = x_{n} - \\lambda \\nabla f(x_n) .$$-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. Gradient descent method\n",
    " \n",
    "The gradient descent method is similar to the Newton-Raphson one in the sense that we perform an iterative step in the steepest direction. The difference is that the goal of this process is to minimise a cost function $C: \\mathbb{R}^n \\to \\mathbb{R}$ (or $C: \\mathbb{R}^n \\to \\mathbb{R}$ in the multivariate case). We update the gradient in every iterative step and we move along the steepest gradient downwards, i.e.\n",
    "\n",
    "$$x_{m+1} = x_{m} - \\lambda \\nabla f(x_m).$$\n",
    "\n",
    "The parameter $\\lambda$ from the above formula is called the **step size** or the **learning rate** of the gradient descent algorithm.\n",
    "\n",
    "To learn about the different types of gradient descent used in a machine learning context please visit [this link](https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"https://www.geogebra.org/classic/xfa7y3wc\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x105e1b080>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/xfa7y3wc\", 1200, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3.4. Backpropagation\n",
    "\n",
    "To perform the gradient descent method, we need to calculate the Jacobian of the cost function w.r.t. all the parameters of the model. As we have seen in the introduction, a neuronal network can be very complex, but here as a starting point for backpropagation let us consider the following simple network\n",
    "\n",
    "<center>\n",
    "<img src=\"Images/Network4Backpropagation1.png\" width=\"400\"> \n",
    "</center>\n",
    "\n",
    "Let us assume that the cost function is the squared error \n",
    "\n",
    "$$C = (y^{(1)} - y)^2$$\n",
    "\n",
    "and let us assume that we have just one training example.\n",
    "\n",
    "By backpropagation is meant nothing else but the multiple application of the chain rule targeted towards the calculation of the Jacobian of $C$ w.r.t. the model parameters $w^{(1)}$, respectively $b^{(1)}$. Let us write down what leads us from the input values $y^{(0)} = x$ all way to the cost function $C$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&z^{(1)} = b^{(1)} + w^{(1)}y^{(0)}\\\\\n",
    "\\\\\n",
    "&y^{(1)} = g\\left(z^{(1)}\\right)\\\\\n",
    "\\\\\n",
    "&C = \\left(y^{(1)} - y\\right)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "From the above the following formula folds out automatically\n",
    "\n",
    "$$\\mathbf{\\frac{\\partial C}{\\partial w^{(1)}}} = \\frac{\\partial C}{\\partial y^{(1)}} \\cdot \\frac{\\partial y^{(1)}}{\\partial w^{(1)}} = \\mathbf{\\frac{\\partial C}{\\partial y^{(1)}} \\cdot \\frac{\\partial y^{(1)}}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial w^{(1)}}}$$\n",
    "\n",
    "We obtain similarly that \n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^{(1)}} = \\frac{\\partial C}{\\partial y^{(1)}} \\cdot \\frac{\\partial y^{(1)}}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial b^{(1)}}$$\n",
    "\n",
    "-----\n",
    "**Remark**<br>\n",
    "For a given activation function $g$ in the above formulae we can calculate every Jacobian\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial y^{(1)}} = 2 \\left(y^{(1)} - y\\right) \\quad \\quad\n",
    "\\frac{\\partial y^{(1)}}{\\partial z^{(1)}} = g'\\left(z^{(1)}\\right) \\quad \\quad\n",
    "\\frac{\\partial z^{(1)}}{\\partial w^{(1)}} = y^{(0)} \\quad \\quad\n",
    "\\frac{\\partial z^{(1)}}{\\partial b^{(1)}} = 1\n",
    "\\end{align*}$$\n",
    "\n",
    "-----\n",
    "\n",
    "Let us consider the setting of the below more complex neuronal network\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"Images/Network4Backpropagation2.png\" width=\"400\"> \n",
    "</center>\n",
    "\n",
    "For this setting the formulae leading from the input to the output can be summarised similarly in matrix form\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&z^{(1)} = b^{(1)} + w^{(1)}y^{(0)}\\\\\n",
    "\\\\\n",
    "&y^{(1)} = g_1\\left(z^{(1)}\\right)\\\\\n",
    "\\\\\n",
    "&C = ||y^{(1)} - y||^2 = (y^{(1)}_1 - y_1)^2 + (y^{(1)}_2 - y_2)^2 + (y^{(1)}_3 - y_3)^2 = (y^{(1)}-y)^T \\cdot (y^{(1)}-y),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y^{(0)} = \\left(\n",
    "\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_d\n",
    "\\end{array}\n",
    "\\right), \\quad \n",
    "w^{(1)} = \\left(\n",
    "\\begin{array}{cccc}\n",
    "w_{1,1}^{(1)} & w_{1,2}^{(1)} & \\ldots & w_{1,d}^{(1)}\\\\\n",
    "w_{2,1}^{(1)} & w_{2,2}^{(1)} & \\ldots & w_{2,d}^{(1)}\\\\\n",
    "w_{3,1}^{(1)} & w_{3,2}^{(1)} & \\ldots & w_{3,d}^{(1)}\n",
    "\\end{array}\n",
    "\\right), \\quad\n",
    "b^{(1)} = \\left(\n",
    "\\begin{array}{c}\n",
    "b_1^{(1)}\\\\\n",
    "b_2^{(1)}\\\\\n",
    "b_3^{(1)}\n",
    "\\end{array}\n",
    "\\right), \\quad\n",
    "z^{(1)} = \\left(\n",
    "\\begin{array}{c}\n",
    "z_1^{(1)}\\\\\n",
    "z_2^{(1)}\\\\\n",
    "z_3^{(1)}\n",
    "\\end{array}\n",
    "\\right), \\quad\n",
    "y^{(1)} = \\left(\n",
    "\\begin{array}{c}\n",
    "g_1\\left(z_1^{(1)}\\right)\\\\\n",
    "g_1\\left(z_2^{(1)}\\right)\\\\\n",
    "g_1\\left(z_3^{(1)}\\right)\n",
    "\\end{array}\n",
    "\\right), \\quad\n",
    "y = \\left(\n",
    "\\begin{array}{c}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "y_3\n",
    "\\end{array}\n",
    "\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "As a consequence for this more complex neuronal network the desired Jacobians can be expressed by similar relations as before, namely\n",
    "\n",
    "$$\n",
    "\\begin{align*}\\frac{\\partial C}{\\partial w^{(1)}} = \\frac{\\partial C}{\\partial y^{(1)}} \\cdot \\frac{\\partial y^{(1)}}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial w^{(1)}}\\\\\n",
    "\\frac{\\partial C}{\\partial b^{(1)}} = \\frac{\\partial C}{\\partial y^{(1)}} \\cdot \\frac{\\partial y^{(1)}}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial b^{(1)}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "-----\n",
    "**Remark**<br>\n",
    "For a given activation function $g$ in the above formulae we can calculate every Jacobian\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\frac{\\partial C}{\\partial y^{(1)}} = 2 \\left(y^{(1)} - y\\right)^T = 2 \\left(y^{(1)}_1 - y_1, y^{(1)}_2 - y_2, y^{(1)}_3 - y_3 \\right)\\\\\n",
    "\\\\\n",
    "&\\frac{\\partial y^{(1)}}{\\partial z^{(1)}} = \n",
    "\\left(\\begin{array}{ccc}\n",
    "g_1'\\left(z_1^{(1)}\\right) & 0 & 0\\\\\n",
    "0 & g_1'\\left(z_2^{(1)}\\right) & 0\\\\\n",
    "0 & 0 & g_1'\\left(z_3^{(1)}\\right)\n",
    "\\end{array}\n",
    "\\right)\\\\\n",
    "\\\\\n",
    "&\\frac{\\partial z^{(1)}}{\\partial w^{(1)}} = \n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "\\left(y^{(0)}\\right)^T & 0_{1\\times d} & 0_{1 \\times d}\\\\\n",
    "0_{1\\times d} & \\left(y^{(0)}\\right)^T & 0_{1\\times d}\\\\\n",
    "0_{1\\times d} & 0_{1\\times d} & \\left(y^{(0)}\\right)^T\n",
    "\\end{array}\n",
    "\\right)\\\\\n",
    "\\\\\n",
    "&\\frac{\\partial z^{(1)}}{\\partial b^{(1)}} = I_3\n",
    "\\end{align*}$$\n",
    "\n",
    "-----\n",
    "\n",
    "<!--\n",
    "Speed vs time, tangent -> acceleration\n",
    "Is the speed as function the derivative of some other function? Related to the integral, antiderivative. Distance vs time.\n",
    "\n",
    "Geometrical def. of the derivative: \"rise over run\" gradient - for two points on the graph of the function\n",
    "\n",
    "the gradient of the tangent line\n",
    "\n",
    "Formal definition of the derivative with $\\Delta x$, $f$ and $\\lim$.\n",
    "\n",
    "Ex. Calculate the derivative of a linear function.\n",
    "Ex. Calculate the derivative of a parabolic function / polynom of grade 2.\n",
    "\n",
    "Sum rule, power rule.\n",
    "\n",
    "Special functions and their derivatives: 1/x, e^x (the only function being equal to its derivative), \n",
    "\n",
    "Product rule. Quotient rule can be derived also from the product rule and whenever the quotient rule should be used, one can use equivalently the product rule as well.\n",
    "\n",
    "## Chain law / rule\n",
    "\n",
    "## Multivariate differentiation\n",
    "dependent and independent variables, how do we select them? speed can be depicted as the function of time, but not the other way around.\n",
    "\n",
    "partial differentiation - fix all the variables except one constant and calc. the derivative w.r.t. the remaining variable\n",
    "\n",
    "Chain rule for the multivariate setup.\n",
    "\n",
    "## The Jacobian \n",
    "The Jacobian vector of a mutivariate function. - the vector pointing to the steepest slope. Contour lines with Jacobian directions.\n",
    "\n",
    "The Jacobian for vector valued functions.\n",
    "\n",
    "## Looking for extremal values of a function. \n",
    "Context: y = f(x), z = f(x,y)\n",
    "\n",
    "Sandpit. Find the deepest point by Jacobians, by the depth of the pit.\n",
    "\n",
    "## The Hessian - the Jacobian of the Jacobian\n",
    "Hessian - shows whether the found stationary point is a min or max point.\n",
    "\n",
    "## In reality we don't know the function\n",
    "We estimate also the Jacobians. What should be the stepsize? We calculate the approx. of the Jac. for different step sizes and we average out.-->\n",
    "\n",
    "<!--## Total derivative\n",
    "When our function depends on n variables and the variables depend on the same parameter t. \n",
    "$$\\frac{{\\rm d}f}{{\\rm d}t} = \\frac{\\partial f}{\\partial x} \\cdot \\frac{{\\rm d} x}{{\\rm d} t} = J_f \\frac{{\\rm d} x}{{\\rm d} t}$$-->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
