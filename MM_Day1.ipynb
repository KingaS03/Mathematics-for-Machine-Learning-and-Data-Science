{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/KingaS03/Introduction-to-Python-2020-June/master)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KingaS03/Introduction-to-Python-2020-June)\n",
    "\n",
    "# 1. Introduction to the Mathematics Module for ML and DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to consider a common machine learning context, as this is illustrating all the major components of our course.\n",
    "\n",
    "## 1.1. Machine learning versus classical programming\n",
    "First let's take a look at machine learning and compare it with classical programming.\n",
    "\n",
    "------------------------------------\n",
    "*Machine Learning is the \"field of study that gives computers the ability to learn without being explicitly programmed.\"* - Arthur Samuel, 1959\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "<center>\n",
    "<img src=\"Images/TraditionalProgr.png\" width=\"300\"> \n",
    "</center>\n",
    "\n",
    "versus\n",
    "\n",
    "<center>\n",
    "<img src=\"Images/MachineLearning0.png\" width=\"300\"> \n",
    "</center>\n",
    "\n",
    "In case of some machine learning problems the resulting rules can be conceived as a model, which for any input data is able to predict the associated output. \n",
    "\n",
    "<center>\n",
    "<img src=\"Images/MachineLearning1.png\" width=\"300\"> \n",
    "</center>\n",
    "\n",
    "Mathematically we can think of this model as a function that assigns to an input value a predicted output value and at the same time it depends also on some model parameters (weights and intercept). The model parameters are determined in such a way to minimise the loss function. This phenomenon is concisely described in the following quote:\n",
    "\n",
    "-----------\n",
    "*\"A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\"* - Tom Mitchell, 1997\n",
    "\n",
    "---------------\n",
    "\n",
    "## 1.2. Simple machine learning setting - Linear regression\n",
    "One of the simplest settings of a machine learning algorithm is the linear regression. To get a quick intuition about how it works play with the below interactive graph. You can change the position of the blue datapoints by dragging them with the mouse. You can change the position of the red line by moving the two red points of it.\n",
    "\n",
    "What happens on the plot on the right hand side if you change the position of one of the red points? How can you explain the observed behaviou?\n",
    "\n",
    "Take a look at the blue end red values in the upper right corner. How do these values change?\n",
    "\n",
    "What is the starting point of a regression analysis and what is its objective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1400\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.geogebra.org/classic/gvtvpem2\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x102df97f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "#IFrame(\"https://www.geogebra.org/m/xC6zq7Zv\",800,800)\n",
    "IFrame(\"https://www.geogebra.org/classic/gvtvpem2\", 1400, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a concrete numerical example:\n",
    "\n",
    "We would like to predict the price of apartments as a linear function of their surface.\n",
    "\n",
    "We consider the following data points:\n",
    "    \n",
    "| Surface area in $m^2$ | Price in tausends of CHF | \n",
    "| --- | --- |\n",
    "| 40| 275| \n",
    "| 70 | 500 | \n",
    "| 80 | 470 | \n",
    "| 100 | 650 | \n",
    "| 115 | 690 | \n",
    "| 120 | 750 | \n",
    "\n",
    "The surface area, denoted by $x$, is the single **explanatory/dependent variable**. \n",
    "\n",
    "The price, denoted by $y$, is the single **independent variable**.\n",
    "\n",
    "The apartments, whose prices are enlisted in the above table are called **observations** and we will refer to their **features** (surface and price) as $x_i$, respectively $y_i$, where $i$ is the index of the apartment ($i = \\overline{0,5}$).\n",
    "\n",
    "We would like to approximate our data points by a line defined by the equation\n",
    "$$ y = w\\cdot x + b,$$\n",
    "where $w$ is called **weight/gradient** and $b$ is called **intercept**. These parameters $w$ and $b$ are determined in such a way that the mean squared error of the approximations is minimal.\n",
    "\n",
    "For our set of apartments the **$MSE$ (mean squared error)** can be calculated as follows:\n",
    "$$\\begin{align*}\n",
    "MSE &= \\frac{1}{6}\\sum_{i=0}^5 \\left(y_i - (w \\cdot x_i + b)\\right)^2\\\\\n",
    "& = \\frac{1}{6}\\left(\\left(275-(w\\cdot 40 + b)\\right)^2 + \\left(500-(w\\cdot 70 + b)\\right)^2 + \\left(470-(w\\cdot 80 + b)\\right)^2 + \\left(650-(w\\cdot 100 + b)\\right)^2 + \\left(690-(w\\cdot 115 + b)\\right)^2 + \\left(750-(w\\cdot 120 + b)\\right)^2\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "$MSE$ is a function of the parameters $b$ and $w$.\n",
    "\n",
    "The goal is to determine the parameters $b$ and $w$ in such a way to obtain the minimal $MSE$.\n",
    "\n",
    "Experiment with the following code. Fit the red line to the data points by trying out different values for the parameters $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Price (tausend CHF)')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 300 random samples\n",
    "x = np.array([40, 70, 80, 100, 115, 120])\n",
    "y = np.array([275, 500, 470, 650, 690, 750])\n",
    "\n",
    "plt.plot(x, y, 'o') #scatter plot of data points\n",
    "\n",
    "w = 5.9 # change this value\n",
    "b = 40 # change this value\n",
    "\n",
    "plt.plot(x, b + w*x) #add line of best fit\n",
    "\n",
    "MSE = np.mean((y-(b + w*x))**2)\n",
    "\n",
    "# legend, title, and labels.\n",
    "plt.text(40,700, f\"MSE ={MSE:.2f}\")\n",
    "plt.title('Surface area and price of apartments', size=18)\n",
    "plt.xlabel('Surface (m^2)', size=18)\n",
    "plt.ylabel('Price (tausend CHF)', size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your values with the optimal ones, by running the code<br> \n",
    "`w, b = np.polyfit(x, y, 1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the simplicity of the linear model, it is possible to derive the explicit formulas for the parameters by calculating the partial derivatives of the $MSE$ w.r.t. the parameters and setting the values of these to $0$. Without detailed explanation \n",
    "$$\\frac{\\partial MSE}{\\partial w} = 0, \\quad \\frac{\\partial MSE}{\\partial b} = 0$$\n",
    "one can conclude:\n",
    "$$\\left\\{\\begin{align*}\n",
    "w &= \\frac{\\sum\\limits_i (x_i - \\overline{x})\\cdot (y_i-\\overline{y})}{\\sum\\limits_i(x_i - \\overline{x})^2} = 5.70\\\\\n",
    "b &= \\overline{y}-w\\cdot \\overline{x} = 57.30\n",
    "\\end{align*}\\right.\n",
    "$$\n",
    "where $\\overline{x} = \\frac{\\sum_i x_i}{n}$ and $\\overline{y} = \\frac{\\sum_i y_i}{n}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Neuronal networks\n",
    "The above univariate linear regression model can be presented as\n",
    "\n",
    "<center>\n",
    "<img src=\"Images/Network0.png\" width=\"300\"> \n",
    "</center>\n",
    "\n",
    "For the future notation we leave away the virtual input of 1.\n",
    "\n",
    "Now we make the model more complex until we get to the a two-layer neuronal network. First we apply an activation function $g$ to the linear transformation $b+w \\cdot x$.\n",
    "\n",
    "<center>\n",
    "<img src=\"Images/Network1.png\" width=\"320\"> \n",
    "</center>\n",
    "\n",
    "The above red ball corresponds to the smallest building unit of a neuronal network, namely a neuron. In a neuron:\n",
    "- there happens a linear transformation\n",
    "- to which an activation function is applied and this provides the output of the neuron.\n",
    "\n",
    "Next we allow for more inputs. \n",
    "<center>\n",
    "<img src=\"Images/MultiNetwork00.png\" width=\"320\"> \n",
    "</center>\n",
    "\n",
    "Finally we allow also for more outputs and we have two hidden layers. \n",
    "<center>\n",
    "<img src=\"Images/MultiNetwork2.png\" width=\"500\"> \n",
    "</center>\n",
    "\n",
    "The number of hidden layers indicates that the last neuronal network is a two-layer neuronal network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Motivation\n",
    "\n",
    "Exactly as in the case of linear regression the weight matrices $W^{(1)}$, $W^{(2)}$, respectively the intercepts $b^{(1)}$ and $b^{2}$ will be parameters of the loss function which is subject to minimisation. In the general case there is no immediate straightforward formula for the optimal parameters. \n",
    "<center>\n",
    "The minimum of the loss function can be approximated by the **gradient descent** method. \n",
    "    $$\\hspace{-2cm}\\Uparrow$$\n",
    "For the gradient descent method we should be able to derive the **partial derivatives** of the outputs w.r.t. all parameters of the model.\n",
    "    $$\\hspace{-2cm}\\Uparrow$$\n",
    "For neuronal networks with more hidden layers and differentiable activation functions these partial derivatives can be deremined by the **chain rule**.\n",
    "$$\\hspace{-2cm}\\Uparrow$$\n",
    "To apply the chain rule for a setting like in the last network, one needs to perform **matrix multiplications**.\n",
    "</center>\n",
    "\n",
    "The number of machine learning algorithms is large. That's why generally a huge amount of input data is needed to determine the model parameters. Alternatively, if we don't possess that much data, we can reduce the dimensionality of the input data (and by that we end up also with a smaller number of model parameters). For dimensionality reduction we can use the **PCA (principal component analysis)**, which is the same as singular value decomposition. The first name is used more in the circle of statisticians and the second name is more popular among theoretical mathematicians. To derive PCA, we need the notion of **orthogonal projection**, **eigenvalues and eigenvectors**, **the method of Lagrange multipliers** and some **descriptive statistics**.\n",
    "\n",
    "Furthermore, when the output of a neuronal network is a distribution, **probability theory** will be needed also to measure the distance between the observed distribution and the predicted one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Schedule\n",
    "-Linear algebra\n",
    " - vector operations\n",
    "     - vector addition, \n",
    "     - vector substraction, \n",
    "     - multiplication of a vector by a scalar\n",
    "     - the dot product\n",
    " - matrix operations\n",
    "     - matrix addition\n",
    "     - matrix substraction\n",
    "     - multiplication of a matrix by a scalar\n",
    "     - matrix multiplication\n",
    "     - inverse of a square matrix\n",
    " - projection and the dot product\n",
    " - orthogonal matrices\n",
    " - change of basis\n",
    " - eigenvalues and eigenvectors of matrices\n",
    " \n",
    "-Calculus\n",
    "\n",
    "-PCA\n",
    "\n",
    "-Probability theory and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Motivation\n",
    "We are able to solve equations of the form:\n",
    "$ax + b = c$, where $a,b,c$ are real coefficients and $x$ is the unknown variable.\n",
    "\n",
    "For example we can follow the next steps to solve the  $5x + 3 = 13$ equation\n",
    "\n",
    "$$\\begin{align*}\n",
    "5x + 3 &= 13 \\quad | -3\\\\\n",
    "5x &= 10 \\quad | : 5\\\\\n",
    "x &= 2\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$\\begin{align*}\n",
    "5x + 3 &= 13 \\quad | +(-3)\\\\\n",
    "5x &= 10 \\quad | \\cdot 5^{-1}= \\frac{1}{5}\\\\\n",
    "x &= 2\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Let us consider the following set-up. You have beakfast together with some of your colleagues and you are paying by turn. You don't know the price of each ordered item, but you remember what was ordered on the previous three days and how much did your colleagues pay for it each time: \n",
    "- 3 days ago your group has ordered 5 croissants, 4 coffees and 3 juices and they have payed 32.3 CHF.\n",
    "- 2 days ago your group has ordered 4 croissant, 5 coffees and 3 juices and they payed 32.5 CHF.\n",
    "- 1 day ago the group has ordered 6 croissants, 5 coffees and 2 juices and that costed 31 CHF.\n",
    "\n",
    "Today the group has ordered 7 croissants, 4 coffees and 2 juices and you would like to know whether the amount of 35 CHF available on your uni card will cover the consumption or you need to recharge it before paying.\n",
    "\n",
    "By introducing the notations\n",
    "- $x_1$ for the price of a croissant,\n",
    "- $x_2$ for the price of a coffee,\n",
    "- $x_3$ for the price of a juice,\n",
    "then our information about the consumption of the previous 3 days can be summarised in the form of the following 3 linear equations <br><br>\n",
    "$$\\left\\{\\begin{align}\n",
    "&5\\cdot x_1 + 4 \\cdot x_2 + 3 \\cdot x_3 = 32.3 \\\\\n",
    "&4\\cdot x_1 + 5 \\cdot x_2 + 3 \\cdot x_3 = 32.5 \\\\\n",
    "&6\\cdot x_1 + 5 \\cdot x_2 + 2 \\cdot x_3 = 31\n",
    "\\end{align}\\right.$$\n",
    "<br>\n",
    "The quantity ordered on the current day is $7\\cdot x_1 + 4 \\cdot x_2 + 2 \\cdot x_3$. To determine this one possibility is to calculate the price of each product separately, i.e. we solve the linear equation system first and then susbstitute the prices in the previous formula.\n",
    "\n",
    "The above system in matrix form\n",
    "<br><br>\n",
    "$$\n",
    "\\left(\\begin{array}{ccc}\n",
    "5 & 4 & 3\\\\\n",
    "4 & 5 & 3\\\\\n",
    "6 & 5 & 2\n",
    "\\end{array}\n",
    "\\right)\\cdot \\left(\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_3 \n",
    "\\end{array}\n",
    "\\right) = \n",
    "\\left(\\begin{array}{c}\n",
    "32.3\\\\\n",
    "32.5\\\\\n",
    "31 \n",
    "\\end{array}\n",
    "\\right)$$\n",
    "<br>\n",
    "If we introduce for the matrix, respectively the two vectors in the above formula the notations $A, x, b$, then we get <br><br>\n",
    "$$A \\cdot x = b$$\n",
    "<br>\n",
    "One can observe that formally this looks the same as the middle state of our introductory linear equation with real coefficients $5x = 10$. So our goal is to perform a similar operation as there, namely we are looking for teh operation that would make $A$ dissappear from the left hand side of the equation. We will see later that this operation will be the inverse operation of multiplication by a matrix, namely multiplication by the inverse of a matrix.\n",
    "\n",
    "In our applications we will encounter for example when deriving the weights of the multivariate linear regression, a matrix equation of the form:\n",
    "$A \\cdot x + b = 0$. This example motivates the introduction of vector substraction, as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Vectors\n",
    "\n",
    "----------\n",
    "**Definition of vectors**\n",
    "\n",
    "Vectors are elements of a linear vector space.\n",
    "The vector space we are going to work with is $\\mathbb{R}^n$, where $n$ is the dimension of the space and it can be $1, 2, 3, 4, ...$. An element of such a vector space can be described by an ordered list of $n$ components of the vector.\n",
    "\n",
    "$x = (x_1, x_2, \\ldots, x_n)$, where $x_1, x_2,\\ldots,x_n \\in \\mathbb{R}$ is an element of $\\mathbb{R}^n$.\n",
    "\n",
    "-----------\n",
    "\n",
    "**Example**<bf>\n",
    "$x = (1,2)$ is a vector of the two dimensional vector space $\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Geometrical representation of vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below a 2-dimensional vector is represented. You can move its endpoints on the grid and you will see how do its components change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.geogebra.org/classic/cnvxpycc\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1124449b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/cnvxpycc\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the 3-dimensional vector in the interactive window below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.geogebra.org/classic/meg4scuj\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1124bd438>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/meg4scuj\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following interactive window explains when are two vectors equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.geogebra.org/classic/fkkbkvuj\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1124bd278>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/fkkbkvuj\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Vector addition\n",
    "\n",
    "------\n",
    "**Definition of vector addition**\n",
    "\n",
    "Vector addition happens component-wise, namely the sum of the vectors $x = (x_1, x_2, \\cdots, x_n)$ and $y = (y_1, y_2, \\cdots, y_n)$ is:\n",
    "\n",
    "$$x+y = (x_1 + y_1, x_2+y_2, \\ldots, x_n+y_n)$$\n",
    "\n",
    "------\n",
    "\n",
    "There exist two approaches to visualise vector addition\n",
    "1. parallelogram method\n",
    "2. triangle method\n",
    "\n",
    "Both are visualised below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.geogebra.org/classic/jnchvrhg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1124bdb70>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/jnchvrhg\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see another approach to vector addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.geogebra.org/classic/mzgchv22\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a246b7240>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/mzgchv22\", 1200, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Multiplication of vectors by a scalar\n",
    "\n",
    "-------------\n",
    "**Definition of the multiplication by a scalar**\n",
    "\n",
    "This happens also component-wise exactly as addition, namely\n",
    "$$\\lambda  (x_1, x_2, \\ldots, x_n) = (\\lambda x_1, \\lambda x_2, \\ldots, \\lambda x_n).$$\n",
    "\n",
    "-------------\n",
    "This operation is illustrated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"800\"\n",
       "            src=\"https://www.geogebra.org/classic/gxhsev8k\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a243c7390>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/gxhsev8k\", 800, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Vector substraction\n",
    "\n",
    "------\n",
    "**Definition of vector substraction**\n",
    "\n",
    "Vector substraction happens component-wise, namely the difference of the vectors $x = (x_1, x_2, \\cdots, x_n)$ and $y = (y_1, y_2, \\cdots, y_n)$ is:\n",
    "\n",
    "$$x-y = (x_1 - y_1, x_2 - y_2, \\ldots, x_n -y_n)$$\n",
    "\n",
    "------\n",
    "\n",
    "Observe that vector substraction is not commutative, i.e. $x-y \\neq y-x$ in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5. Abstract linear algebra terminology\n",
    "\n",
    "1. For any two elements $x,y \\in \\mathbb{R}^n$ it holds that\n",
    "$$x+y \\in \\mathbb{R}^n.$$\n",
    "This property is called **closedness** of $\\mathbb{R}^n$ w.r.t. addition.\n",
    "\n",
    "2. Observe the **commutativity** of the addition on $\\mathbb{R}^n$ is inherited by the vectors in $\\mathbb{R}^n$, i.e. $$x + y = y + x$$ for any $x, y \\in \\mathbb{R}^n$.\n",
    "\n",
    "3. Observe that addition is also **associative** on $\\mathbb{R}^n$, i.e.\n",
    "$$x + (y + z) = (x + y) + z, \\quad \\mbox{ for any } x,y,z \\in \\mathbb{R}^n$$\n",
    "\n",
    "4. If we add the zero vector $\\mathbf{0} = (0, 0, ..., 0) \\in \\mathbb{R}^n$ to any other vector $x \\in \\mathbb{R}^n$ it holds that\n",
    "$$\\mathbf{0} + x = x + \\mathbf{0} = x.$$\n",
    "The single element with the above property is called the **neutral element** w.r.t. addition.\n",
    "\n",
    "5. For a vector $x = (x_1, x_2, \\ldots, x_n)$ the vector $x^*$ for which\n",
    "$$x + x^* = x^* + x = \\mathbf{0}$$\n",
    "is called the **inverse vector** of $x$ w.r.t. addition.\n",
    "\n",
    "What is the inverse of the vector $x = (2, 3, -1)$? Inverse: $-x = (-2,-3,1)$\n",
    "\n",
    "What is the inverse of a vector $x = (x_1, x_2, \\ldots, x_n)$? Inverse: $-x = (-x_1, -x_2, \\ldots, -x_n)$\n",
    "\n",
    "As every vector of $\\mathbb{R}^n$ possesses an inverse, we introduce the notation $-x$ for its inverse  w.r.t addition.\n",
    "\n",
    "A set $V$ with an operation $\\circ$ that satsifies the above properties is called a **commutative or Abelian group** in linear algebra. For us $V = \\mathbb{R}^n$ and $\\circ = +$.\n",
    "\n",
    "The scalar mutiplication, that we have introduced, has the following properties\n",
    "6. **associativity** of multiplication: $(\\lambda_1\\lambda_2) x = \\lambda_1 (\\lambda_2x)$, \n",
    "7. **distributivity**: $(\\lambda_1 + \\lambda_2) x = \\lambda_1  x + \\lambda_2  x$ and $\\lambda(x+y) = \\lambda x + \\lambda y$,\n",
    "8. **unitarity**: $1 x = x$,\n",
    "for all $x,y \\in \\mathbb{R}^n$ and $\\lambda, \\lambda_1, \\lambda_2$ scalars.\n",
    "\n",
    "Our scalars are elements of $\\mathbb{R}$. This set is a **field**, i.e. the operations $\\lambda_1+\\lambda_2$, $\\lambda_1-\\lambda_2$, $\\lambda_1\\cdot\\lambda_2$ make sense for any $\\lambda_1, \\lambda_2 \\in \\mathbb{R}$ and the $\\lambda_1/\\lambda_2 = \\lambda_1 \\cdot \\lambda_2^{-1}$ can be performed also when $\\lambda_2 \\neq 0$.\n",
    "\n",
    "A **vector space** consists of a set $V$ and a field $F$ and two operations:\n",
    "- an operation called vector  addition that takes two vectors $v,w \\in V$, and produces a third vector, written $v+w \\in V$,\n",
    "- an operation called scalar multiplication that takes a scalar $\\lambda \\in F$ and a vector $v\\in V$, and produces a new vector, written $cv \\in V$,\n",
    "which satisfy all the properties enlisted above (5+3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "**Remark**<br>\n",
    "Observe that\n",
    "$$x-y = x+(-y),$$\n",
    "<br>\n",
    "which means that the difference of $x$ and $y$ can be visualised as a vector addition of $x$ and $-y$.\n",
    "\n",
    "$-y$ is here the inverse of the vector $y$ w.r.t. addition. Geometrically $-y$ can be represented by the same oriented segment as $y$, just with opposite orientation.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.6. Modulus of a vector, length of a vector, size of a vector\n",
    "\n",
    "The length of a vector or norm of a vector $x = (x_1, x_2, \\cdots, x_n)$ is given by the formula\n",
    "$$||x|| = \\sqrt{x_1^2 + x_2^2 + \\cdots x_n^2}$$\n",
    "\n",
    "Experiment with the interactive window below and derive the missing formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.geogebra.org/classic/mfzdes3n\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a246f5710>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/mfzdes3n\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector $x = (x_1, x_2, \\ldots, x_n) \\in \\mathbb{R}^n$ is uniquely determined by the following two features: \n",
    "- its magnitude / length / size / norm: $r(x) = ||x|| = \\sqrt{x_1^2 + x_2^2 + \\cdots +x_n^2}$,\n",
    "- its direction: $e(x) = \\frac{x}{||x||} = \\frac{1}{\\sqrt{x_1^2 + x_2^2 + \\cdots +x_n^2}}(x_1, x_2, \\ldots, x_n)$.\n",
    "\n",
    "If the maginute $r \\in \\mathbb{R}$ and the direction $e \\in \\mathbb{R}^n$ of a vector is given, then this vector can be written as $re$.\n",
    "\n",
    "Observe that $\\frac{x}{||x||}$ has length $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.7. Dot product / inner product / scalar product\n",
    "\n",
    "-----------------\n",
    "Definition of the **dot product**\n",
    "\n",
    "The **dot product / inner product / scalar product** of two vectors $x = (x_1, x_2)$ and $y = (y_1, y_2)$ is denoted by $\\langle x, y\\rangle$ and it is equal to the scalar $x_1\\cdot y_1+ x_2 \\cdot y_2$.\n",
    "\n",
    "This can be generalised to the vectors $x = (x_1, x_2, \\cdots, x_n) \\in \\mathbb{R}^n$ and $y = (y_1, y_2, \\cdots, y_n) \\in \\mathbb{R}^n$ as $\\langle x, y\\rangle = x_1\\cdot y_1 + x_2 \\cdot y_2 + \\cdots + x_n\\cdot y_n$.\n",
    "\n",
    "---------------\n",
    "\n",
    "Observe that as a consequence of the definition distributivity over addition holds, i.e. $\\langle x , y + z\\rangle = \\langle x , y \\rangle + \\langle x , z \\rangle$.\n",
    "\n",
    "Furthermore $\\lambda \\langle x , y\\rangle  = \\lambda \\langle x, y\\rangle = \\langle x , \\lambda y\\rangle $.\n",
    "\n",
    "The last two properties together are called also **bilinearity** of the scalar product.\n",
    "\n",
    "Observe also that the scalar product is **commutative**, i.e. $x \\cdot y = y \\cdot x$.\n",
    "\n",
    "--------------------------------------\n",
    "**Question:** What's the relation between the length of a vector and the dot product?\n",
    "\n",
    "Length of a vector: $||x|| = \\sqrt{x_1^2+x_2^2 + \\cdots x_n^2}$\n",
    "\n",
    "The dot product of two vectors: $\\langle x , y \\rangle= x_1\\cdot y_1 + x_2 \\cdot y_2 + \\cdots + x_n\\cdot y_n$.\n",
    "\n",
    "Substituting $x$ in the last equation instead of $y$, we obtain\n",
    "\n",
    "$$\\mathbf{\\langle x , x \\rangle} = x_1^2+x_2^2 + \\cdots x_n^2 = \\mathbf{||x||^2}$$\n",
    "\n",
    "--------------------------------\n",
    "**Convention**<br>\n",
    "When talking exclusively about vectors, for the simplicity of writing, we often think of them as row vectors. However, when matrices appear in the same context and there is a chance that we will multiply a matrix by a vector, it is important to specify also whether we talk about a row or column vector. In this extended context a vector is considered to be a column vector by default.\n",
    "\n",
    "From now on we are going to follow also this convention and we are going to think of a vector always as a column vector.\n",
    "\n",
    "----------------\n",
    "**Relationship of dot product and matrix multiplication**<br> \n",
    "\n",
    "Even if we didn't define formally the matrix product yet, we mention its relationship with the dot product, because in mathematical formulas it proves to be handy to have an alternative way for writing the dot multiplication.\n",
    "\n",
    "The following holds for any vectors $x,y \\in \\mathbb{R}^n$\n",
    "$$\\mathbf{\\langle x,y \\rangle} = x_1\\cdot y_1 + x_2 \\cdot y_2 + \\cdots x_n \\cdot y_n = (\\begin{array}{cccc}\n",
    "x_1 & x_2 & \\ldots & x_n\n",
    "\\end{array})\\cdot\\left(\\begin{array}{c}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{array}\\right) = \\mathbf{x^T \\cdot y},$$\n",
    "where $\\mathbf{x^T \\cdot y}$ denotes the matrix product of the row vector $x_T$ and the column vector $y$.\n",
    "\n",
    "Furthermore, observe that due to the commutativity of the dot product <br><br>\n",
    "$$x^T \\cdot y = \\langle x, y \\rangle = \\langle y, x\\rangle = y^T \\cdot x$$\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.8. The dot product and the cosine rule\n",
    "Let us consider two vectors $u$ and $d$ and denote their angle by $\\theta$. We construct the triangle having as sides the vectors $u$, $d$ and $u-d$. In the forthcoming we derive the formula\n",
    "$$\\langle u,  d \\rangle = ||u||\\cdot ||d||\\cdot cos(\\theta)$$ from the law of cosines.\n",
    "\n",
    "<center>\n",
    "<img src=\"Images/Projection.png\" width=\"400\"> \n",
    "</center>\n",
    "\n",
    "The **law of cosines** is a generalisation of the **Pythagorean theorem** in a triangle, which holds not just for right triangles. As the Pythagorean theorem, this formulates also a relationship between the lengths of the three sides. In a triangle with side lengths $a$, $b$ and $c$ and an angle $\\theta$ opposite to the side with length $a$, the law of cosines claimes that <br>\n",
    "\n",
    "$$a^2 = b^2 + c^2 - 2bc\\cos(\\theta).$$\n",
    "\n",
    "In our setting we can write for the side lengths the norm / length of the vectors $u$, $d$, respectively $u-d$. In this way we obtain <br>\n",
    "\n",
    "$$||u-d||^2 = ||u||^2 + ||d||^2 - 2\\cdot||u||\\cdot ||d||\\cdot\\cos(\\theta).$$\n",
    "\n",
    "On the other hand using the relationship between the length of a vector and the dot product, we can write the following <br>\n",
    "\n",
    "$$\\begin{align*}\n",
    "||u-d||^2 = \\langle u-d,u-d \\rangle\n",
    "\\end{align*}$$\n",
    "\n",
    "Using the bilinearity and commutativity of the dot product we can continue by <br><br>\n",
    "$$\\begin{align*}\n",
    "||u-d||^2 = \\langle u-d,u-d \\rangle = \\langle u, u\\rangle - \\langle d,u \\rangle - \\langle u,d \\rangle - \\langle d, d \\rangle = ||u||^2 + ||d||^2- 2\\langle d,u \\rangle\n",
    "\\end{align*}$$\n",
    "\n",
    "Summing up what did we obtain until now <br><br>\n",
    "$$\\left\\{\n",
    "\\begin{align*}\n",
    "||u-d||^2 = ||u||^2 + ||d||^2 - 2\\cdot||u||\\cdot ||d||\\cdot\\cos(\\theta)\\\\\n",
    "||u-d||^2 = ||u||^2 + ||d||^2- 2\\langle d,u \\rangle\n",
    "\\end{align*}\n",
    "\\quad \\right|\n",
    "\\Rightarrow  \\quad\n",
    "||u||^2 + ||d||^2 - 2\\cdot||u||\\cdot ||d||\\cdot\\cos(\\theta) = ||u||^2 + ||d||^2- 2\\langle d,u \\rangle\n",
    "$$\n",
    "<br>\n",
    "or equivalently\n",
    "$$ ||u||\\cdot ||d||\\cdot\\cos(\\theta) = \\langle d,u \\rangle.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.9. Scalar and vector projection\n",
    "\n",
    "**Scalar projection:** length of the resulting projection vector, namely <br>\n",
    "\n",
    "$$||\\pi_d(u)|| = cos(\\theta) \\cdot ||u|| $$\n",
    "\n",
    "where $\\theta$ is the angle of the vectors $d$ and $u$.\n",
    "\n",
    "Due the cosine rule that we have derived for the scalar product, we can substitute $\\cos(\\theta)$ by $\\frac{\\langle d,u\\rangle}{||u|| \\cdot ||d||}$ and we obtain the following formula for the length of the projection\n",
    "\n",
    "$$||\\pi_d(u)|| = \\frac{\\langle u, d \\rangle}{||u|| \\cdot ||d||} \\cdot ||u|| = \\frac{\\langle u,d \\rangle}{||d||}$$\n",
    "\n",
    "**Vector projection:** We have determined the magnite of the projection vector, the direction is given by the one of the vector $d$. These two characteristics do uniquely define the projection vector, thus we can write\n",
    "\n",
    "$$\\pi_d(u) = ||\\pi_d(u)|| \\frac{d}{||d||}= \\mathbf{\\frac{\\langle u, d\\rangle }{||d||} \\frac{d}{||d||}} = \\frac{d \\langle u, d\\rangle}{||d||^2} = \\frac{d \\langle d, u\\rangle}{||d||^2}= \\frac{d \\cdot (d^T \\cdot u)}{||d||^2}= \\frac{(d \\cdot d^T) \\cdot u}{||d||^2} = \\mathbf{\\frac{d \\cdot d^T}{||d||^2}\\cdot u}$$\n",
    "\n",
    "The projection matrix $\\frac{d \\cdot d^T}{||d||^2}$ in $\\mathbb{R}^n$ is an $n \\times n$-dimensional matrix.\n",
    "\n",
    "**Exersize**<br> \n",
    "Change the canonical basis to another orthogonal basis by scalar projection.\n",
    "\n",
    "<!--Ex. 2. In PCA we can project basically on the first eigenvector and on its orthogonal complement.-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.geogebra.org/classic/qhhqpmrt\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a24585cc0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/qhhqpmrt\", 1000, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.10. Basis of a vectorspace, linear independence of vectors\n",
    "\n",
    "--------\n",
    "**Definition of linear combination**\n",
    "\n",
    "A linear combination of the vectors $x^{(1)}, x^{(2)}, \\ldots, x^{(m)} \\mathbb{R}^n$ is a vector of $\\mathbb{R}^n$, which can be written in the form of \n",
    "\n",
    "$$\\lambda_1x^{(1)} + \\lambda_2 x^{(2)} + \\cdots \\lambda_m \\cdot x^{(m)},$$\n",
    "\n",
    "where $\\lambda_1, \\lambda_2, \\ldots, \\lambda_m$ are real valued coefficients.\n",
    "\n",
    "--------\n",
    "**Example**\n",
    "\n",
    "Consider the vectors $x^{(1)} = \\left(\n",
    "\\begin{array}{c}\n",
    "1 \\\\\n",
    "0\\\\\n",
    "2\n",
    "\\end{array}\n",
    "\\right)$ and \n",
    "$x^{(2)} = \\left(\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right).$\n",
    "\n",
    "Then \n",
    "\n",
    "$$2 x^{(1)} + 3x^{(2)}= 2\\left(\n",
    "\\begin{array}{c}\n",
    "1 \\\\\n",
    "0\\\\\n",
    "2\n",
    "\\end{array}\n",
    "\\right) + 3 \\left(\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right) = \\left(\n",
    "\\begin{array}{c}\n",
    "2 \\\\\n",
    "3\\\\\n",
    "4\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "is a linear combination of $x^{(1)}$ and $x^{(2)}$.\n",
    "\n",
    "-------\n",
    "**Definition of linear dependence**\n",
    "\n",
    "Let us consider a set of $m$ vectors $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$ in $\\mathbb{R}^n$. They are said to be linearly dependent if and only if there exist the not all zero factors $\\lambda_1, \\lambda_2, \\ldots, \\lambda_m \\in \\mathbb{R}$  such that<br>\n",
    "\n",
    "$$\\lambda_1x^{(1)} + \\lambda_2 x^{(2)} + \\cdots +\\lambda_m  x^{(m)} = \\mathbf{0}$$\n",
    "\n",
    "-------\n",
    "\n",
    "**Remark**<br>\n",
    "Observe that if $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$ are dependent, then for some not all zero factors $\\lambda_1, \\lambda_2, \\ldots, \\lambda_m \\in \\mathbb{R}$ it holds that<br>\n",
    "\n",
    "$$\\lambda_1x^{(1)} + \\lambda_2 x^{(2)} + \\cdots + \\lambda_mx^{(m)} = \\mathbf{0}$$\n",
    "\n",
    "We know that at least one of the factors is not zero, let us assume that $\\lambda_i$ is such a factor. This means that from the above equation we can express the vector $x_i$ as a linear combination of the others.\n",
    "\n",
    "-------\n",
    "**Definition of linear independence**\n",
    "\n",
    "$m$ vectors $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$ in $\\mathbb{R}^n$ are linearly independent if there exist no such factors $\\lambda_1, \\lambda_2, \\ldots, \\lambda_m \\in \\mathbb{R}$ for which <br>\n",
    "\n",
    "$$\\lambda_1x^{(1)} + \\lambda_2 x^{(2)} + \\cdots + \\lambda_m  x^{(m)} = \\mathbf{0}$$\n",
    "\n",
    "and where at least one factor is different of zero. \n",
    "\n",
    "**Alternative definition of linear independence**\n",
    "\n",
    "Equivalently $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$ in $\\mathbb{R}^n$ are linearly independent if and only if the equation <br>\n",
    "\n",
    "$$\\lambda_1x^{(1)} + \\lambda_2 x^{(2)} + \\cdots +\\lambda_m  x^{(m)} = \\mathbf{0}$$\n",
    "\n",
    "holds just for $\\lambda_1 = \\lambda_2 = \\cdots = \\lambda_m = 0.$\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "If we write the vectors from the above equation by their components, the above equation can be equivalently transformed to  \n",
    "$$\\begin{align}\n",
    "\\lambda_1 \\left(\n",
    "\\begin{array}{c}\n",
    "x_1^{(1)} \\\\\n",
    "x_2^{(1)}\\\\\n",
    "\\vdots\\\\\n",
    "x_n^{(1)}\n",
    "\\end{array}\n",
    "\\right) + \n",
    "\\lambda_2 \\left(\n",
    "\\begin{array}{c}\n",
    "x_1^{(2)} \\\\\n",
    "x_2^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "x_n^{(2)}\n",
    "\\end{array}\n",
    "\\right) + \\cdots +\n",
    "\\lambda_m \\left(\n",
    "\\begin{array}{c}\n",
    "x_1^{(m)} \\\\\n",
    "x_2^{(m)}\\\\\n",
    "\\vdots\\\\\n",
    "x_n^{(m)}\n",
    "\\end{array}\n",
    "\\right) &= \\mathbf{0}\\\\ \\\\\n",
    "\\Leftrightarrow \\quad \n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\lambda_1 x_1^{(1)} + \\lambda_2 x_1^{(2)} + \\cdots+ \\lambda_m x_1^{(m)}\\\\\n",
    "\\lambda_1 x_2^{(1)} + \\lambda_2 x_2^{(2)} + \\cdots+ \\lambda_m x_2^{(m)}\\\\\n",
    "\\vdots\\\\\n",
    "\\lambda_1 x_n^{(1)} + \\lambda_2 x_n^{(2)} + \\cdots+ \\lambda_m x_n^{(m)}\n",
    "\\end{array}\n",
    "\\right)\n",
    "&=\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right) \\\\ \\\\\n",
    "\\Leftrightarrow \\quad \n",
    "\\left(\n",
    "\\begin{array}{cccc}\n",
    "x_1^{(1)} & x_1^{(2)} & \\cdots & x_1^{(m)}\\\\\n",
    "x_2^{(1)} & x_2^{(2)} & \\cdots & x_2^{(m)}\\\\\n",
    "\\vdots\\\\\n",
    "x_n^{(1)} & x_n^{(2)} & \\cdots & x_n^{(m)}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\cdot\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\lambda_1\\\\\n",
    "\\lambda_2\\\\\n",
    "\\vdots\\\\\n",
    "\\lambda_n\n",
    "\\end{array}\n",
    "\\right)\n",
    "&=\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\\\ \\\\\n",
    "\\Leftrightarrow \\quad \n",
    "\\left(\n",
    "\\begin{array}{cccc}\n",
    "\\uparrow & \\uparrow & \\cdots & \\uparrow\\\\\n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)}\\\\\n",
    "\\downarrow & \\downarrow & \\cdots & \\downarrow\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\cdot\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\lambda_1\\\\\n",
    "\\lambda_2\\\\\n",
    "\\vdots\\\\\n",
    "\\lambda_n\n",
    "\\end{array}\n",
    "\\right)\n",
    "&=\n",
    "\\mathbf{0}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process of the above transformation we used tacitly the definition of the matrix product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.11. Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, Layout, Box, GridspecLayout\n",
    "from File4MCQ import create_multipleChoice_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = create_multipleChoice_widget(\"1. Question: What day of the week is it today the 1st of September 2020?\", ['a. Monday', 'b. Tuesday', 'c. Wednesday'],'b. Tuesday','[Hint]:')\n",
    "#test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Matrices\n",
    "\n",
    "------------\n",
    "**Definition of matrices, matrix addition, multiplication by a scalar of matrices**\n",
    "\n",
    "$n \\times m$-dimensional matrices are elements of the set $\\mathbb{R}^{n \\times m}$.\n",
    "\n",
    "We organise the elements of an $n \\times m$-dimensional matrix in $n$ rows and $m$ columns. \n",
    "\n",
    "For the notation of matrices we use often capital letters of the alphabet.\n",
    "\n",
    "For a matrix $X \\in \\mathbb{R}^{n \\times m}$ let us denote the element at the intersection of the $i$th row and $j$th column by $x_{i,j}$. Then we can define the matrix by its compenents in the following way\n",
    "\n",
    "$$X = \\left(\n",
    "\\begin{array}{cccc}\n",
    "x_{1,1} & x_{1,2} & \\cdots & x_{1,m}\\\\\n",
    "x_{2,1} & x_{2,2} & \\cdots & x_{2,m}\\\\\n",
    "\\vdots\\\\\n",
    "x_{n,1} & x_{n,2} & \\cdots & x_{n,m}\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "**Matrix addition** and **multiplication by a scalar** happens component-wise, exactly as in the case of vectors.\n",
    "The sum of the matrices $X = (x_{i,j})_{i=\\overline{1,n}, j = \\overline{1,m}} \\in \\mathbb{R}^{n \\times m}$ and $Y = (y_{i,j})_{i=\\overline{1,n}, j = \\overline{1,m}} \\in \\mathbb{R}^{n \\times m}$ is the matrix \n",
    "\n",
    "$$X+Y = (x_{i,j} + y_{i,j})_{i=\\overline{1,n}, j = \\overline{1,m}},$$ \n",
    "\n",
    "that is\n",
    "\n",
    "$$X+Y =\n",
    "\\left(\n",
    "\\begin{array}{cccc}\n",
    "x_{1,1} + y_{1,1} & x_{1,2} + y_{1,2} & \\cdots & x_{1,m} + y_{1,m}\\\\\n",
    "x_{2,1} + y_{2,1} & x_{2,2} + y_{2,2} & \\cdots & x_{2,m} + y_{2,m}\\\\\n",
    "\\vdots\\\\\n",
    "x_{n,1} + y_{n,1} & x_{n,2} + y_{n,2} & \\cdots & x_{n,m} + y_{n,m}\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "For $\\lambda \\in \\mathbb{R}$  and $X = (x_{i,j})_{i=\\overline{1,n}, j = \\overline{1,m}} \\in \\mathbb{R}^{n \\times m}$ we define the mutiplication of $X$ by the scalar $\\lambda$ as the matrix\n",
    "\n",
    "$$\\lambda X = (\\lambda x_{i,j})_{i=\\overline{1,n}, j = \\overline{1,m}}$$\n",
    "\n",
    "that is\n",
    "\n",
    "$$\\lambda X = \\left(\n",
    "\\begin{array}{cccc}\n",
    "\\lambda x_{1,1} & \\lambda x_{1,2} & \\cdots & \\lambda x_{1,m}\\\\\n",
    "\\lambda x_{2,1} & \\lambda x_{2,2} & \\cdots & \\lambda x_{2,m}\\\\\n",
    "\\vdots\\\\\n",
    "\\lambda x_{n,1} & \\lambda x_{n,2} & \\cdots & \\lambda x_{n,m}\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "-----------------\n",
    "**Remark**<br>\n",
    "The set $\\mathbb{R}^{n\\times m}$ with the field $\\mathbb{R}$ and the two, above defined operations (matrix addition and multiplication by a scalar) is a vector space. To check the necessary properties, the calculation happen in the same way as in case of the vectors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1. Matrix multiplication\n",
    "Matrix multiplication of a vector as a linear transformation that transforms basis vectors of the original space to basis vectors of the image space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Gauss elimination to solve a system of linear equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Inverse of a matrix by Gaussian elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4. The determinant of a $2\\times 2$ matrix\n",
    "as the volume of the paralellogram spanned by the colmn vectors of the original matrix.\n",
    "\n",
    "What means if the determinant is 0?\n",
    "\n",
    "Ex. for a 3x3 system with a multiple solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5. Rotation in a different coordinate system than the canonical one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6. Orthogonal matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.7. Gram-Schmidt orthogonalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.8 Reflection in $\\mathbb{R}^3$ w.r.t. a plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.9. Eigenvectors, eigenvalues\n",
    "\"eigen\" = \"characteristic\"\n",
    "Eigenvectors are the vectors, which are just scaled by a factor when appliying the matrix operation on them.\n",
    "Eigenvalues are the solutions of characteristic polynomial.\n",
    "\n",
    "Rotation in $\\mathbb{R}^2$: no eigenvector\n",
    "\n",
    "Rotation in $\\mathbb{R}^3$: the only eigenvector is the axis of rotation\n",
    "\n",
    "Scaling along one axis: 2 eigenvectors\n",
    "\n",
    "Identity matrix: Every vector is an eigenvector\n",
    "\n",
    "### 2.3.10. Diagonalisation by changing the basis to the eigenvectors\n",
    "Application to calculating the $n$th power of a matrix. We can be interested in this question when $T$ is s transition matrix encorporating the change that happens in one time unit. Then $T^n$ shows the change happening in $n$ time units. If we can find a basis, where the matrix is diagonal, then calculate its $n$th power and afterwards transform it back, it is easier than calculating the $n$th power of the original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_multipleChoice_widget' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f79ed3bf0b3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_multipleChoice_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1. Question: What day of the week is it today?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a. Monday'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b. Tuesday'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c. Wednesday'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b. Tuesday'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'[Hint]:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_multipleChoice_widget' is not defined"
     ]
    }
   ],
   "source": [
    "test = create_multipleChoice_widget(\"1. Question: What day of the week is it today?\", ['a. Monday', 'b. Tuesday', 'c. Wednesday'],'b. Tuesday','[Hint]:')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.11. Eigenvectors, eigenvalues\n",
    "\"eigen\" = \"characteristic\"\n",
    "Eigenvectors are the vectors, which are just scaled by a factor when appliying the matrix operation on them.\n",
    "Eigenvalues are the solutions of characteristic polynomial.\n",
    "\n",
    "Please associate the following transformations in $\\mathbb{R}^2$ to the number of eigenvectors that they have:\n",
    "1. Rotation in $\\mathbb{R}^2$\n",
    "2. Rotation in $\\mathbb{R}^3$\n",
    "3. Scaling along one axis\n",
    "4. Scaling along 2 axis\n",
    "5. Multiplication by the identity matrix\n",
    "\n",
    "Number of eigenvectors:<br>\n",
    "a. every vector is an eigenvector<br>\n",
    "b. the transformation has exactly 2 eigenvectors<br>\n",
    "c. the transformation has exactly one eigenvectors<br>\n",
    "d. the transformation can have none or two eigenvectors\n",
    "\n",
    "\n",
    "\n",
    "### 2.3.9. Diagonalisation by changing the basis to the eigenvectors\n",
    "Application to calculating the $n$th power of a matrix. We can be interested in this question when $T$ is s transition matrix encorporating the change that happens in one time unit. Then $T$^n shows the change happening in $n$ time units. If we can find a basis, where the matrix is diagonal, then calculate its $n$th power and afterwards transform it back, it is easier than calculating the $n$th power of the original matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Homework\n",
    "Read through the material of the first day and summarise the relevant formulas, notions on a cheat sheet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
