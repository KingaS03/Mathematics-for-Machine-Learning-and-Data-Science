{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/KingaS03/Mathematics-for-Machine-Learning-and-Data-Science/master)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KingaS03/Mathematics-for-Machine-Learning-and-Data-Science)\n",
    "\n",
    "<[ Calculus ](Calculus.ipynb)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Calculus & Statistics\n",
    "\n",
    "Agenda:\n",
    "* Mean, variance, covariance for one-dimensionsal data sets\n",
    "* Covariance matrix for higher dimensional data sets\n",
    "* Inner products\n",
    "* Projections\n",
    "* PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Basic concepts of descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------\n",
    "### 4.1.1 Mean, variance, standard deviation\n",
    "Instead of working directly with the data many times we want to characterise it by its mean, variance and correlation between the observed features.\n",
    "\n",
    "For a set of data points $X = \\{x_1, x_2, \\ldots, x_n\\}$ the mean or expected value of this data set is:\n",
    "$$E(X) = \\frac{x_1 + x_2 + \\cdots + x_n}{n} \\quad \\mbox{or shorter} \\quad E(X) = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i.$$\n",
    "\n",
    "<!---Calculate the mean value of the next two data points: ..... . \n",
    "\n",
    "If you have calculated correctly, they have the same mean value. To differentiate among them, we can calculate their variance, which is a measure of average square distance from the mean value.-->\n",
    "\n",
    "The average square distance from the mean is\n",
    "$$\\frac{(x_1 - \\mu)^2 + (x_2-\\mu)^2 + \\cdots + (x_n - \\mu)^2}{n} = \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i - \\mu)^2, \\quad \\mbox{where} \\quad \\mu = E(X).$$\n",
    "\n",
    "This value above is denoted by ${\\rm Var}(X)$ and it is called the variance of the data points $X = \\{x_1, x_2, \\ldots, x_n\\}$. A common notation for it is also $\\sigma^2$.\n",
    "\n",
    "<!---Calculate the variance for the above two data points.\n",
    "\n",
    "One can conclude that the spread of ..... data points is larger than the spread of the ....... data points.-->\n",
    "\n",
    "If the data is measured in units, then observe that the measurement unit for the variance is square units. \n",
    "\n",
    "The standard deviation is obtained from the variance by calculating the square root of it, i.e.\n",
    "$$\\sigma = \\sqrt{{\\rm Var}(X)}.$$\n",
    "\n",
    "The above formula explains also the $\\sigma^2$ notation in case of the variance.\n",
    "\n",
    "**Remarks:** \n",
    "* If the data $X = \\{x_1, x_2, \\ldots, x_n\\}$ is centered (i.e. $E(X) = \\mu = 0$), then the variance can be calculated as\n",
    "\n",
    "$${\\rm Var}(X) = \\frac{x_1^2 + x_2^2 + \\cdots + x_n^2}{n}$$\n",
    "\n",
    "or if $x = \\left(\\begin{array}{c}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right)$ is the column vector containing all the observations, then $${\\rm Var}(X) = \\frac{1}{n}x^T \\cdot x = \\frac{1}{n}\\left(\\begin{array}{cccc}\n",
    "x_1 &\n",
    "x_2 & \n",
    "\\ldots &\n",
    "x_n\n",
    "\\end{array}\\right) \\cdot \n",
    "\\left(\\begin{array}{c}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right).$$\n",
    "\n",
    "* Any data $X = \\{x_1, x_2, \\ldots, x_n\\}$ can be centered by extracting the mean of the data from each element, i.e. the data $X' = \\{x'_1 = x_1 - \\mu,x'_2 = x_2 - \\mu, \\ldots, x'_n = x_n - \\mu\\}$ is centered. \n",
    "* For not centered data organised into a column vector $x = \\left(\\begin{array}{c}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right)$ the variance can be calculated in the following way by matrix operations: \n",
    "\n",
    "$${\\rm Var}(X) = \\frac{1}{n}(x-\\mu)^T \\cdot (x-\\mu) = \\left(\\begin{array}{cccc}\n",
    "x_1-\\mu &\n",
    "x_2-\\mu & \n",
    "\\ldots &\n",
    "x_n-\\mu\n",
    "\\end{array}\\right) \\cdot \n",
    "\\left(\\begin{array}{c}\n",
    "x_1-\\mu\\\\ \n",
    "x_2-\\mu\\\\ \n",
    "\\vdots\\\\\n",
    "x_n-\\mu\n",
    "\\end{array}\\right), \\quad \\mbox{where} \\quad \\mu = E(X).$$\n",
    "\n",
    "-------------\n",
    "\n",
    "### 4.1.2. Higher dimensional variance, covariance\n",
    "The generalisation of the variance to higher dimensional datasets.\n",
    "\n",
    "If we observe more features at the same time (i.e. we deal with higher dimensional data), then we can calculate the variance along each dimension.\n",
    "\n",
    "<!---Consider the following two datasets and calculate for both of them the mean and variance along $X$ and the variance along $Y$.\n",
    "\n",
    "If you have calculated correctly, then the two datasets have the same variance and same mean values along both features. However, if me plot them, we can see that they are different. To capture a difference we can calculate their covariance, as well. -->\n",
    "\n",
    "--------\n",
    "**Definition**\n",
    "\n",
    "The covariance of two observed features $X$ and $Y$ based on the observed value pairs $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$ is calulated by the following formula:\n",
    "\n",
    "$${\\rm Cov}(X,Y) = E\\left((X - \\mu_x)(Y-\\mu_y)\\right) = \\frac{(x_1-\\mu_x)(y_1-\\mu_y) + (x_2-\\mu_x)(y_2-\\mu_y) + \\cdots + (x_n-\\mu_x)(y_n-\\mu_y)}{n} = \\frac{1}{n} \\sum\\limits_{i=1}^n (x_i-\\mu_x)(y_i-\\mu_y),$$\n",
    "\n",
    "where $\\mu_x = E(X)$ and $\\mu_y = E(Y)$.\n",
    "\n",
    "--------------\n",
    "\n",
    "**Remark**\n",
    "\n",
    "1. The covariance of $X$ and $Y$ with matrix operations is\n",
    "\n",
    "$${\\rm Cov}(X,Y) = (X-\\mu_x)^T \\cdot (Y - \\mu_y), \\quad \\mbox{where} \\quad x = \\left(\\begin{array}{c}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right), y = \\left(\\begin{array}{c}\n",
    "y_1\\\\ \n",
    "y_2\\\\ \n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{array}\\right), \\mu_x = E(X) \\mbox{ and }\\mu_y = E(Y)$$\n",
    "\n",
    "\n",
    "2. What is the relationship between ${\\rm Cov}(X,Y)$ and ${\\rm Cov}(Y,X)$? \n",
    "$${\\rm Cov}(X,Y)\\ ???\\ {\\rm Cov}(Y,X)$$\n",
    "\n",
    "\n",
    "--------------\n",
    "\n",
    "**Interpretation of the covariance**\n",
    "* If ${\\rm Cov}(X,Y) > 0$, then the values of $Y$ increase on average when the values of $X$ increase.\n",
    "* If ${\\rm Cov}(X,Y) < 0$, then the values of $Y$ decrease on average when the values of $X$ increase.\n",
    "* If ${\\rm Cov}(X,Y) = 0$, then there is no observable linear trend between $X$ and $Y$.\n",
    "\n",
    "-----------\n",
    "\n",
    "**Definition**\n",
    "\n",
    "For a two-dimensional data $D = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}$ we can calculate the following 4 values: ${\\rm Var}(X),{\\rm Var}(Y),{\\rm Cov}(X,Y)$ and ${\\rm Cov}(Y,X)$. The covariance matrix of this two-dimensional data is:\n",
    "\n",
    "$${\\rm Var}(D) = \\left(\\begin{array}{cc}\n",
    "{\\rm Var}(X) & {\\rm Cov}(X,Y)\\\\\n",
    "{\\rm Cov(Y,X)} & {\\rm Var(Y)}\n",
    "\\end{array}\\right).$$\n",
    "\n",
    "---------------\n",
    "**Flash questions**\n",
    "\n",
    "<p>\n",
    "1. How does look like the covariance matrix of a three-dimensional data $D = \\{(x_1, y_1, z_1), (x_2, y_2, z_2), \\ldots, (x_n, y_n, z_n)\\}$?\n",
    "\n",
    "<!--\n",
    "It is the following $3 x 3$ matrix: ${\\rm Var}(D) = \\left(\n",
    "\\begin{array}{ccc}\n",
    "{\\rm Var}(X) & {\\rm Cov(X,Y)} & {\\rm Cov(X,Y)}\\\\\n",
    "{\\rm Cov}(Y,X) & {\\rm Var(Y)} & {\\rm Cov(Y,Z)}\\\\\n",
    "{\\rm Cov}(Z,X) & {\\rm Cov(Z,Y)} & {\\rm Var(Z)}\n",
    "\\end{array}\\right)$\n",
    "</p>\n",
    "-->\n",
    "  \n",
    "<p>  \n",
    "2. How can one calculate the covariance matrix with the help of matrix operations?\n",
    "\n",
    "<!--\n",
    "Due to the fact that the variance and covariance can be calculated as \n",
    "<p type='vertical-padding:0.5cm'>\n",
    "    $${\\rm Var}(X) = (X-\\mu_x)^T \\cdot (X-\\mu_x) = \\langle X-\\mu_x, X-\\mu_x \\rangle, \\qquad {\\rm Cov}(X,Y) = (X-\\mu_x)^T \\cdot (Y-\\mu_y) = \\langle X-\\mu_x, Y-\\mu_y \\rangle,$$\n",
    "</p>\n",
    "    \n",
    "the variance of the 3 dimensional dataset $D$ from the previous exercise can be expressed as\n",
    "<p type='vertical-padding:0.5cm'>\n",
    "$${\\rm Var}(D) = \\left(\n",
    "\\begin{array}{ccc}\n",
    "{\\rm Var}(X) & {\\rm Cov(X,Y)} & {\\rm Cov(X,Y)}\\\\\n",
    "{\\rm Cov}(Y,X) & {\\rm Var(Y)} & {\\rm Cov(Y,Z)}\\\\\n",
    "{\\rm Cov}(Z,X) & {\\rm Cov(Z,Y)} & {\\rm Var(Z)}\n",
    "\\end{array}\\right) = \n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "\\langle X-\\mu_x, X-\\mu_x \\rangle & \\langle X-\\mu_x, Y-\\mu_y \\rangle & \\langle X-\\mu_x, Z-\\mu_z \\rangle\\\\\n",
    "\\langle Y-\\mu_y, X-\\mu_x \\rangle & \\langle Y-\\mu_y, Y-\\mu_y \\rangle & \\langle Y-\\mu_y, Z-\\mu_z \\rangle\\\\\n",
    "\\langle Z-\\mu_z, X-\\mu_x \\rangle & \\langle Z-\\mu_z, Y-\\mu_y \\rangle & \\langle Z-\\mu_z, Z-\\mu_z \\rangle\n",
    "\\end{array}\\right),$$\n",
    "</p>\n",
    "\n",
    "which by the definition of the matrix product is equal to \n",
    "<p type='vertical-padding:0.5cm'>\n",
    "$$\\left(\n",
    "\\begin{array}{ccc}\n",
    "\\leftarrow & X-\\mu_x & \\rightarrow\\\\\n",
    "\\leftarrow & Y-\\mu_y & \\rightarrow\\\\\n",
    "\\leftarrow & Z-\\mu_x & \\rightarrow\\\\\n",
    "\\end{array}\\right) \\cdot \\left(\\begin{array}{ccc}\n",
    "\\uparrow & \\uparrow & \\uparrow\\\\\n",
    "X-\\mu_x & Y-\\mu_y & Z-\\mu_x\\\\\n",
    "\\downarrow & \\downarrow & \\downarrow\\\\\n",
    "\\end{array}\\right)$$\n",
    "<p>\n",
    "-->\n",
    "<p>\n",
    "\n",
    "------------\n",
    "**Remark**\n",
    "\n",
    "The covariance is measured in ${\\rm unit}_1 \\cdot {\\rm unit}_2$ if the measurement unit of the values in data set $X$ \n",
    "is ${\\rm unit}_1$ and the measurement unit of the values in data set $Y$ is ${\\rm unit}_2$. Depending on our choice of measuremeant units, the covariance is going to be larger or smaller. For example if both sets of our measurements are expressed in $m$ and the covariance of them is $25$, then changing our measurement unit to $cm$, would yield a covariance of $25*100^2 = 250.000$. Both covariances are characterising the same dataset. How to interpret them? How to compare two covariances if they come from different data sets?\n",
    "\n",
    "The correlation is an answer to all these questions:\n",
    "\n",
    "-------------\n",
    "\n",
    "**Definition**\n",
    "\n",
    "The correlation of two observed features $X$ and $Y$ based on the observed value pairs $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$ is calulated by the following formula:\n",
    "\n",
    "$${\\rm Corr}(X,Y) = \\frac{{\\rm Cov}(X,Y)}{\\sigma_X \\cdot \\sigma_Y}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "### 4.1.3. The effect of linear transformations on the mean and on the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As start, please take a short quiz by running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone \"https://github.com/KingaS03/Mathematics-for-Machine-Learning-and-Data-Science.git\"\n",
    "cd \"Mathematics-for-Machine-Learning-and-Data-Science\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'questions/questions1.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we generalise what we have observed?\n",
    "\n",
    "\\begin{align*}\n",
    "E(X + c) = c+ E(X) \\\\[0.5em]\n",
    "E(cX) = c E(X)\\\\[0.5em]\n",
    "E(aX + b) = aE(X)+b\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the test in the below field!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'questions/questions2.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we say about the variance of a one-dimensional data set $X$ in general?\n",
    "\n",
    "\\begin{align*}\n",
    "{\\rm Var}(X + c) = {\\rm Var}(X)\\\\[0.5em]\n",
    "{\\rm Var}(cX) = c^2{\\rm Var}(X)\\\\[0.5em]\n",
    "{\\rm Var}(aX + b) = a^2{\\rm Var}(X)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the covariance matrix?\n",
    "\n",
    "\\begin{align*}\n",
    "{\\rm Var}(AD + c) = A \\cdot {\\rm Var}(D) \\cdot A^T,\n",
    "\\end{align*}\n",
    "\n",
    "where $D = \\{(x_1, x_2, \\ldots, x_d)^T\\}$, $A \\in \\mathbb{R}^{dxd}$ and $c \\in \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "## 4.2 Descriptive statistics and linear algebra\n",
    "\n",
    "### 4.2.1 The inner product\n",
    "\n",
    "The inner product is a generalisation of the dot product or Euclidean scalar product, that we have introduced in Chapter 2. Once we have an inner product, we are able to calculate lengths (distances) and angles.\n",
    "\n",
    "-----------\n",
    "**Definition**\n",
    "\n",
    "On a vector space $V$ the inner product is a **symmetric, positive definite, bilinear** mapping $\\langle \\cdot, \\cdot \\rangle: V \\times V \\mapsto \\mathbb{R}$.\n",
    "\n",
    "* symmetry holds if $\\langle x, y\\rangle = \\langle y, x \\rangle$ for all $x, y \\in V$\n",
    "* positive definiteness holds if $\\langle x, x \\rangle > 0$ for all $x \\in V$, where $x \\neq \\mathbf{0}$\n",
    "* bilinearity holds if \n",
    "\\begin{align*}\n",
    "\\langle c_1 \\cdot x^{(1)} + c_2 \\cdot x^{(2)}, y \\rangle = c_1 \\langle x^{(1)}, y \\rangle + c_2 \\langle x^{(2)}, y \\rangle, \\quad \\mbox{for all} \\quad c_1, c_2 \\in \\mathbb{R}, x^{(1)}, x^{(2)}, y \\in \\mathbb{R}^n\\\\\n",
    "\\langle x, c_1 \\cdot y^{(1)} + c_2 \\cdot y^{(2)} \\rangle = c_1 \\langle x, y^{(1)} \\rangle + c_2 \\langle x, y^{(2)} \\rangle \\quad \\mbox{for all} \\quad c_1, c_2 \\in \\mathbb{R}, x, y^{(1)}, y^{(2)} \\in \\mathbb{R}^n\n",
    "\\end{align*}\n",
    "\n",
    "-------------\n",
    "**Definition**\n",
    "A matrix $A \\in \\mathbb{R}^{n \\times n}$ is positive definite if $x^T \\cdot A \\cdot x >0$ for all $x \\in \\mathbb{R}$.\n",
    "\n",
    "---------------\n",
    "**Sylvester's criterion**\n",
    "A symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is positive definite iff all the leading principal minors are positive, i.e. if all the following matrices have positive determinants:\n",
    "* the upper left $1\\times 1$ corner of $A$,\n",
    "* the upper left $2\\times 2$ corner of $A$,\n",
    "* the upper left $3\\times 3$ corner of $A$,\n",
    "* ...\n",
    "* $A$ itself.\n",
    "\n",
    "-----------\n",
    "**Remarks**\n",
    "\n",
    "1. For our context it suffices to consider for $V$ the $n$-dimensional real vector space $\\mathbb{R}^n$. \n",
    "2. $V = \\mathbb{R}^n$ for every inner product $\\langle \\cdot, \\cdot \\rangle$ defined on $V \\times V$ there exists a symmetric, positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$ such that \n",
    "$$\\langle x, y \\rangle = x^T \\cdot A \\cdot y, \\quad \\mbox{for every} \\quad x,y \\in \\mathbb{R}^n.$$\n",
    "3. Observe that the dot product introduced in Chapter 2 satisfies the general definition of an inner product, therefore the inner product can be seen as a generalisation of the dot product.\n",
    "\n",
    "The dot product can be written as\n",
    "$$\\langle x, y \\rangle = x^T \\cdot I_n \\cdot y,$$\n",
    "where $I_n$ is the $n \\times n$-dimensional identity matrix.\n",
    "\n",
    "--------------\n",
    "**Definitions**\n",
    "\n",
    "1. We say that two vectors $x, y \\in \\mathbb{R}^n$ are orthogonal to each other w.r.t. the considered inner product, if their inner product $\\langle x, y \\rangle$ is $0$. \n",
    "2. In a right triangle the side opposite to the right angle is called hypotenuse.\n",
    "3. The cosine of an angle in a right triangle is the ratio of the leg next to the angle and of the hypotenuse. Furthermore, $\\cos(\\alpha) = - \\cos(\\pi-\\alpha)$.\n",
    "4. For a vector $x \\in \\mathbb{R}^n$ we introduce the $||x||^2$ notation for the value $\\langle x, x\\rangle$ and we call the value $||x|| = \\sqrt{\\langle x, y \\rangle}$ the length of the  vector $x$.\n",
    "5. For two vectors $x, y \\in \\mathbb{R}^n$ we introduce the notation $d(x,y)$ for the value $||x-y||$. The properties of the inner product assure that the so defined function $d: \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}_{\\geq 0}$ is a distance (or with other words metric) in the mathematical sense, i.e. the following properties are satisfied by it for all $x,y, z \\in \\mathbb{R}^n$:\n",
    "\n",
    "    \\begin{align*}\n",
    "    & (1)\\ \\mbox{identity of indiscernibles:} \\quad d(x,y) = 0 \\ \\Leftrightarrow \\ x = y\\\\[0.5em]\n",
    "    & (2)\\ \\mbox{symmetry:} \\quad d(x,y) = d(y,x)\\\\[0.5em]\n",
    "    & (3)\\ \\mbox{triangle inequality:} \\quad d(x,y) \\leq d(x,z) + d(z,y)\n",
    "    \\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"https://www.geogebra.org/classic/qhxyf5cj\", 900, 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Remarks**\n",
    "\n",
    "\n",
    "1. The above defined inner product can be also interpreted as the inner product of functions defined on a finite set of points. The notion of inner product can be extended also to continuous functions. For example for two continuous functions $f: [a,b] \\to \\mathbb{R}$ and $g: [a,b] \\to \\mathbb{R}$ their inner product can be defined like:\n",
    "\n",
    "$$\\langle f, g \\rangle = \\int_{a}^{b} f(x)g(x)\\, dx.$$\n",
    "\n",
    "In this space of functions an integrable function $f$ defined on the interval $[-\\pi,\\pi]$ can be approximated by its projection to a subspace spanned by the orthonormal functions $1, \\cos(x), \\cos(2x), \\ldots, \\cos(nx), \\sin(x), \\sin(2x), \\ldots, \\sin(nx)$. \n",
    "\n",
    "The projection of $f$ to ${\\rm span}\\{1, \\cos(x), \\cos(2x), \\ldots, \\cos(nx), \\sin(x), \\sin(2x), \\ldots, \\sin(nx)\\}$ is the $n$-th Fourier series.\n",
    "\n",
    "2. With the above definition of orthogonality, in a right triangle the Pythagorean theorem and the more general law of cosines also hold, i.e.\n",
    "\\begin{align*}\n",
    "    &||x-y||^2 = ||x||^2 + ||y||^2, \\quad \\mbox{if} \\quad \\langle x,y \\rangle = 0\\\\[0.5em]\n",
    "    &||x-y||^2 = ||x||^2 + ||y||^2 - 2\\cdot\\cos(\\theta_{x,y})\\cdot||x||\\cdot||y||,\n",
    "\\end{align*}\n",
    "where $\\theta_{x,y}$ denotes the angle between the vectors $x$ and $y$. \n",
    "\n",
    "As we did in the linear algebra part, we can conclude, that\n",
    "\n",
    "$$\\cos(\\theta_{x,y}) = \\frac{\\langle x, y\\rangle}{||x|| \\cdot ||y||}$$\n",
    "\n",
    "3. Observe that if you think of a 1-dimensional centered data set $X = \\{x_1, x_2, \\ldots, x_n\\}$ as a column vector \n",
    "\n",
    "$$x = \\left(\n",
    "\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\n",
    "\\right)\\in \\mathbb{R},$$ \n",
    "\n",
    "then \n",
    "\n",
    "$${\\rm Var}(X) = \\frac{1}{n}x^T \\cdot x = \\frac{1}{n}\\langle x, x \\rangle = \\frac{1}{n}||x||^2 \\quad \\mbox{and} \\quad \\sigma_X = \\sqrt{{\\rm Var}(X)} = ||x||$$\n",
    "\n",
    "\n",
    "For a 2-dimensional centered data set $\\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}$ it holds that \n",
    "\n",
    "\\begin{align*}\n",
    "&{\\rm Cov}(X,Y) = x^T \\cdot y = \\langle x, y \\rangle\\\\[0.5em]\n",
    "&{\\rm Corr}(X,Y) = \\frac{{\\rm Cov}(X,Y)}{\\sigma_X \\cdot \\sigma_Y} = \\frac{\\langle x, y \\rangle}{||x|| \\cdot ||y||} = {\\rm cos}(X,Y)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Applications \n",
    "\n",
    "------------------------\n",
    "\n",
    "We load the images of digits from the well-known MNIST data set and we create a function to calculate distances between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2-rc3\n",
      "  Using cached tensorflow-2.2.0rc3-cp36-cp36m-macosx_10_11_x86_64.whl (175.3 MB)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (0.11.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (2.2.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (2.2.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (0.35.1)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.15.2-cp36-cp36m-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.19.5)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.32.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (3.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.6.3)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (0.3.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.15.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (0.4.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (3.2.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (1.27.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (49.6.0.post20200814)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (1.0.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (2020.6.20)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (1.7.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (0.2.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (0.4.6)\n",
      "Installing collected packages: protobuf, tensorflow\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 1.8.0\n",
      "    Uninstalling tensorflow-1.8.0:\n",
      "      Successfully uninstalled tensorflow-1.8.0\n",
      "Successfully installed protobuf-3.15.2 tensorflow-2.2.0rc3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.2-rc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "#from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = train_images.astype(np.double)\n",
    "labels = train_labels.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the following code do? Modify it in such a way to plot out a 7 instead of a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[labels==1].reshape(-1, 28, 28)[0], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and plot the average digit of $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.average(images[labels==1].reshape(-1, 28, 28),0), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider our images as vectors and calculate their distance and angle, by using the Euclidean metric on the space of image vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x0, x1):\n",
    "    \"\"\"Compute distance between two vectors x0, x1 using the dot product\"\"\"\n",
    "    distance = np.sum((x1-x0)*(x1-x0))**0.5 # <-- EDIT THIS to compute the distance between x0 and x1\n",
    "    return distance\n",
    "\n",
    "def angle(x0, x1):\n",
    "    \"\"\"Compute the angle between two vectors x0, x1 using the dot product\"\"\"\n",
    "    angle = np.arccos(np.sum(x0*x1)/(distance(x0,0)*distance(x1,0))) # <-- EDIT THIS to compute angle between x0 and x1\n",
    "    return angle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an image of a digit. Plot it. Find the one, which is closest to it among the next 1000 images. Plot this second image, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682\n"
     ]
    }
   ],
   "source": [
    "# scratch cell\n",
    "mindist = 100000\n",
    "first = 2\n",
    "second2remember = 0\n",
    "for second in range(1000):\n",
    "    f = images[first].reshape(28, 28)\n",
    "    s = images[second].reshape(28, 28)\n",
    "    d = distance(f.ravel(), s.ravel())\n",
    "    if 0 < d < mindist:\n",
    "        mindist = d\n",
    "        second2remember = second\n",
    "print(second2remember)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0].reshape(28, 28), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[832].reshape(28, 28), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Projections\n",
    "\n",
    "----------------\n",
    "\n",
    "### 4.4.1 Projection to a multi-dimensional space\n",
    "\n",
    "**Definition**\n",
    "In the real vector space $\\mathbb{R}^n$ the projection of a vector $x \\in\\mathbb{R}^n$ to a subspace with basis $B = \\{b_1, b_2, \\ldots, b_m\\} \\subset \\mathbb{R}^n$ is defined to be that element $\\pi_B(x) \\in {\\rm span}(B)$, for which $||x - \\pi_B(x)||$ is minimal. \n",
    "\n",
    "--------\n",
    "**Remark**\n",
    "By definition the projection of $x \\in \\mathbb{R}^n$ to $B = \\{b_1, b_2, \\ldots, b_m\\} \\subset \\mathbb{R}^n$ is the best approximation of the vector that can be spanned by $B$.\n",
    "\n",
    "----------------\n",
    "\n",
    "**Claim**\n",
    "For the above defined projection it holds that \n",
    "$$ x - \\pi_B(x)\\ \\bot\\ {\\rm span}(B)$$\n",
    "\n",
    "*Proof*\n",
    "\n",
    "Let $y \\in {\\rm span}(B)$ such that $x - y \\ \\bot\\ {\\rm span}(B)$.\n",
    "\n",
    "For any $b \\in {\\rm span}(B)$ it holds, that\n",
    "\n",
    "\\begin{align*}\n",
    "    ||x - b||^2 &= ||(x - y) + (y - b)||^2\\\\[0.5em]\n",
    "    & = ||x-y||^2 + ||y-b||^2 + 2 \\underbrace{\\langle x-y, y-b\\rangle}_{ 0, \\mbox{ as } y-b\\ \\in\\ {\\rm span}(B)}\\\\[0.5em]\n",
    "    &\\geq ||x-y||^2\n",
    "\\end{align*}\n",
    "\n",
    "To complete the proof, we will show that the conditions\n",
    "\\begin{align*}\n",
    "    &(1) \\quad  y \\in {\\rm span}(B)\\\\\n",
    "    &(2) \\quad x-y \\ \\bot \\ {\\rm span}(B)\n",
    "\\end{align*}\n",
    "do uniquely determine the vector $y$.\n",
    "\n",
    "Condition (1) means that there exist the coefficients $\\lambda_1, \\lambda_2, \\ldots, \\lambda_m$ such that \n",
    "\n",
    "$$y = \\lambda_1 b_1 + \\lambda_2 b_2 +  \\cdots + \\lambda_m b_m$$\n",
    "\n",
    "Condition (2) means that the vector $x- y$ is perpendicular to every $b \\in {\\rm span}(B)$, so particularly also to the elements $b_i$ of the basis $B$, i.e.\n",
    "\n",
    "$$\\langle x-y, b_i \\rangle = 0, \\quad \\forall i \\in \\{1,2, \\ldots, m\\}.$$\n",
    "\n",
    "Let us substitute in the above formula $y = \\lambda_1 b_1 + \\lambda_2 b_2 +  \\cdots + \\lambda_m b_m$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle b_i, x - (\\lambda_1 b_1 + \\lambda_2 b_2 +  \\cdots + \\lambda_m b_m) \\rangle= 0\\\\[0.5em]\n",
    "\\langle b_i, x - B\\cdot \\lambda \\rangle = 0, \\quad \\forall i \\in \\{1,2, \\ldots, m\\},\n",
    "\\end{align*}\n",
    "\n",
    "where by $B$ we denote the matrix containing on its columns the basis vectors $b_1, b_2, \\ldots b_m$, i.e. $B = [b_1|b_2|\\ldots|b_m]$ and $\\lambda = \\left(\n",
    "\\begin{array}{c}\n",
    "\\lambda_1\\\\\n",
    "\\lambda_2\\\\\n",
    "\\ldots\\\\\n",
    "\\lambda_m\n",
    "\\end{array}\n",
    "\\right)$.\n",
    "\n",
    "Thus the last equation can be rewritten equivalently as\n",
    "\n",
    "\\begin{align*}\n",
    "b_i^T \\cdot (x - B \\cdot \\lambda) = 0, \\quad \\forall i \\in \\{1,2, \\ldots, m\\}\n",
    "\\end{align*}\n",
    "\n",
    "These $m$ equations can be merged together and written in the following form\n",
    "\n",
    "\\begin{align*}\n",
    "&B^T \\cdot (x - B\\cdot \\lambda) = \\mathbf{0}\\\\[0.5em]\n",
    "&B^T \\cdot x - B^T \\cdot B \\cdot \\lambda = \\mathbf{0}\\\\[0.5em]\n",
    "&B^T \\cdot B \\cdot \\lambda = B^T \\cdot x\\\\[0.5em]\n",
    "&\\lambda = \\left(B^T\\cdot B\\right)^{-1} B^T \\cdot x \\quad \\Rightarrow \\quad y = B \\cdot \\underbrace{\\left(B^T\\cdot B\\right)^{-1} B^T \\cdot x}_{\\lambda}\n",
    "\\end{align*}\n",
    "\n",
    "-------------------------\n",
    "\n",
    "**Remark**\n",
    "\n",
    "<p>\n",
    "1. In the proof of the above claim we have also derived the projection formula\n",
    "\n",
    "$$\\pi_{B}(x) = B \\cdot \\left(B^T\\cdot B\\right)^{-1} B^T \\cdot x.$$\n",
    "\n",
    "Observe that this is an extension of the formula that we have obtained in Chapter 2 for projecting on a single line $b$:\n",
    "\n",
    "$$\\pi_{b}(x) = \\frac{b \\cdot b^T}{||b||^2} \\cdot x.$$\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "2. In the special case when $b_1, b_2, \\ldots, b_m$ form an orthonomal set of vectors, then the $B^T \\cdot B$ matrix appearing in the projection formula will be the unit matrix $I_m$. Indeed, the element on the posiion $(i,j)$ of the product matrix $B^T \\cdot B$ can be calculated as $\\langle b_i, b_j \\rangle = b_i^T \\cdot b_j$, which is $1$ for $i = j$ and is $0$ otherwise, due to orthonormality of the set of vectors $\\{b_1, b_2, \\ldots, b_m\\}$.\n",
    "\n",
    "Thus in such a case $\\pi_B(x) = B \\cdot \\underbrace{B^T \\cdot x}_{\\lambda}$.\n",
    "\n",
    "In the particular case, when $B = \\{b\\}$ and $||b|| = 1$, then $\\pi_B(x) = \\pi_b(x)= b \\cdot \\underbrace{b^T \\cdot x}_{\\lambda}$.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "3. Observe furthermore that when $\\{b_1, b_2, \\ldots, b_m\\}$ is an orthonomal set of vectors, then \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{\\pi_B(x)} &= B \\cdot \\underbrace{B^T \\cdot x}_{\\lambda} \\\\\n",
    "&= \\left(\n",
    "\\begin{array}{cccc}\n",
    "\\uparrow & \\uparrow & \\ldots & \\uparrow\\\\\n",
    "b_1 & b_2 & \\ldots & b_m\\\\\n",
    "\\downarrow & \\downarrow & \\ldots & \\downarrow\n",
    "\\end{array}\\right) \\cdot \\left( \\begin{array}{c}\n",
    "b_1^T \\cdot x\\\\\n",
    "b_2^T \\cdot x\\\\\n",
    "\\vdots\\\\\n",
    "b_m^T \\cdot x\n",
    "\\end{array}\\right)\\\\[0.5em] \n",
    "&= \\mathbf{b_1 \\cdot b_1^T \\cdot x + b_2 \\cdot b_2^T \\cdot x + \\cdots + b_m \\cdot b_m^T \\cdot x}\\\\[0.5em]\n",
    "&= \\mathbf{\\pi_{b_1}(x) + \\pi_{b_2}(x) + \\cdots +\\pi_{b_m}(x)}\n",
    "\\end{align*}\n",
    "\n",
    "In particular if $m = n$, then $x \\in {\\rm span}B$. To show this, let us denote the components of $x$ w.r.t. the basis $B$ by $\\beta_i$ ($i \\in \\{1, 2, \\ldots, n\\}$), i.e.\n",
    "\n",
    "$$x = \\beta_1 \\cdot b_1 + \\beta_2 \\cdot b_2 +  \\cdots + \\beta_n \\cdot b_n.$$\n",
    "\n",
    "Multiplying the above equality from the left by $b_1^T$ yields\n",
    "\n",
    "$$b_1^T \\cdot x = \\beta_1 \\cdot \\underbrace{b_1^T \\cdot b_1}_{ = 1} + \\beta_2 \\cdot \\underbrace{b_1^T \\cdot b_2}_{ = 0} + \\cdots + \\beta_n \\cdot \\underbrace{b_1^T \\cdot b_n}_{= 0}$$ \n",
    "\n",
    "Thus $\\beta_1 = b_1^T \\cdot x$ and $\\beta_1 \\cdot b_1 = \\underbrace{b_1^T \\cdot x}_{\\in \\mathbb{R}} \\cdot b_1 = b_1 \\cdot b_1^T \\cdot x = \\pi_{b_1}(x).$\n",
    "\n",
    "One can prove similarly for all $i \\in \\{1, 2, \\ldots, n\\}$ that \n",
    "\n",
    "$$\\beta_i = b_i^T \\cdot x \\quad \\mbox{and} \\quad \\beta_i \\cdot b_i = b_i \\cdot b_i^T \\cdot x = \\pi_{b_i}(x).$$\n",
    "\n",
    "Thus $x = b_1 \\cdot b_1^T \\cdot x + b_2 \\cdot b_2^T \\cdot x + \\cdots + b_n \\cdot b_n^T \\cdot x = \\pi_B(x)$.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 PCA\n",
    "\n",
    "**Step 1**\n",
    "\n",
    "Let us assume that we have chosen a sample of $n$ individuals, and for each of them we have recorded $d$ features. The features describing the $i$th individual we store in the vector denoted by $x_i \\in \\mathbb{R}^d$.\n",
    "\n",
    "For any orthonormal basis $\\{b_1, b_2, \\ldots, b_d\\} \\subset \\mathbb{R}^d$ of $\\mathbb{R}^d$ any element $x$ of $\\mathbb{R}^d$ can be written as:\n",
    "\n",
    "$$x = \\underbrace{b_1 \\cdot b_1^T \\cdot x + b_2 \\cdot b_2^T \\cdot x + \\cdots + b_m \\cdot b_m^T \\cdot x}_{\\pi_{B'}(x)} + b_{m+1} \\cdot b_{m+1}^T \\cdot x + \\cdots + b_d \\cdot b_d^T \\cdot x$$\n",
    "\n",
    "Our goal is to express the $x_1, x_2, \\ldots, x_n$ vectors in a lower dimensional space, therefore from the above exact representation of $x$ we want to keep just the sum of the first $m$ terms, which is exactly $\\pi_{B'}(x)$, where $B'$ denotes the smaller set of orthonormal vectors $\\{b_1, b_2, \\ldots, b_m\\}$.\n",
    "\n",
    "We will measure by the mean squared reconstruction error $f(b_1, b_2, \\ldots, b_m)$ how well can our data points $x_1, x_2,\\ldots, x_n$ be represented in the subspace spanned by $B' = \\{b_1, b_2, \\ldots, b_m\\}$: \n",
    "\n",
    "\\begin{align*}\n",
    "f(b_1, b_2, \\ldots, b_d) &= \\frac{1}{n} \\sum_{i=1}^n ||x_i - \\pi_{\\{b_1, \\ldots, b_m\\}}(x_i)||^2\\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^n ||\\pi_{\\{b_{m+1}, \\ldots, b_d\\}}(x_i)||^2\\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n || b_{m+1} \\cdot b_{m+1}^T \\cdot x_i + \\cdots + b_d \\cdot b_d^T \\cdot x_i||^2\\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n || b_{m+1} \\cdot (b_{m+1}^T \\cdot x_i) + \\cdots + b_d \\cdot (b_d^T \\cdot x_i)||^2\\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n (b_{m+1}^T \\cdot x_i)^2 + \\cdots + (b_{d}^T \\cdot x_i)^2\n",
    "\\end{align*}\n",
    "\n",
    "$f(b_1, b_2, \\ldots, b_d)$ is our function subject to minimisation. But before proceeding to determine the optimal \n",
    "orthonormal vectors $\\{b_1, b_2, \\ldots, b_d\\}$ we give a statistical interpretation to the above sum.\n",
    "\n",
    "<!---\n",
    "**Step 2**\n",
    "\n",
    "If we think of an $x \\in \\mathbb{R}^d$ as a vector represented w.r.t. $\\{b_1, b_2, \\ldots, b_d\\}$, i.e. $x = b_1 \\cdot (b_1^T \\cdot x) + b_2 \\cdot (b_2^T \\cdot x) + \\cdots b_d \\cdot (b_d^T \\cdot x)$ and we denote by $X$ the set of data points with values $b_1^T \\cdot x, b_2^T \\cdot x, \\ldots, b_d^T \\cdot x$, then\n",
    "\n",
    "$${\\rm Var}(X) = \\frac{1}{d}\\left((b_{1}^T \\cdot x)^2 + (b_{2}^T \\cdot x)^2 \\cdots + (b_{d}^T \\cdot x)^2\\right).$$\n",
    "\n",
    "If $X_i$ denotes the associated dataset to $x_i$, then\n",
    "$${\\rm Var}(X_i) = \\frac{1}{d}\\left((b_{1}^T \\cdot x_i)^2 + (b_{2}^T \\cdot x_i)^2 \\cdots + (b_{d}^T \\cdot x_i)^2\\right).$$\n",
    "\n",
    "$\\pi_{\\{b_{m+1}, \\ldots, b_d\\}}(x_i) \\in {\\rm span}\\{b_1, b_2, \\ldots, b_m\\} \\subset {\\rm span}\\{b_1, b_2, \\ldots, b_d\\}$. \n",
    "\n",
    "$$\\pi_{\\{b_{m+1}, \\ldots, b_d\\}}(x_i) = 0 \\cdot (b_1^T \\cdot x_i) + 0 \\cdot (b_2^T \\cdot x_i) + \\cdots + 0 \\cdot (b_m^T \\cdot x_i) + b_{m+1} \\cdot (b_{m+1}^T \\cdot x_i) + \\cdots + b_d \\cdot (b_d^T \\cdot x_i).$$ \n",
    "\n",
    "Analogously to how we proceeded before we can associate to the above projection also the set of data points $\\underbrace{0, 0, \\ldots, 0}_{m \\mbox{ times}}, b_{m+1}^T \\cdot x_i, \\ldots, b_{d}^T \\cdot x_i$, which we denote by $\\Pi_{\\{b_{m+1}, \\ldots, b_d\\}}(x_i)$. Then\n",
    "\n",
    "$${\\rm Var}\\left(\\Pi_{\\{b_{m+1}, \\ldots, b_d\\}}(x_i)\\right) = \\frac{1}{d}\\left((b_{m+1}^T \\cdot x_i)^2 + \\cdots + (b_{d}^T \\cdot x_i)^2\\right).$$\n",
    "\n",
    "By all the above reasoning $f(b_1, b_2, \\ldots, b_m) = \\frac{1}{n} \\sum_{i=1}^n (b_{m+1}^T \\cdot x_i)^2 + \\cdots + (b_{d}^T \\cdot x_i)^2$ is the average variance of the part of $x$, which is encoded in ${\\rm span}\\{b_{m+1}, \\ldots, b_d\\}$.-->\n",
    "\n",
    "**Step 2**\n",
    "\n",
    "We rewrite $f(b_1, b_2, \\ldots, b_m)$ as follows\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "f(b_1, b_2, \\ldots b_m) &=& \\frac{1}{n} \\sum_{i=1}^n \\sum_{k=m+1}^d (b_k^T x_i)^2\\\\\n",
    "&=& \\frac{1}{n} \\sum_{i=1}^n \\sum_{k=m+1}^d b_k^T x_i x_i^T b_k\\\\\n",
    "&=& \\frac{1}{n} \\sum_{k=m+1}^d b_k^T\\left(\\frac{1}{n} \\sum_{i=1}^n x_i x_i^T \\right) b_k\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Observe that $\\frac{1}{n} \\sum_{i=1}^n x_i x_i^T = \\frac{1}{n} X \\cdot X^T = {\\rm var(X)}$, where $X$ is the data matrix, the columns of which are the observations $x_1, x_2, \\ldots, x_n$. \n",
    "\n",
    "Thus $f(b_1, b_2, \\ldots b_d) = \\sum_{k=m+1}^d b_k^T \\cdot S \\cdot b_k = {\\rm Trace}\\left(\\sum_{k=m+1}^d (b_k \\cdot b_k^T) \\cdot S\\right)$, which is the total variance of the projected data to the complement of the principal space. Our goal is to find such an orthonormal basis $\\{b_1, b_2, \\ldots, b_d\\}$ for which the average reconstruction error $f(b_1, b_2, \\ldots, b_d)$ is minimal.\n",
    "\n",
    "**Step 3**\n",
    "We will minimize $f(b_1, b_2, \\ldots b_d) = \\sum_{k=m+1}^d b_k^T \\cdot S \\cdot b_k$ under the constraints that $b_{k}^T \\cdot b_k = 1$ for all $k \\in \\{m+1, m+2, \\ldots, d\\}$.\n",
    "\n",
    "We construct the Lagrange function:\n",
    "$$L = \\sum_{k=m+1}^d b_k^T \\cdot S \\cdot b_k - \\sum_{k=m+1}^d \\lambda_k (b_{k}^T \\cdot b_k-1),$$\n",
    "where $\\lambda_k \\in \\mathbb{R}$ for all $k \\in \\{m+1, \\ldots, d\\}$.\n",
    "\n",
    "We calculate the gradients of $f$ w.r.t. $b_{m+1}, \\ldots, b_d$ and we obtain:\n",
    "$$\\frac{\\partial f}{\\partial b_{m+1}} = 2 b_{m+1}^T\\cdot S - \\lambda_{m+1}b_{m+1}^T \\quad \\ldots \\quad \\frac{\\partial f}{\\partial b_{d}} = 2 b_{d}^T\\cdot S - \\lambda_{d}b_{d}^T.$$\n",
    "\n",
    "Setting all gradients to be $0$ and transposing the above equalities yields, that \n",
    "$$S b_{m+1} = \\lambda_{m+1}b_{m+1} \\quad \\ldots \\quad S b_{d} = \\lambda_{d}b_{d}.$$\n",
    "\n",
    "Therefore $b_{m+1}, \\ldots, b_{d}$ are eigenvectors of $S$ corresponding to the eigenvalues $\\lambda_{m+1}, \\ldots, \\lambda_d$, respectively.\n",
    "\n",
    "In this case the value of the average reconstruction error is \n",
    "$$f(b_1, b_2, \\ldots, b_d) = \\sum_{k=m+1}^d b_k^T \\cdot \\underbrace{S \\cdot b_k}_{\\lambda_k b_k} = \\sum_{k=m+1}^d \\lambda k$$, which will be minimal if we choose for $\\lambda_{m+1}, \\ldots, \\lambda_{d}$ the smallest eigenvalues of $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<[ Calculus ](Calculus.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
