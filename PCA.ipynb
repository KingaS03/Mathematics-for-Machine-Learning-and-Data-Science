{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Calculus & Statistics\n",
    "\n",
    "Agenda:\n",
    "* Concluding the gradient descent method\n",
    "* The method of Lagrange multipliers\n",
    "* Mean, variance, covariance for one-dimensionsal data sets\n",
    "* Covariance matrix for higher dimensional data sets\n",
    "* Inner products\n",
    "* Projections\n",
    "* PCA\n",
    "\n",
    "How do linear transformations affect the mean and the variance. Also for the higher dimensional data.\n",
    "\n",
    "Inner products:\n",
    "    Explain inner products as a generalisation of the dot product\n",
    "    - A symmetric, positive definite, bilinear mapping\n",
    "    - Sylvester criteria for pos.def.\n",
    "    - triangle ineq.\n",
    "    - Cauchy-Schwarz ineq.\n",
    "    Compute angles and distances using inner products\n",
    "    Write code that computes distances and angles between images\n",
    "    Demonstrate an understanding of properties of inner products\n",
    "    Discover that orthogonality depends on the inner product\n",
    "    Inner product of functions\n",
    "    orthogonality of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "1. For which starting points will the gradient descent method lead us to the phone on the below surface?\n",
    "\n",
    "<center>\n",
    "<img src=\"Images/Sandpit1.png\" width=\"350\"> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'questions/questions3_1.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For which starting points will the gradient descent method lead us to the phone on the below surface?\n",
    "\n",
    "<center>\n",
    "<img src=\"Images/Sandpit2.png\" width=\"350\"> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run 'questions/questions3_2.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For which starting points will the gradient descent method lead us to the phone on the below surface?\n",
    "\n",
    "<center>\n",
    "<img src=\"Images/Sandpit3.png\" width=\"350\"> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'questions/questions3_3.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For which starting points will the gradient descent method lead us to the phone on the below surface?\n",
    "\n",
    "<center>\n",
    "<img src=\"Images/Sandpit2.png\" width=\"350\"> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Can you imagine a surface where the gradient descent algorithm gets stuck?\n",
    "\n",
    "6. Can you imagine such a surface with two starting points very close to each other, which lead to totally different local extrema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Discuss the [gradient descent types used in a machine learning context](https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a) with your team, collect the major characteristics of the one allocated to your team.\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. The method of Lagrange multipliers\n",
    "\n",
    "--------------\n",
    "\n",
    "Let $f: \\mathbb{R}^n \\to \\mathbb{R}$ and $g: \\mathbb{R}^n \\to \\mathbb{R}$ be two functions.\n",
    "\n",
    "**Goal:** Minimise the value $f(x)$ under the constraint $g(x) = 0$, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\mbox{target function:}& f(x) \\to \\min\\\\\n",
    "\\mbox{constraint:}& g(x) = 0\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Developing an intuition\n",
    "\n",
    "----------------\n",
    "**Exercise**\n",
    "\n",
    "To derive the general theory of the Lagrange multipliers method, play with the two GeoGebra applets below and find the points for which the function $f$ attains its minimum under the constraint $g = 0$. \n",
    "\n",
    "* In the first applet on the left hand side the formulas of the target function $f$ and of $g$ (describing the constraint) are explicitely provided. The function $f$ is plotted by its concentric contour lines. The set of points satisfying the constraint is a parabola in this case, its graph is plotted in blue. You can move the blue point on the parabola. The contour line, on which the point is situated, is plotted in red and the value of the target function reached on this line is also provided. The current coordinates of the blue point $(x_0, y_0)$ are indicated in red on the left.\n",
    "\n",
    "The solution of the first constrained optimisation problem: $(x,y) = (...,...)$.\n",
    "\n",
    "* Figure out how should we use the second app to solve the constrained minimisation problem on the surface given by its contour lines under the constraint given by the blue circle.\n",
    "\n",
    "The solution of the second constrained optimisation problem is: $(x,y) = (...,...)$.\n",
    "\n",
    "Do you observe something interesting about the solutions? Do they have something in common?\n",
    "..............................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://www.geogebra.org/classic/vbbkutmc\", 1100, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/kr3vzrvm\", 1100, 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--To explain intuitively our observation consider the surface described by the funcion $\\underbrace{f(x,y)}_{z} = x^2 + y^2$. If you run the code in the cell below, this surface will be plotted. Consider the additional explanation on the whiteboard-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(-10, 10, 0.5)\n",
    "y = np.arange(-10, 10, 0.5)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "fig = plt.figure(figsize = (14, 7))\n",
    "\n",
    "#first subplot\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.jet, linewidth=0)\n",
    "fig.colorbar(surf)\n",
    "\n",
    "#fig.tight_layout()\n",
    "\n",
    "#second subplot\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Formalising the method of Lagrange multipliers\n",
    "\n",
    "The behaviour that we have observed for the two particular exmaples can be generalised. For any extremum point $x$ of $f$ under the constraint $g = 0$ it holds that their gradients should be alined, i.e. there exists a coefficient $\\lambda \\in \\mathbb{R}$ such that\n",
    "\n",
    "\\begin{align}\n",
    "    \\nabla f(x) = \\lambda \\nabla g(x) \\quad \\quad (1)\n",
    "\\end{align}\n",
    "\n",
    "Of course the point $x$ we are looking for should also satisfy the constraint equation \n",
    "\n",
    "$$g(x) = 0 \\quad \\quad \\quad \\quad \\ \\ \\  \\ (2)$$\n",
    "\n",
    "Let us define the Lagrangian function $\\mathcal{L}: \\mathbb{R}^n \\times \\mathbb{R}$ by \n",
    "$$\\mathcal{L}(x,\\lambda) = f(x) - \\lambda g(x)$$\n",
    "\n",
    "The gradient of the Lagrangian is\n",
    "\n",
    "$$\\nabla \\mathcal{L}(x,\\lambda) = \\nabla \\mathcal{L}(\\overbrace{x_1, x_2, \\ldots, x_n}^{x},\\lambda) = \\Big(\\overbrace{\\frac{\\partial \\mathcal{L}}{\\partial x_1}, \\frac{\\partial \\mathcal{L}}{\\partial x_2}, \\ldots, \\frac{\\partial \\mathcal{L}}{\\partial x_n}}^{\\nabla_x \\mathcal{L}}, \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} \\Big) = \\Big(\\overbrace{\\frac{\\partial(f-\\lambda g)}{\\partial x_1}, \\frac{\\partial(f-\\lambda g)}{\\partial x_2}, \\ldots, \\frac{\\partial(f-\\lambda g)}{\\partial x_n}}^{\\quad \\quad \\quad \\nabla(f-\\lambda g)\\ =\\ \\nabla f - \\lambda \\nabla g}, -g \\Big)$$\n",
    "\n",
    "Thus equations (1) and (2) are equivalent to\n",
    "\n",
    "$$\\nabla \\mathcal{L}(x, \\lambda) = 0$$\n",
    "\n",
    "Our goal was to find the solutions of the constrained optimisation problem. We have managed to conclude that all the solutions should satisfy $\\nabla \\mathcal{L}(x, \\lambda) = 0$, i.e. they should be stationary points of the associated Lagrangian $\\mathcal{L}(x,\\lambda) = f(x) - \\lambda g(x)$. \n",
    "\n",
    "The method of Lagrange multilpliers for solving constrained optimisatiosn problems formulated in two steps:\n",
    "* find the candidates, i.e. the stationary points of the Lagrangian,\n",
    "* check for which of them is the value of the target function optimal.\n",
    "\n",
    "-----------------\n",
    "\n",
    "### 4.1.3 Visual summary for the method of Lagrange multipliers\n",
    "Let $f, g: \\mathbb{R}^n \\to \\mathbb{R}$ be two differentiable functions.\n",
    "\n",
    "For a fixed $\\lambda \\in \\mathbb{R}$ we defined the Lagrangian $\\mathcal{L}: \\mathbb{R}^n \\times \\mathbb{R} \\to \\mathbb{R}$ as \n",
    "\n",
    "$$\\mathcal{L}(x, \\lambda) = f(x) - \\lambda g(x)$$\n",
    "\n",
    "**The method of Lagrange multipliers**\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\mbox{target function:}& f(x) \\to \\min\\\\\n",
    "\\mbox{constraint:}& g(x) = 0\n",
    "\\end{array}\n",
    "\\right. \\quad \\Rightarrow \\quad \n",
    "\\left\\{\\begin{array}{l}\n",
    "\\exists \\lambda:  \\nabla f(x) = \\lambda \\nabla g(x)\\\\\n",
    " g(x) = 0\n",
    "\\end{array}\n",
    "\\right. \\quad \\Leftrightarrow \\quad \\nabla \\mathcal{L}(x,\\lambda) = 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "**Exercise**\n",
    "\n",
    "Minimise the function $f(x,y) = 0.5(y-x)^2 + 0.5(1-x)^2$ under the constraint $\\frac{x^2}{40^2} + \\frac{y^2}{20^2} = 1$ by using the Lagrange multipliers method. You can do so, by carrying out the calculations by hand or by completing the below code with the missing functions. Check whether your solution is realistic by taking a look at the plot of the surface with its contour lines and gradients and of the curve satisfying the constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "# First we define the functions, YOU SHOULD IMPLEMENT THESE\n",
    "def f(x, y):\n",
    "    return ....\n",
    "\n",
    "def g(x, y):\n",
    "    return .....\n",
    "\n",
    "# Next their derivatives, YOU SHOULD IMPLEMENT THESE\n",
    "def dfdx(x, y):\n",
    "    return ......\n",
    "\n",
    "def dfdy(x, y):\n",
    "    return .....\n",
    "\n",
    "def dgdx(x, y):\n",
    "    return ......\n",
    "\n",
    "def dgdy(x, y):\n",
    "    return .......\n",
    "\n",
    "# We define the gradient of the Lagrangian function.\n",
    "def DL(xyλ):\n",
    "    [x, y, λ] = xyλ\n",
    "    return np.array([\n",
    "            dfdx(x, y) - λ * dgdx(x, y),\n",
    "            dfdy(x, y) - λ * dgdy(x, y),\n",
    "            - g(x, y)\n",
    "        ])\n",
    "\n",
    "# We use the optimize.root method to find the roots of the Lagrangian.\n",
    "\n",
    "x0, y0, λ0 = (10, 10, 0)\n",
    "x, y, λ = optimize.root(DL, [x0, y0, λ0]).x\n",
    "print(f\"x = {x:.3f}\")\n",
    "print(f\"y = {y:.3f}\")\n",
    "print(f\"λ = {λ:.3f}\")\n",
    "print(f\"f(x, y) = {f(x,y):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_x = np.arange(-50, 50, 4)\n",
    "feature_y = np.arange(-50, 50, 4)\n",
    "\n",
    "x, y = np.meshgrid(feature_x, feature_y)\n",
    "f = 0.5*(y-x)**2 + 0.5*(1-x)**2 #x**2 - y**2 #0.5*(y-x)**2 + 0.5*(1-x)**2\n",
    "dfdx = 2*x - y - 1 #2*x #2*x - y - 1\n",
    "dfdy = y-x #-2*y #y - x\n",
    "\n",
    "# Normalize all gradients to focus on the direction not the magnitude\n",
    "norm = np.linalg.norm(np.array((dfdx, dfdy)), axis=0)\n",
    "dfdx = dfdx / norm\n",
    "dfdy = dfdy / norm\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.set_aspect(1)\n",
    "#ax.plot(feature_x, feature_y, c='k')\n",
    "ax.quiver(x, y, dfdx, dfdy, units='xy', scale=0.3, color='gray')\n",
    "ax.contour(x, y, z, 15, cmap='jet')\n",
    "\n",
    "#g_x = np.arange(-10, 10, 0.1)\n",
    "#g_y = g_x**2\n",
    "angle = np.arange(0, 2*np.pi, 0.1)\n",
    "g_x = 40*np.cos(angle)\n",
    "g_y = 20*np.sin(angle)\n",
    "ax.plot(g_x,g_y, '-r')\n",
    "ax.set_ylim([-50,50]);\n",
    "\n",
    "#arrow = FancyArrowPatch((35, 35), (35+34*0.2, 35+0), arrowstyle='simple',\n",
    "#                        color='r', mutation_scale=10)  \n",
    "#ax.add_patch(arrow)  # NOTE: this gradient is scaled to make it better visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "**Last remark on calculus**\n",
    "\n",
    "Towards the end of the 17th century, beginning of the 18th century there was a big fight between Newton and Leibniz, who both started to develop and formalise the theory of calculus. You can read more about it [here](https://en.wikipedia.org/wiki/Leibniz%E2%80%93Newton_calculus_controversy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Basic concepts of descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Projection to a subspace\n",
    "- Real symmetric matrices\n",
    "- PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a method for linear dimensionality reduction.\n",
    "\n",
    "----------------------------\n",
    "### 4.2.1 Mean, variance, standard deviation\n",
    "Instead of working directly with the data many times we want to characterise it by its mean, variance and correlation between the observed features.\n",
    "\n",
    "Calculate the average of the image of 0's.\n",
    "\n",
    "For a set of data points $X = \\{x_1, x_2, \\ldots, x_n\\}$ the mean or expected value of this data set is:\n",
    "$$E(X) = \\frac{x_1 + x_2 + \\cdots + x_n}{n} \\quad \\mbox{or shorter} \\quad E(X) = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i.$$\n",
    "\n",
    "Calculate the mean value of the next two data points: ..... . \n",
    "\n",
    "If you have calculated correctly, they have the same mean value. To differentiate among them, we can calculate their variance, which is a measure of average suqre distance from the mean value.\n",
    "\n",
    "The average square distance from the mean is\n",
    "$$\\frac{(x_1 - \\mu)^2 + (x_2-\\mu)^2 + \\cdots + (x_n - \\mu)^2}{n} = \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i - \\mu)^2, \\quad \\mbox{where} \\quad \\mu = E(X).$$\n",
    "\n",
    "This value above is denoted by ${\\rm Var}(X)$ and it is called the variance of the data points $X = \\{x_1, x_2, \\ldots, x_n\\}$. A common notation for it is also $\\sigma^2$.\n",
    "\n",
    "Calculate the variance for the above two data points.\n",
    "\n",
    "One can conclude that the spread of ..... data points is larger than the spread of the ....... data points.\n",
    "\n",
    "If the data is measured in units, then observe that the measurement unit for the variance is square units. \n",
    "\n",
    "The standard deviation is obtained from the variance by calculating the square root of it, i.e.\n",
    "$$\\sigma = \\sqrt{{\\rm Var}(X)}.$$\n",
    "\n",
    "The above formula explains also the $\\sigma^2$ notation in case of the variance.\n",
    "\n",
    "**Remarks:** \n",
    "* If the data $X = \\{x_1, x_2, \\ldots, x_n\\}$ is centered (i.e. $E(X) = \\mu = 0$), then the variance can be calculated as\n",
    "\n",
    "$${\\rm Var}(X) = \\frac{x_1^2 + x_2^2 + \\cdots + x_n^2}{n}$$\n",
    "\n",
    "or if $x = \\left(\\begin{array}{c}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right)$ is the column vector containing all the observations, then $${\\rm Var}(X) = x^T \\cdot x = \\left(\\begin{array}{cccc}\n",
    "x_1 &\n",
    "x_2 & \n",
    "\\ldots &\n",
    "x_n\n",
    "\\end{array}\\right) \\cdot \n",
    "\\left(\\begin{array}{c}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right).$$\n",
    "\n",
    "* Any data $X = \\{x_1, x_2, \\ldots, x_n\\}$ can be centered by extracting the mean of the data from each element, i.e. the data $X' = \\{x'_1 = x_1 - \\mu,x'_2 = x_2 - \\mu, \\ldots, x'_n = x_n - \\mu\\}$ is centered. \n",
    "* For not centered data organised into a column vector $x = \\left(\\begin{array}{c}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right)$ the variance can be calculated in the following way by matrix operations: \n",
    "\n",
    "$${\\rm Var}(X) = (x-\\mu)^T \\cdot (x-\\mu) = \\left(\\begin{array}{cccc}\n",
    "x_1-\\mu &\n",
    "x_2-\\mu & \n",
    "\\ldots &\n",
    "x_n-\\mu\n",
    "\\end{array}\\right) \\cdot \n",
    "\\left(\\begin{array}{c}\n",
    "x_1-\\mu\\\\ \n",
    "x_2-\\mu\\\\ \n",
    "\\vdots\\\\\n",
    "x_n-\\mu\n",
    "\\end{array}\\right), \\quad \\mbox{where} \\quad \\mu = E(X).$$\n",
    "\n",
    "-------------\n",
    "\n",
    "### 4.2.2. Higher dimensional variance, covariance\n",
    "The generalisation of the variance to higher dimensional datasets.\n",
    "\n",
    "If we observe more features at the same time (i.e. we deal with higher dimensional data), then we can calculate the variance along each dimension.\n",
    "\n",
    "Consider the following two datasets and calculate for both of them the mean and variance along $X$ and the variance along $Y$.\n",
    "\n",
    "If you have calculated correctly, then the two datasets have the same variance and same mean values along both features. However, if me plot them, we can see that they are different. To capture a difference we can calculate their covariance, as well. \n",
    "\n",
    "--------\n",
    "**Definition**\n",
    "\n",
    "The covariance of two observed features $X$ and $Y$ based on the observed value pairs $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$ is calulated by the following formula:\n",
    "\n",
    "$${\\rm Cov}(X,Y) = E\\left((X - \\mu_x)(Y-\\mu_y)\\right) = \\frac{(x_1-\\mu_x)(y_1-\\mu_y) + (x_2-\\mu_x)(y_2-\\mu_y) + \\cdots + (x_n-\\mu_x)(y_n-\\mu_y)}{n} = \\frac{1}{n} \\sum\\limits_{i=1}^n (x_i-\\mu_x)(y_i-\\mu_y),$$\n",
    "\n",
    "where $\\mu_x = E(X)$ and $\\mu_y = E(Y)$.\n",
    "\n",
    "--------------\n",
    "\n",
    "**Remark**\n",
    "\n",
    "1. The covariance of $X$ and $Y$ with matrix operations is\n",
    "\n",
    "$${\\rm Cov}(X,Y) = (x-\\mu_x)^T \\cdot (y - \\mu_y), \\quad \\mbox{where} \\quad x = \\left(\\begin{array}{c}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right), y = \\left(\\begin{array}{c}\n",
    "y_1\\\\ \n",
    "y_2\\\\ \n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{array}\\right), \\mu_x = E(X) \\mbox{ and }\\mu_y = E(Y)$$\n",
    "\n",
    "2. What is the relationship between ${\\rm Cov}(X,Y)$ and ${\\rm Cov}(Y,X)$? \n",
    "$${\\rm Cov}(X,Y)\\ ???\\ {\\rm Cov}(Y,X)$$\n",
    "\n",
    "--------------\n",
    "\n",
    "**Interpretation of the covariance**\n",
    "* If ${\\rm Cov}(X,Y) > 0$, then the values of $Y$ increase on average when the values of $X$ increase.\n",
    "* If ${\\rm Cov}(X,Y) < 0$, then the values of $Y$ decrease on average when the values of $X$ increase.\n",
    "* If ${\\rm Cov}(X,Y) = 0$, then there is no observable linear trend between $X$ and $Y$.\n",
    "\n",
    "-----------\n",
    "\n",
    "**Definition**\n",
    "\n",
    "For a two-dimensional data $D = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}$ we can calculate the following 4 values: ${\\rm Var}(X),{\\rm Var}(Y),{\\rm Cov}(X,Y)$ and ${\\rm Cov}(Y,X)$. The covariance matrix of this two-dimensional data is:\n",
    "\n",
    "$${\\rm Var}(D) = \\left(\\begin{array}{cc}\n",
    "{\\rm Var}(X) & {\\rm Cov}(X,Y)\\\\\n",
    "{\\rm Cov(Y,X)} & {\\rm Var(Y)}\n",
    "\\end{array}\\right).$$\n",
    "\n",
    "---------------\n",
    "**Flash questions**\n",
    "\n",
    "1. How does look like the covariance matrix of a three-dimensional data $D = \\{(x_1, y_1, z_1), (x_2, y_2, z_2), \\ldots, (x_n, y_n, z_n)\\}$?\n",
    "\n",
    "2. How can one calculate the covariance matrix with the help of matrix operations?\n",
    "\n",
    "------------\n",
    "**Remark**\n",
    "\n",
    "The covariance is measured in ${\\rm unit}_1 \\cdot {\\rm unit}_2$ if the measurement unit of the values in data set $X$ \n",
    "is ${\\rm unit}_1$ and the measurement unit of the values in data set $Y$ is ${\\rm unit}_2$. Depending on our choice of measuremeant units, the covariance is going to be larger of smaller. For example if both sets of our measurements are expressed in $m$ and the covariance of them is $25$, then changing our measurement unit to $cm$, would yield a covariance of $25*100^2 = 250.000$. Both covariances are characterising the same dataset. How to interpret them? How to compare two covariances if they come from different data sets?\n",
    "\n",
    "The correlation is an answer to all these questions:\n",
    "\n",
    "-------------\n",
    "\n",
    "**Definition**\n",
    "\n",
    "The correlation of two observed features $X$ and $Y$ based on the observed value pairs $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$ is calulated by the following formula:\n",
    "\n",
    "$${\\rm Corr}(X,Y) = \\frac{{\\rm Cov}(X,Y)}{\\sigma_X \\cdot \\sigma_Y}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "### 4.2.3. The effect of linear transformations on the mean and on the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As start, please take a short quiz by running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'questions/questions1.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we generalise what we have observed?\n",
    "\n",
    "\\begin{align*}\n",
    "E(X + c) = \\_\\_\\_\\_ E(X) \\\\[0.5em]\n",
    "E(cX) = \\_\\_\\_\\_ E(X)\\\\[0.5em]\n",
    "E(aX + b) = \\_\\_\\_\\_ E(X)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the test in the below field!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'questions/questions2.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we say about the variance of a one-dimensional data set $X$ in general?\n",
    "\n",
    "\\begin{align*}\n",
    "{\\rm Var}(X + c) = \\_\\_\\_\\_ {\\rm Var}(X)\\\\[0.5em]\n",
    "{\\rm Var}(cX) = \\_\\_\\_\\_ {\\rm Var}(X)\\\\[0.5em]\n",
    "{\\rm Var}(aX + b) = \\_\\_\\_\\_ {\\rm Var}(X)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the covariance matrix?\n",
    "\n",
    "\\begin{align*}\n",
    "{\\rm Var}(AD + c) = A \\cdot {\\rm Var}(D) \\cdot A^T,\n",
    "\\end{align*}\n",
    "\n",
    "where $D = \\{(x_1, x_2, \\ldots, x_d)^T| x_i \\in \\mathbb{R} \\mbox{ is the value of }\\}$, $A \\in \\mathbb{R}^{dxd}$ and $c \\in \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "## 4.3 Descriptive statistics and linear algebra\n",
    "\n",
    "### 4.3.1 The inner product\n",
    "\n",
    "The inner product is a generalisation of the dot product or Euclidea scalar product, that we was introduced in Chapter 2. Once we have an inner product, we are able to calculate lengths (distances) and angles.\n",
    "\n",
    "-----------\n",
    "**Definition**\n",
    "\n",
    "On a vector space $V$ the inner product is a **symmetric, positive definite, bilinear** mapping $\\langle \\cdot, \\cdot \\rangle: V \\times V \\mapsto \\mathbb{R}$.\n",
    "\n",
    "* symmetry holds if $\\langle x, y\\rangle = \\langle y, x \\rangle$ for all $x, y \\in V$\n",
    "* positive definiteness holds if $\\langle x, x \\rangle > 0$ for all $x \\in V$, where $x \\neq \\mathbf{0}$\n",
    "* bilinearity holds if \n",
    "\\begin{align*}\n",
    "\\langle c_1 \\cdot x^{(1)} + c_2 \\cdot x^{(2)}, y \\rangle = c_1 \\langle x^{(1)}, y \\rangle + c_2 \\langle x^{(2)}, y \\rangle, \\quad \\mbox{for all} \\quad c_1, c_2 \\in \\mathbb{R}, x^{(1)}, x^{(2)}, y \\in \\mathbb{R}^n\\\\\n",
    "\\langle x, c_1 \\cdot y^{(1)} + c_2 \\cdot y^{(2)} \\rangle = c_1 \\langle x, y^{(1)} \\rangle + c_2 \\langle x, y^{(2)} \\rangle \\quad \\mbox{for all} \\quad c_1, c_2 \\in \\mathbb{R}, x, y^{(1)}, y^{(2)} \\in \\mathbb{R}^n\n",
    "\\end{align*}\n",
    "\n",
    "-------------\n",
    "**Definition**\n",
    "A matrix $A \\in \\mathbb{R}^{n \\times n}$ is positive definite if $x^T \\cdot A \\cdot x >0$ for all $x \\in \\mathbb{R}$.\n",
    "\n",
    "---------------\n",
    "**Sylvester's criterion**\n",
    "A symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is positive definite iff all the leading principal minors are positive, i.e. if all the following matrices have positive determinants:\n",
    "* the upper left $1\\times 1$ corner of $A$,\n",
    "* the upper left $2\\times 2$ corner of $A$,\n",
    "* the upper left $3\\times 3$ corner of $A$,\n",
    "* ...\n",
    "* $A$ itself.\n",
    "\n",
    "-----------\n",
    "**Remarks**\n",
    "\n",
    "1. For our context it suffices to consider for $V$ the $n$-dimensional real vector space $\\mathbb{R}^n$. \n",
    "2. $V = \\mathbb{R}^n$ for every inner product $\\langle, \\cdot, \\cdot \\rangle$ there exists a symmetric, positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$ such that \n",
    "$$\\langle x, y \\rangle = x^T \\cdot A \\cdot y, \\quad \\mbox{for every} \\quad x,y \\in \\mathbb{R}^n.$$\n",
    "3. Observe that the dot product introduced in Chapter 2 satisties the general definition of an inner product, therefore the inner product can be seen as a generalisation of the dot product.\n",
    "\n",
    "The dot product can be written as\n",
    "$$\\langle x, y \\rangle = x^T \\cdot I_n \\cdot y,$$\n",
    "where $I_n$ is the $n \\times n$-dimensional identity matrix.\n",
    "\n",
    "--------------\n",
    "**Definitions**\n",
    "\n",
    "1. We say that two vectors $x, y \\in \\mathbb{R}^n$ are orthogonal to each other w.r.t. the considered inner product, if their inner product $\\langle x, y \\rangle$ is $0$. \n",
    "2. In a right triangle the side opposite to the right angle is called hypotenuse.\n",
    "3. The cosine of an angle in a right triangle is the ratio of the leg next to the angle and of the hypotenuse. Furthermore, $\\cos(\\alpha) = - \\cos(\\pi-\\alpha)$.\n",
    "4. For a vector $x \\in \\mathbb{R}^n$ we introduce the $||x||$ notation for the value $\\langle x, x\\rangle$ and we call the value $||x||$ the length of the  vector $x$.\n",
    "5. For two vectors $x, y \\in \\mathbb{R}^n$ we introduce the notation $d(x,y)$ for the value $||x-y||$. The properties of the inner product assure that the so defined function $d: \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}_{\\geq 0}$ is a distance (or with other words metric) in the mathematical sense, i.e. the following properties are satisfied by it for all $x,y, z \\in \\mathbb{R}^n$:\n",
    "\n",
    "    \\begin{align*}\n",
    "    & (1)\\ \\mbox{identity of indiscernibles:} \\quad d(x,y) = 0 \\ \\Leftrightarrow \\ x = y\\\\[0.5em]\n",
    "    & (2)\\ \\mbox{symmetry:} \\quad d(x,y) = d(y,x)\\\\[0.5em]\n",
    "    & (3)\\ \\mbox{triangle inequality:} \\quad d(x,y) \\leq d(x,z) + d(z,y)\n",
    "    \\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"https://www.geogebra.org/classic/qhxyf5cj\", 900, 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Remarks**\n",
    "\n",
    "1. With the above definition of orthogonality, in a right triangle the Pythagorean theorem and the more general law of cosines also hold, i.e.\n",
    "\\begin{align*}\n",
    "    &||x-y||^2 = ||x||^2 + ||y||^2, \\quad \\mbox{if} \\quad \\langle x,y \\rangle = 0\\\\[0.5em]\n",
    "    &||x-y||^2 = ||x||^2 + ||y||^2 - 2\\cdot\\cos(\\theta_{x,y})\\cdot||x||\\cdot||y||,\n",
    "\\end{align*}\n",
    "where $\\theta_{x,y}$ denotes the angle between the vectors $x$ and $y$. \n",
    "\n",
    "As we did in the linear algebra part, we can conclude from these two, that\n",
    "\n",
    "$$\\cos(\\theta_{x,y}) = \\frac{\\langle x, y\\rangle}{||x|| \\cdot ||y||}$$\n",
    "\n",
    "2. Observe that if you think of a 1-dimensional centered data set $X = \\{x_1, x_2, \\ldots, x_n\\}$ as a column vector \n",
    "\n",
    "$$x = \\left(\n",
    "\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\n",
    "\\right)\\in \\mathbb{R},$$ \n",
    "\n",
    "then \n",
    "\n",
    "$${\\rm Var}(X) = x^T \\cdot x = \\langle x, x \\rangle = ||x||^2 \\quad \\mbox{and} \\quad \\sigma_X = \\sqrt{{\\rm Var}(X)} = ||x||$$\n",
    "\n",
    "\n",
    "For a 2-dimensional centered data set $\\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}$ it holds that \n",
    "\n",
    "\\begin{align*}\n",
    "&{\\rm Cov}(X,Y) = x^T \\cdot y = \\langle x, y \\rangle\\\\[0.5em]\n",
    "&{\\rm Corr}(X,Y) = \\frac{{\\rm Cov}(X,Y)}{\\sigma_X \\cdot \\sigma_Y} = \\frac{\\langle x, y \\rangle}{||x|| \\cdot ||y||} = {\\rm cos}(X,Y)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Applications \n",
    "\n",
    "------------------------\n",
    "\n",
    "We load the images of digits from the well-known MNIST data set and we create a function to calculate distances between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.2-rc3 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (2.2.0rc3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (2.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (0.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.32.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.4.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.19.5)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (0.3.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (3.12.4)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (0.35.1)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (2.2.2)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorflow==2.2-rc3) (1.6.3)\n",
      "Requirement already satisfied: setuptools in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow==2.2-rc3) (49.6.0.post20200814)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (2.24.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (3.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (0.4.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (1.27.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (0.2.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (4.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/kingasipos/anaconda3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2-rc3) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.2-rc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "#from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = train_images.astype(np.double)\n",
    "labels = train_labels.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the following code do? Modify it in such a way to plot out a 7 instead of a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[labels==1].reshape(-1, 28, 28)[0], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and plot the average digit of $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.average(images[labels==1].reshape(-1, 28, 28),0), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider our images as vectors and calculate their distance and angle, by using the Euclidean metric on the space of image vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x0, x1):\n",
    "    \"\"\"Compute distance between two vectors x0, x1 using the dot product\"\"\"\n",
    "    distance = np.sum((x1-x0)*(x1-x0))**0.5 # <-- EDIT THIS to compute the distance between x0 and x1\n",
    "    return distance\n",
    "\n",
    "def angle(x0, x1):\n",
    "    \"\"\"Compute the angle between two vectors x0, x1 using the dot product\"\"\"\n",
    "    angle = np.arccos(np.sum(x0*x1)/(distance(x0,0)*distance(x1,0))) # <-- EDIT THIS to compute angle between x0 and x1\n",
    "    return angle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an image of a digit. Plot it. Find the one, which is closest to it among the next 1000 images. Plot this second image, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832\n"
     ]
    }
   ],
   "source": [
    "# scratch cell\n",
    "mindist = 100000\n",
    "first = 0\n",
    "second2remember = 0\n",
    "for second in range(1000):\n",
    "    f = images[first].reshape(28, 28)\n",
    "    s = images[second].reshape(28, 28)\n",
    "    d = distance(f.ravel(), s.ravel())\n",
    "    if 0 < d < mindist:\n",
    "        mindist = d\n",
    "        second2remember = second\n",
    "print(second2remember)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Projections\n",
    "\n",
    "----------------\n",
    "\n",
    "### 4.4.1. Projection to a one-dimensional space\n",
    "\n",
    "**Definition**\n",
    "In the real vector space $\\mathbb{R}^n$ the projection of a vector $x \\in\\mathbb{R}^n$ to a subspace with basis $B = \\{b_1, b_2, \\ldots, b_m\\} \\subset \\mathbb{R}^n$ is defined to be that element $\\pi_B(x) \\in {\\rm span}(B)$, for which $||x - \\pi_B(x)||$ is minimal. \n",
    "\n",
    "----------------\n",
    "\n",
    "**Claim**\n",
    "For the above defined projection it holds that \n",
    "$$ x - \\pi_d(x)\\ \\bot\\ {\\rm span}(B)$$\n",
    "\n",
    "*Proof*\n",
    "\n",
    "Let $y \\in {\\rm span}(B)$ such that $x - y \\ \\bot\\ {\\rm span}(B)$.\n",
    "\n",
    "For any $b \\in {\\rm span}(B)$ it holds, that\n",
    "\n",
    "\\begin{align*}\n",
    "    ||x - b||^2 &= ||(x - y) + (y - b)||^2\\\\[0.5em]\n",
    "    & = ||x-y||^2 + ||y-b||^2 + 2 \\underbrace{\\langle x-y, y-b\\rangle}_{ 0, \\mbox{ as } y-b\\ \\in\\ {\\rm span}(B)}\\\\[0.5em]\n",
    "    &\\geq ||x-y||^2\n",
    "\\end{align*}\n",
    "\n",
    "To complete the proof, we will show that the conditions\n",
    "\\begin{align*}\n",
    "    (1) \\quad  y \\in {\\rm span}(B)\\\\\n",
    "    (2) \\quad x-y \\ \\bot \\ {\\rm span}(B)\n",
    "\\end{align*}\n",
    "do uniquely determine the vector $y$.\n",
    "\n",
    "Condition (1) means that there exist the coefficients $\\lambda_1, \\lambda_2, \\ldots, \\lambda_m$ such that \n",
    "\n",
    "$$y = \\lambda_1 b_1 + \\lambda_2 b_2 +  \\cdots + \\lambda_m b_m$$\n",
    "\n",
    "Condition (2) means that the vector $x- y$ is perpendicular to every $b \\in {\\rm span}(B)$, so particularly also to the elements $b_i$ of the basis $B$, i.e.\n",
    "\n",
    "$$\\langle x-y, b_i \\rangle = 0, \\quad \\forall i \\in \\{1,2, \\ldots, m\\}.$$\n",
    "\n",
    "Let us substitute in the above formula $y = \\lambda_1 b_1 + \\lambda_2 b_2 +  \\cdots + \\lambda_m b_m$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle b_i, x - (\\lambda_1 b_1 + \\lambda_2 b_2 +  \\cdots + \\lambda_m b_m) \\rangle= 0\\\\[0.5em]\n",
    "\\langle b_i, x - B\\cdot \\lambda \\rangle = 0, \\quad \\forall i \\in \\{1,2, \\ldots, m\\},\n",
    "\\end{align*}\n",
    "\n",
    "where by $B$ we denote the matrix containing on its columns the basis vectors $b_1, b_2, \\ldots b_m$, i.e. $B = [b_1|b_2|\\ldots|b_m]$ and $\\lambda = \\left(\n",
    "\\begin{array}{c}\n",
    "\\lambda_1\\\\\n",
    "\\lambda_2\\\\\n",
    "\\ldots\\\\\n",
    "\\lambda_m\n",
    "\\end{array}\n",
    "\\right)$.\n",
    "\n",
    "Thus the last equation can be rewritten equivalently as\n",
    "\n",
    "\\begin{align*}\n",
    "b_i^T \\cdot x - B \\cdot \\lambda = 0, \\quad \\forall i \\in \\{1,2, \\ldots, m\\}\n",
    "\\end{align*}\n",
    "\n",
    "These $m$ equations can be merged together and written in the following form\n",
    "\n",
    "\\begin{align*}\n",
    "&B^T \\cdot (x - B\\cdot \\lambda) = \\mathbf{0}\\\\[0.5em]\n",
    "&B^T \\cdot x - B^T \\cdot B \\cdot \\lambda = \\mathbf{0}\\\\[0.5em]\n",
    "&B^T \\cdot B \\cdot \\lambda = B^T \\cdot x\\\\[0.5em]\n",
    "&\\lambda = \\left(B^T\\cdot B\\right)^{-1} B^T \\cdot x \\quad \\Rightarrow \\quad y = B \\cdot \\left(B^T\\cdot B\\right)^{-1} B^T \\cdot x\n",
    "\\end{align*}\n",
    "\n",
    "-------------------------\n",
    "\n",
    "**Remark**\n",
    "\n",
    "1. In the proof of the above claim we have also derived the projection formula\n",
    "\n",
    "$$\\pi_{B}(x) = B \\cdot \\left(B^T\\cdot B\\right)^{-1} B^T \\cdot x.$$\n",
    "\n",
    "Observe that this is an extension of the formula that we have obtained in Chapter 1 for projecting on a single line $b$:\n",
    "\n",
    "$$\\pi_{b}(x) = \\frac{b \\cdot b^T}{||b||^2} \\cdot x.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. PCA\n",
    "\n",
    "See explanation on whiteboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
