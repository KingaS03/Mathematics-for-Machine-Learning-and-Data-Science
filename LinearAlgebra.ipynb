{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jN57tclWDwK-"
   },
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/KingaS03/Mathematics-for-Machine-Learning-and-Data-Science/master)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KingaS03/Mathematics-for-Machine-Learning-and-Data-Science)\n",
    "\n",
    "<[ Introduction ](Introduction.ipynb)|[ Calculus ](Calculus.ipynb)>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpIC_HWHDwLB"
   },
   "source": [
    "# 2. Linear algebra\n",
    "\n",
    "Agenda\n",
    "\n",
    " - vector operations\n",
    "     - vector addition\n",
    "     - vector substraction\n",
    "     - multiplication of a vector by a scalar\n",
    "     - the dot product\n",
    " - matrix operations\n",
    "     - matrix addition\n",
    "     - matrix substraction\n",
    "     - multiplication of a matrix by a scalar\n",
    "     - matrix multiplication\n",
    "     - inverse of a square matrix\n",
    " - Gaussian elimination\n",
    " - projection and the dot product\n",
    " - orthogonal matrices\n",
    " - Gram-Schmidt orthogonalisation\n",
    " - change of basis\n",
    " - eigenvalues and eigenvectors of matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEJZEIuwDwLC"
   },
   "source": [
    "## 2.1. Motivation\n",
    "We are able to solve equations of the form:\n",
    "$ax + b = c$, where $a,b,c$ are real coefficients and $x$ is the unknown variable.\n",
    "\n",
    "For example we can follow the next steps to solve the  $5x + 3 = 13$ equation\n",
    "\n",
    "$$\\begin{align*}\n",
    "5x + 3 &= 13 \\quad | -3\\\\\n",
    "5x &= 10 \\quad | : 5\\\\\n",
    "x &= 2\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$\\begin{align*}\n",
    "5x + 3 &= 13 \\quad | +(-3)\\\\\n",
    "5x &= 10 \\quad | \\cdot 5^{-1}= \\frac{1}{5}\\\\\n",
    "x &= 2\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Let us consider the following set-up. You have beakfast together with some of your colleagues and you are paying by turn. You don't know the price of each ordered item, but you remember what was ordered on the previous three days and how much did your colleagues pay for it each time:\n",
    "- 3 days ago your group has ordered 5 croissants, 4 coffees and 3 juices and they have payed 32.3 CHF.\n",
    "- 2 days ago your group has ordered 4 croissants, 5 coffees and 3 juices and they payed 32.5 CHF.\n",
    "- 1 day ago the group has ordered 6 croissants, 5 coffees and 2 juices and that costed 31 CHF.\n",
    "\n",
    "Today the group has ordered 7 croissants, 4 coffees and 2 juices and you would like to know whether the amount of 35 CHF available on your uni card will cover the consumption or you need to recharge it before paying.\n",
    "\n",
    "By introducing the notations\n",
    "- $x_1$ for the price of a croissant,\n",
    "- $x_2$ for the price of a coffee,\n",
    "- $x_3$ for the price of a juice,\n",
    "\n",
    "our information about the consumption of the previous 3 days can be summarised in the form of the following 3 linear equations <br><br>\n",
    "$$\\left\\{\\begin{align}\n",
    "&5\\cdot x_1 + 4 \\cdot x_2 + 3 \\cdot x_3 = 32.3 \\\\\n",
    "&4\\cdot x_1 + 5 \\cdot x_2 + 3 \\cdot x_3 = 32.5 \\\\\n",
    "&6\\cdot x_1 + 5 \\cdot x_2 + 2 \\cdot x_3 = 31\n",
    "\\end{align}\\right.$$\n",
    "<br>\n",
    "The quantity ordered on the current day is $7\\cdot x_1 + 4 \\cdot x_2 + 2 \\cdot x_3$. To determine this one possibility is to calculate the price of each product separately, i.e. we solve the linear equation system first and then susbstitute the prices in the previous formula.\n",
    "\n",
    "The above system in matrix form\n",
    "<br><br>\n",
    "$$\n",
    "\\left(\\begin{array}{ccc}\n",
    "5 & 4 & 3\\\\\n",
    "4 & 5 & 3\\\\\n",
    "6 & 5 & 2\n",
    "\\end{array}\n",
    "\\right)\\cdot \\left(\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_3\n",
    "\\end{array}\n",
    "\\right) =\n",
    "\\left(\\begin{array}{c}\n",
    "32.3\\\\\n",
    "32.5\\\\\n",
    "31\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "<br>\n",
    "If we introduce for the matrix, respectively the two vectors in the above formula the notations $A, x, b$, then we get <br><br>\n",
    "$$A \\cdot x = b$$\n",
    "<br>\n",
    "One can observe that formally this looks the same as the middle state of our introductory linear equation with real coefficients $5x = 10$. So our goal is to perform a similar operation as there, namely we are looking for the operation that would make $A$ \"dissappear\" from the left hand side of the equation. We will see later that this operation is the inverse operation of multiplication by a matrix, namely multiplication by the inverse of a matrix.\n",
    "\n",
    "In our applications we will also encounter matrix equations of the form:\n",
    "$A \\cdot x + b = 0$ (for example when deriving the weights of the multivariate linear regression).  This example motivates the introduction of vector substraction, as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnjeK0d0DwLC"
   },
   "source": [
    "## 2.2. Vectors\n",
    "\n",
    "----------\n",
    "**Definition of vectors**\n",
    "\n",
    "Vectors are elements of a linear vector space.\n",
    "The vector space we are going to work with is $\\mathbb{R}^n$, where $n$ is the dimension of the space and it can be $1, 2, 3, 4, ...$. An element of such a vector space can be described by an ordered list of $n$ components of the vector.\n",
    "\n",
    "$x = (x_1, x_2, \\ldots, x_n)$, where $x_1, x_2,\\ldots,x_n \\in \\mathbb{R}$ is an element of $\\mathbb{R}^n$.\n",
    "\n",
    "-----------\n",
    "\n",
    "**Example**<bf>\n",
    "$x = (1,2)$ is a vector of the two dimensional vector space $\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YbzGM2UDwLD"
   },
   "source": [
    "### 2.2.1. Geometrical representation of vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnE2gpuADwLD"
   },
   "source": [
    "In the below GeoGebra window a 2-dimensional vector is represented. You can move its endpoints on the grid and you will see how do its components change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "ZQwbF1Z9DwLD",
    "outputId": "8f20b744-c1b4-457c-99f2-3a5ae5b66886"
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://www.geogebra.org/classic/cnvxpycc\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bm9Y3xgTDwLE"
   },
   "source": [
    "Experiment with the 3-dimensional vector in the interactive window below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "8hcvPa36DwLF",
    "outputId": "aabf400f-dc64-4354-a3f6-8f998e3a1143"
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/meg4scuj\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6Iuf9i8DwLF"
   },
   "source": [
    "The following interactive window explains when are two vectors equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "JSZ8cl9xDwLF",
    "outputId": "dc9bfcf0-ab5d-425f-aa9b-e82c09696f6a"
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/fkkbkvuj\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtnqF0A1DwLF"
   },
   "source": [
    "### 2.2.2. Vector addition\n",
    "\n",
    "------\n",
    "**Definition of vector addition**\n",
    "\n",
    "Vector addition happens component-wise, namely the sum of the vectors $x = (x_1, x_2, \\cdots, x_n)$ and $y = (y_1, y_2, \\cdots, y_n)$ is:\n",
    "\n",
    "$$x+y = (x_1 + y_1, x_2+y_2, \\ldots, x_n+y_n)$$\n",
    "\n",
    "------\n",
    "\n",
    "There exist two approaches to visualise vector addition\n",
    "1. parallelogram method\n",
    "2. triangle method\n",
    "\n",
    "Both are visualised below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "1O-Xv9peDwLF",
    "outputId": "df088bb8-c472-4130-9206-f81e75467b9a"
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/jnchvrhg\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vam5BnoDwLG"
   },
   "source": [
    "Below you can see another approach to vector addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "tqwbkD7JDwLG",
    "outputId": "ec6595f9-bb2d-4529-f168-e77dea43ecba"
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/mzgchv22\", 1200, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJlISmdvDwLG"
   },
   "source": [
    "### 2.2.3. Multiplication of vectors by a scalar\n",
    "\n",
    "-------------\n",
    "**Definition of the multiplication by a scalar**\n",
    "\n",
    "This happens also component-wise exactly as addition, namely\n",
    "$$\\lambda  (x_1, x_2, \\ldots, x_n) = (\\lambda x_1, \\lambda x_2, \\ldots, \\lambda x_n).$$\n",
    "\n",
    "-------------\n",
    "This operation is illustrated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "id": "S7lqOBiPDwLG",
    "outputId": "4918c457-4b3a-472b-ce20-eb1f21f4fbb5"
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/gxhsev8k\", 1000, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-ACaIFyDwLG"
   },
   "source": [
    "### 2.2.4. Vector substraction\n",
    "\n",
    "------\n",
    "**Definition of vector substraction**\n",
    "\n",
    "Vector substraction happens component-wise, namely the difference of the vectors $x = (x_1, x_2, \\cdots, x_n)$ and $y = (y_1, y_2, \\cdots, y_n)$ is:\n",
    "\n",
    "$$x-y = (x_1 - y_1, x_2 - y_2, \\ldots, x_n -y_n)$$\n",
    "\n",
    "------\n",
    "\n",
    "Observe that vector substraction is not commutative, i.e. $x-y \\neq y-x$ in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AjoUKKzDwLG"
   },
   "source": [
    "### 2.2.5. Abstract linear algebra terminology\n",
    "\n",
    "1. For any two elements $x,y \\in \\mathbb{R}^n$ it holds that\n",
    "$$x+y \\in \\mathbb{R}^n.$$\n",
    "This property is called **closedness** of $\\mathbb{R}^n$ w.r.t. addition.\n",
    "\n",
    "2. Observe that the **commutativity** of the addition on $\\mathbb{R}^n$ is inherited by the vectors in $\\mathbb{R}^n$, i.e. $$x + y = y + x$$ for any $x, y \\in \\mathbb{R}^n$.\n",
    "\n",
    "3. Observe that addition is also **associative** on $\\mathbb{R}^n$, i.e.\n",
    "$$x + (y + z) = (x + y) + z, \\quad \\mbox{ for any } x,y,z \\in \\mathbb{R}^n$$\n",
    "\n",
    "4. If we add the zero vector $\\mathbf{0} = (0, 0, ..., 0) \\in \\mathbb{R}^n$ to any other vector $x \\in \\mathbb{R}^n$ it holds that\n",
    "$$\\mathbf{0} + x = x + \\mathbf{0} = x.$$\n",
    "The single element with the above property is called the **neutral element** w.r.t. addition.\n",
    "\n",
    "5. For a vector $x = (x_1, x_2, \\ldots, x_n)$ the vector $x^*$ for which\n",
    "$$x + x^* = x^* + x = \\mathbf{0}$$\n",
    "is called the **inverse vector** of $x$ w.r.t. addition.\n",
    "\n",
    "What is the inverse of the vector $x = (2, 3, -1)$? Inverse: $...$\n",
    "\n",
    "What is the inverse of a vector $x = (x_1, x_2, \\ldots, x_n)$? Inverse: $...$\n",
    "\n",
    "As every vector of $\\mathbb{R}^n$ possesses an inverse, we introduce the notation $-x$ for its inverse  w.r.t addition.\n",
    "\n",
    "A set $V$ with an operation $\\circ$ that satsifies the above properties is called a **commutative or Abelian group** in linear algebra. For us $V = \\mathbb{R}^n$ and $\\circ = +$.\n",
    "\n",
    "The scalar mutiplication, that we have introduced, has the following properties\n",
    "\n",
    "6. **associativity** of multiplication: $(\\lambda_1\\lambda_2) x = \\lambda_1 (\\lambda_2x)$,\n",
    "\n",
    "7. **distributivity**: $(\\lambda_1 + \\lambda_2) x = \\lambda_1  x + \\lambda_2  x$ and $\\lambda(x+y) = \\lambda x + \\lambda y$,\n",
    "\n",
    "8. **unitarity**: $1 x = x$,\n",
    "\n",
    "for all $x,y \\in \\mathbb{R}^n$ and $\\lambda, \\lambda_1, \\lambda_2$ scalars.\n",
    "\n",
    "Our scalars are elements of $\\mathbb{R}$, which is a **field**. This means the operations $\\lambda_1+\\lambda_2$, $\\lambda_1-\\lambda_2$, and $\\lambda_1\\cdot\\lambda_2$ are well-defined for any $\\lambda_1, \\lambda_2 \\in \\mathbb{R}$. Additionally, the division operation $\\lambda_1/\\lambda_2 = \\lambda_1 \\cdot \\lambda_2^{-1}$ is valid provided that $\\lambda_2 \\neq 0$. Importantly, the field also satisfies the distributive property of multiplication over addition, expressed mathematically as:\n",
    "$$\n",
    "\\lambda_1 \\cdot (\\lambda_2 + \\lambda_3) = \\lambda_1 \\cdot \\lambda_2 + \\lambda_1 \\cdot \\lambda_3\n",
    "$$\n",
    "for any $\\lambda_1, \\lambda_2, \\lambda_3 \\in \\mathbb{R}$.\n",
    "\n",
    "A **vector space** consists of a set $V$ and a field $F$ and two operations:\n",
    "- an operation called vector  addition that takes two vectors $v,w \\in V$, and produces a third vector, written $v+w \\in V$,\n",
    "- an operation called scalar multiplication that takes a scalar $\\lambda \\in F$ and a vector $v\\in V$, and produces a new vector, written $cv \\in V$,\n",
    "\n",
    "which satisfy all the properties enlisted above (5+3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMD9sQfWDwLG"
   },
   "source": [
    "---------\n",
    "**Remark**<br>\n",
    "Observe that\n",
    "$$x-y = x+(-y),$$\n",
    "<br>\n",
    "which means that the difference of $x$ and $y$ can be visualised as a vector addition of $x$ and $-y$.\n",
    "\n",
    "$-y$ is here the inverse of the vector $y$ w.r.t. addition. Geometrically $-y$ can be represented by the same oriented segment as $y$, just with opposite orientation.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En1W8K7wDwLG"
   },
   "source": [
    "### 2.2.6. Modulus of a vector, length of a vector, size of a vector\n",
    "\n",
    "The length of a vector or norm of a vector $x = (x_1, x_2, \\cdots, x_n)$ is given by the formula\n",
    "$$||x|| = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}$$\n",
    "\n",
    "When executing the content of the next cell, randomly generated vectors and their length appears. Execute the below cell iteratively and derive the formula for the length of a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "CqRCH7gBDwLG",
    "outputId": "a4675d9f-2257-4eec-fcf9-bd2671100858",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/mfzdes3n\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMEmQeH8DwLH"
   },
   "source": [
    "Each vector $x = (x_1, x_2, \\ldots, x_n) \\in \\mathbb{R}^n$ is uniquely determined by the following two features:\n",
    "- its magnitude / length / size / norm: $r(x) = ||x|| = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}$,\n",
    "- its direction: $e(x) = \\frac{x}{||x||} = \\frac{1}{\\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}}(x_1, x_2, \\ldots, x_n)$.\n",
    "\n",
    "If the magnitude $r \\in \\mathbb{R}$ and the direction $e \\in \\mathbb{R}^n$ of a vector is given, then this vector can be written as $re$.\n",
    "\n",
    "Observe that $\\frac{x}{||x||}$ has length $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImDhsIoSDwLH"
   },
   "source": [
    "### 2.2.7. Dot product / inner product / scalar product\n",
    "\n",
    "-----------------\n",
    "Definition of the **dot product**\n",
    "\n",
    "The **dot product / inner product / scalar product** of two vectors $x = (x_1, x_2)$ and $y = (y_1, y_2)$ is denoted by $\\langle x, y\\rangle$ and it is equal to the scalar $x_1\\cdot y_1+ x_2 \\cdot y_2$.\n",
    "\n",
    "This can be generalised to the vectors $x = (x_1, x_2, \\cdots, x_n) \\in \\mathbb{R}^n$ and $y = (y_1, y_2, \\cdots, y_n) \\in \\mathbb{R}^n$ as $\\langle x, y\\rangle = x_1\\cdot y_1 + x_2 \\cdot y_2 + \\cdots + x_n\\cdot y_n$.\n",
    "\n",
    "---------------\n",
    "\n",
    "Observe that as a consequence of the definition distributivity over addition holds, i.e. $\\langle x , y + z\\rangle = \\langle x , y \\rangle + \\langle x , z \\rangle$.\n",
    "\n",
    "Furthermore $\\lambda \\langle x , y\\rangle  =  \\langle \\lambda x, y\\rangle = \\langle x , \\lambda y\\rangle $.\n",
    "\n",
    "The last two properties together are called also **bilinearity** of the scalar product.\n",
    "\n",
    "Observe also that the scalar product is **commutative**, i.e. $\\langle x, y\\rangle = \\langle y, x \\rangle$.\n",
    "\n",
    "--------------------------------------\n",
    "**Question:** What's the relation between the length of a vector and the dot product?\n",
    "\n",
    "Length of a vector: $||x|| = \\sqrt{x_1^2+x_2^2 + \\cdots x_n^2}$\n",
    "\n",
    "The dot product of two vectors: $\\langle x , y \\rangle= x_1\\cdot y_1 + x_2 \\cdot y_2 + \\cdots + x_n\\cdot y_n$.\n",
    "\n",
    "Substituting $x$ in the last equation instead of $y$, we obtain\n",
    "\n",
    "$$\\mathbf{\\langle x , x \\rangle} = x_1^2 + x_2^2 + \\cdots + x_n^2 = ||x||^2$$\n",
    "\n",
    "--------------------------------\n",
    "**Convention**<br>\n",
    "When talking exclusively about vectors, for the simplicity of writing, we often think of them as row vectors. However, when matrices appear in the same context and there is a chance that we will multiply a matrix by a vector, it is important to specify also whether we talk about a row or column vector. In this extended context a vector is considered to be a column vector by default.\n",
    "\n",
    "From now on we are going to follow also this convention and we are going to think of a vector always as a column vector.\n",
    "\n",
    "----------------\n",
    "**Relationship of dot product and matrix multiplication**<br>\n",
    "\n",
    "Even if we didn't define formally the matrix product yet, we mention its relationship with the dot product, because in mathematical formulas it proves to be handy to have an alternative way for writing the dot multiplication.\n",
    "\n",
    "The following holds for any vectors $x,y \\in \\mathbb{R}^n$\n",
    "\n",
    "$$\\mathbf{\\langle x,y \\rangle} = x_1\\cdot y_1 + x_2 \\cdot y_2 + \\cdots x_n \\cdot y_n = (\\begin{array}{cccc}\n",
    "x_1 & x_2 & \\ldots & x_n\n",
    "\\end{array})\\cdot\\left(\\begin{array}{c}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{array}\\right) = \\mathbf{x^T \\cdot y},$$\n",
    "where $\\mathbf{x^T \\cdot y}$ denotes the matrix product of the row vector $x^T$ and the column vector $y$.\n",
    "\n",
    "Furthermore, observe that due to the commutativity of the dot product <br><br>\n",
    "$$x^T \\cdot y = \\langle x, y \\rangle = \\langle y, x\\rangle = y^T \\cdot x$$\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbeNIKpZDwLJ"
   },
   "source": [
    "### 2.2.8. The dot product and the cosine rule\n",
    "Let us consider two vectors $u$ and $d$ and denote their angle by $\\theta$. We construct the triangle having as sides the vectors $u$, $d$ and $u-d$. In the forthcoming we derive the formula\n",
    "$$\\langle u,  d \\rangle = ||u||\\cdot ||d||\\cdot cos(\\theta)$$ from the law of cosines.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/KingaS03/Mathematics-for-Machine-Learning-and-Data-Science/blob/master/Images/Projection.png?raw=1\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "The **law of cosines** is a generalisation of the **Pythagorean theorem** in a triangle, which holds not just for right triangles. As the Pythagorean theorem, this formulates also a relationship between the lengths of the three sides. In a triangle with side lengths $a$, $b$ and $c$ and an angle $\\theta$ opposite to the side with length $a$, the law of cosines claimes that <br>\n",
    "\n",
    "$$a^2 = b^2 + c^2 - 2bc\\cos(\\theta)$$\n",
    "\n",
    "In our setting we can write for the side lengths the norm / length of the vectors $u$, $d$, respectively $u-d$. In this way we obtain <br>\n",
    "\n",
    "$$||u-d||^2 = ||u||^2 + ||d||^2 - 2\\cdot||u||\\cdot ||d||\\cdot\\cos(\\theta)$$\n",
    "\n",
    "On the other hand, using the relationship between the length of a vector and the dot product, we can write the following <br>\n",
    "\n",
    "$$\\begin{align*}\n",
    "||u-d||^2 = \\langle u-d,u-d \\rangle\n",
    "\\end{align*}$$\n",
    "\n",
    "Using the bilinearity and commutativity of the dot product we can continue by <br><br>\n",
    "$$\\begin{align*}\n",
    "||u-d||^2 = \\langle u-d,u-d \\rangle = \\langle u-d, u\\rangle - \\langle u-d, d\\rangle = \\langle u, u\\rangle - \\langle d,u \\rangle - \\langle u,d \\rangle + \\langle d, d \\rangle = ||u||^2 + ||d||^2- 2\\langle d,u \\rangle\n",
    "\\end{align*}$$\n",
    "\n",
    "Summing up what did we obtain until now <br><br>\n",
    "$$\\left\\{\n",
    "\\begin{align*}\n",
    "||u-d||^2 = ||u||^2 + ||d||^2 - 2\\cdot||u||\\cdot ||d||\\cdot\\cos(\\theta)\\\\\n",
    "||u-d||^2 = ||u||^2 + ||d||^2- 2\\langle d,u \\rangle\n",
    "\\end{align*}\n",
    "\\quad \\right|\n",
    "\\Rightarrow  \\quad\n",
    "||u||^2 + ||d||^2 - 2\\cdot||u||\\cdot ||d||\\cdot\\cos(\\theta) = ||u||^2 + ||d||^2- 2\\langle d,u \\rangle\n",
    "$$\n",
    "<br>\n",
    "or equivalently\n",
    "$$ ||u||\\cdot ||d||\\cdot\\cos(\\theta) = \\langle u,d \\rangle$$\n",
    "\n",
    "Observe that from the above cosine rule one can express $\\cos(\\theta)$ of the vectors $u$ and $d$ as\n",
    "\n",
    "$$\\cos(\\theta) = \\frac{\\langle u, d\\rangle }{||u|| \\cdot ||d||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCgxLGlyDwLJ"
   },
   "source": [
    "### 2.2.9. Scalar and vector projection\n",
    "\n",
    "**Scalar projection:** length of the resulting projection vector, namely <br>\n",
    "\n",
    "$$||\\pi_d(u)|| = cos(\\theta) \\cdot ||u|| $$\n",
    "\n",
    "where $\\theta$ is the angle of the vectors $d$ and $u$.\n",
    "\n",
    "Due to the cosine rule that we have derived for the scalar product, we can substitute $\\cos(\\theta)$ by $\\frac{\\langle u, d\\rangle}{||u|| \\cdot ||d||}$ and we obtain the following formula for the length of the projection\n",
    "\n",
    "$$||\\pi_d(u)|| = \\frac{\\langle u, d \\rangle}{||u|| \\cdot ||d||} \\cdot ||u|| = \\frac{\\langle u,d \\rangle}{||d||}$$\n",
    "\n",
    "**Vector projection:** We have determined the magnitude of the projection vector, the direction is given by the one of the vector $d$. These two characteristics do uniquely define the projection vector, thus we can write\n",
    "\n",
    "$$\\pi_d(u) = ||\\pi_d(u)|| \\frac{d}{||d||}= \\mathbf{\\frac{\\langle u, d\\rangle }{||d||} \\frac{d}{||d||}} = \\frac{d \\langle u, d\\rangle}{||d||^2} = \\frac{d \\langle d, u\\rangle}{||d||^2}= \\frac{d \\cdot (d^T \\cdot u)}{||d||^2}= \\frac{(d \\cdot d^T) \\cdot u}{||d||^2} = \\mathbf{\\frac{d \\cdot d^T}{||d||^2}\\cdot u}$$\n",
    "\n",
    "The projection matrix $\\frac{d \\cdot d^T}{||d||^2}$ in $\\mathbb{R}^n$ is an $n \\times n$-dimensional matrix.\n",
    "\n",
    "<!--Ex. 2. In PCA we can project basically on the first eigenvector and on its orthogonal complement.-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "5PdCWrjcDwLJ",
    "outputId": "0530bbef-4126-4a26-ff44-17078c02b7ed"
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/qhhqpmrt\", 1000, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZgyGepODwLJ"
   },
   "source": [
    "### 2.2.10. Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0z1h4_gDDwLJ"
   },
   "source": [
    "**Question 1**\n",
    "\n",
    "What is the dot product / scalar product of the vectors $x$ and $y$ given below?\n",
    "\n",
    "$$\\mbox{a) } x = \\left(\n",
    "\\begin{array}{c}\n",
    "1\\\\\n",
    "2\n",
    "\\end{array}\n",
    "\\right),\\\n",
    "y = \\left(\n",
    "\\begin{array}{c}\n",
    "3\\\\\n",
    "-2\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "$$\\mbox{b) } x = \\left(\n",
    "\\begin{array}{c}\n",
    "1\\\\\n",
    "2\\\\\n",
    "-1\n",
    "\\end{array}\n",
    "\\right),\\\n",
    "y = \\left(\n",
    "\\begin{array}{c}\n",
    "3\\\\\n",
    "-2\\\\\n",
    "-1\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXudp8UADwLJ"
   },
   "source": [
    "**Question 2**\n",
    "\n",
    "What is the length of the vectors $u = \\left(\n",
    "\\begin{array}{c}\n",
    "4\\\\\n",
    "3\n",
    "\\end{array}\n",
    "\\right)$ and $v = \\left(\n",
    "\\begin{array}{c}\n",
    "1\\\\\n",
    "0\\\\\n",
    "-1\n",
    "\\end{array}\n",
    "\\right)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe0vgIuRDwLK"
   },
   "source": [
    "**Question 3**\n",
    "\n",
    "What is the angle of the vectors $x$ and $y$ given below?\n",
    "\n",
    "$$\\mbox{a) } x = \\left(\n",
    "\\begin{array}{c}\n",
    "2\\\\\n",
    "3\\\\\n",
    "2\n",
    "\\end{array}\n",
    "\\right),\n",
    "y = \\left(\n",
    "\\begin{array}{c}\n",
    "1\\\\\n",
    "0\\\\\n",
    "-1\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "$$\\mbox{b) } x = \\left(\n",
    "\\begin{array}{c}\n",
    "\\sqrt{3}\\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right),\n",
    "y = \\left(\n",
    "\\begin{array}{c}\n",
    "-\\sqrt{2}/2\\\\\n",
    "\\sqrt{2}/2\n",
    "\\end{array}\n",
    "\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pYW2blEDwLK"
   },
   "source": [
    "**Question 4**\n",
    "\n",
    "What is the length of the projection of vector $x$ to vector $y$, where $x$ and $y$ are the vectors from the previous question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtVh1bi-DwLK"
   },
   "source": [
    "**Question 5**\n",
    "\n",
    "Calculate the vector projection $\\pi_u(x)$, where $u$ and $x$ are given as follows?\n",
    "\n",
    "$$\\mbox{a) } u = \\left(\n",
    "\\begin{array}{c}\n",
    "1\\\\\n",
    "2\n",
    "\\end{array}\n",
    "\\right),\\\n",
    "x = \\left(\n",
    "\\begin{array}{c}\n",
    "3\\\\\n",
    "-2\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "$$\\mbox{b) } u = \\left(\n",
    "\\begin{array}{c}\n",
    "1\\\\\n",
    "2\\\\\n",
    "-1\n",
    "\\end{array}\n",
    "\\right),\\\n",
    "x = \\left(\n",
    "\\begin{array}{c}\n",
    "3\\\\\n",
    "-2\\\\\n",
    "-7\n",
    "\\end{array}\n",
    "\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIKMSdalDwLK"
   },
   "source": [
    "### 2.2.11. Basis of a vectorspace\n",
    "--------\n",
    "**Definition of linear combination**\n",
    "\n",
    "A linear combination of the vectors $x^{(1)}, x^{(2)}, \\ldots, x^{(m)} \\in \\mathbb{R}^n$ is a vector of $\\mathbb{R}^n$, which can be written in the form of\n",
    "\n",
    "$$\\lambda_1x^{(1)} + \\lambda_2 x^{(2)} + \\cdots \\lambda_m \\cdot x^{(m)},$$\n",
    "\n",
    "where $\\lambda_1, \\lambda_2, \\ldots, \\lambda_m$ are real valued coefficients.\n",
    "\n",
    "--------\n",
    "**Example**\n",
    "\n",
    "Consider the vectors $x^{(1)} = \\left(\n",
    "\\begin{array}{c}\n",
    "1 \\\\\n",
    "0\\\\\n",
    "2\n",
    "\\end{array}\n",
    "\\right)$ and\n",
    "$x^{(2)} = \\left(\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right).$\n",
    "\n",
    "Then\n",
    "\n",
    "$$2 x^{(1)} + 3x^{(2)}= 2\\left(\n",
    "\\begin{array}{c}\n",
    "1 \\\\\n",
    "0\\\\\n",
    "2\n",
    "\\end{array}\n",
    "\\right) + 3 \\left(\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right) = \\left(\n",
    "\\begin{array}{c}\n",
    "2 \\\\\n",
    "3\\\\\n",
    "4\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "is a linear combination of $x^{(1)}$ and $x^{(2)}$.\n",
    "\n",
    "-------\n",
    "**Definition of linear dependence**\n",
    "\n",
    "Let us consider a set of $m$ vectors $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$ in $\\mathbb{R}^n$. They are said to be linearly dependent if and only if there exist the not all zero factors $\\lambda_1, \\lambda_2, \\ldots, \\lambda_m \\in \\mathbb{R}$  such that<br>\n",
    "\n",
    "$$\\lambda_1x^{(1)} + \\lambda_2 x^{(2)} + \\cdots +\\lambda_m  x^{(m)} = \\mathbf{0}$$\n",
    "\n",
    "-------\n",
    "\n",
    "**Remark**<br>\n",
    "Observe that if $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$ are dependent, then for some not all zero factors $\\lambda_1, \\lambda_2, \\ldots, \\lambda_m \\in \\mathbb{R}$ it holds that<br>\n",
    "\n",
    "$$\\lambda_1x^{(1)} + \\lambda_2 x^{(2)} + \\cdots + \\lambda_mx^{(m)} = \\mathbf{0}$$\n",
    "\n",
    "We know that at least one of the factors is not zero, let us assume that $\\lambda_i$ is such a factor. This means that from the above equation we can express the vector $x^{(i)}$ as a linear combination of the others, i.e.\n",
    "\n",
    "$$x^{(i)} = -\\frac{1}{\\lambda_i}\\left(\\lambda_1 x^{(1)} + \\cdots + \\lambda_{i-1} x^{(i-1)} + \\lambda_{i+1} x^{(i+1)} + \\cdots + \\lambda_m x^{(m)}\\right)$$\n",
    "\n",
    "-------\n",
    "**Definition of linear independence**\n",
    "\n",
    "$m$ vectors $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$ in $\\mathbb{R}^n$ are linearly independent if there exist no such factors $\\lambda_1, \\lambda_2, \\ldots, \\lambda_m \\in \\mathbb{R}$ for which <br>\n",
    "\n",
    "$$\\lambda_1x^{(1)} + \\lambda_2 x^{(2)} + \\cdots + \\lambda_m  x^{(m)} = \\mathbf{0}$$\n",
    "\n",
    "and where at least one factor is different of zero.\n",
    "\n",
    "**Alternative definition of linear independence**\n",
    "\n",
    "Equivalently $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$ in $\\mathbb{R}^n$ are linearly independent if and only if the equation <br>\n",
    "\n",
    "$$\\lambda_1x^{(1)} + \\lambda_2 x^{(2)} + \\cdots +\\lambda_m  x^{(m)} = \\mathbf{0}$$\n",
    "\n",
    "holds just for $\\lambda_1 = \\lambda_2 = \\cdots = \\lambda_m = 0.$\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZE8OyezDwLK"
   },
   "source": [
    "**Remark**\n",
    "\n",
    "If we write the vectors from the above equation by their components, the above equation can be equivalently transformed to <br>\n",
    "\n",
    "$$\\begin{align}\n",
    "\\lambda_1 \\left(\n",
    "\\begin{array}{c}\n",
    "x_1^{(1)} \\\\\n",
    "x_2^{(1)}\\\\\n",
    "\\vdots\\\\\n",
    "x_n^{(1)}\n",
    "\\end{array}\n",
    "\\right) +\n",
    "\\lambda_2 \\left(\n",
    "\\begin{array}{c}\n",
    "x_1^{(2)} \\\\\n",
    "x_2^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "x_n^{(2)}\n",
    "\\end{array}\n",
    "\\right) + \\cdots +\n",
    "\\lambda_m \\left(\n",
    "\\begin{array}{c}\n",
    "x_1^{(m)} \\\\\n",
    "x_2^{(m)}\\\\\n",
    "\\vdots\\\\\n",
    "x_n^{(m)}\n",
    "\\end{array}\n",
    "\\right) &= \\mathbf{0}\\\\ \\\\\n",
    "\\Leftrightarrow \\quad\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\lambda_1 x_1^{(1)} + \\lambda_2 x_1^{(2)} + \\cdots+ \\lambda_m x_1^{(m)}\\\\\n",
    "\\lambda_1 x_2^{(1)} + \\lambda_2 x_2^{(2)} + \\cdots+ \\lambda_m x_2^{(m)}\\\\\n",
    "\\vdots\\\\\n",
    "\\lambda_1 x_n^{(1)} + \\lambda_2 x_n^{(2)} + \\cdots+ \\lambda_m x_n^{(m)}\n",
    "\\end{array}\n",
    "\\right)\n",
    "&=\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right) \\\\ \\\\\n",
    "\\Leftrightarrow \\quad\n",
    "\\left(\n",
    "\\begin{array}{cccc}\n",
    "x_1^{(1)} & x_1^{(2)} & \\cdots & x_1^{(m)}\\\\\n",
    "x_2^{(1)} & x_2^{(2)} & \\cdots & x_2^{(m)}\\\\\n",
    "\\vdots\\\\\n",
    "x_n^{(1)} & x_n^{(2)} & \\cdots & x_n^{(m)}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\cdot\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\lambda_1\\\\\n",
    "\\lambda_2\\\\\n",
    "\\vdots\\\\\n",
    "\\lambda_m\n",
    "\\end{array}\n",
    "\\right)\n",
    "&=\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\\\ \\\\\n",
    "\\Leftrightarrow \\quad\n",
    "\\left(\n",
    "\\begin{array}{cccc}\n",
    "\\uparrow & \\uparrow & \\cdots & \\uparrow\\\\\n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)}\\\\\n",
    "\\downarrow & \\downarrow & \\cdots & \\downarrow\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\cdot\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\lambda_1\\\\\n",
    "\\lambda_2\\\\\n",
    "\\vdots\\\\\n",
    "\\lambda_m\n",
    "\\end{array}\n",
    "\\right)\n",
    "&=\n",
    "\\mathbf{0}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7f0kSetDwLK"
   },
   "source": [
    "In the process of the above transformation we used tacitly the definition of the matrix product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMxZjIAwDwLK"
   },
   "source": [
    "-------\n",
    "**Definition of generator set / spanning set**\n",
    "\n",
    "Let us consider the set of vectors $S = \\{x^{(1)}, x^{(2)}, \\ldots, x^{(m)}\\}$ in a vector space $V$ over a field $F$. $S$ is said to be a generator set of the vectors space $V$ if <br>\n",
    "\n",
    "$$V = \\{\\lambda_1x^{(1)} + \\lambda_2 x^{(2)} + \\cdots +\\lambda_m  x^{(m)}| \\lambda_1, \\lambda_2, \\ldots, \\lambda_m \\in F\\},$$\n",
    "\n",
    "that is every element of the vector space $V$ can be written as a linear combination of the vectors $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$.\n",
    "\n",
    "**Remark**<br>\n",
    "The set of all linear combinations that can be formed from the elements of $S$: $\\{\\lambda_1x^{(1)} + \\lambda_2 x^{(2)} + \\cdots +\\lambda_m  x^{(m)}| \\lambda_1, \\lambda_2, \\ldots, \\lambda_m \\in F\\}$, is denoted by ${\\rm span}(S)$.\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQhoaT0-DwLL"
   },
   "source": [
    "**Remark** <br>\n",
    "With the help of the new notion of spanning set we can give an equivalent definition for linear dependence and independence of vectors:\n",
    "1. The vectors $x^{(1)}, x^{(2)}, \\ldots, x^{(m)} \\in \\mathbb{R}^n$ are linearly dependent if and only if one of them belongs to the span of the others.\n",
    "2. The vectors $x^{(1)}, x^{(2)}, \\ldots, x^{(m)} \\in \\mathbb{R}^n$ are linearly independent if and only if none of them belongs to the span of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayDCpEyrDwLL"
   },
   "source": [
    "**Definition of a basis**\n",
    "\n",
    "The vectors $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$ of a vector space $V$ form a basis of $V$ if\n",
    "\n",
    "$$\\mbox{they are linearly independent}$$\n",
    "$$\\mbox{&}$$\n",
    "$$\\mbox{they form a generator set for }V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atwOUL-1DwLP"
   },
   "source": [
    "-----------\n",
    "**Remark**<br>\n",
    "If $S =\\{x^{(1)}, x^{(2)}, \\ldots, x^{(m)}\\}$ is a generator set of a vector space $V$ over a field $F$, then each element of the space can be written as a linear combination of the elements of $S$. Moreover, if the vectors contained in the generator set are linearly independent, then the coefficients of the linear combination associated to a vector are unique and in this case we call the elements of $S$ a basis.\n",
    "\n",
    "\n",
    "- The fact that $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$ are forming a generator set assures that every vector **can be expressed** w.r.t. to the $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$ (as a linear combination of $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$).\n",
    "\n",
    "- The fact that in addition $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$ are linearly independent assures that every vector from their span is **uniquely expressed** as a linear combination of $x^{(1)}, x^{(2)}, \\ldots, x^{(m)}$.\n",
    "\n",
    "We express our vectors in a basis, because this gives a representation for all vectors and two vectors are equal in this representation if and only if the corresponding coordinates / components of the two vectors are the same.\n",
    "\n",
    "**Remark about the components of a vector and the canonical basis**<br>\n",
    "The canonical basis of the vector space $\\mathbb{R}^n$ is given by the vectors\n",
    "\n",
    "$$e_1 = \\left(\n",
    "\\begin{array}{c}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right),\n",
    "e_2 = \\left(\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right), \\ldots\n",
    ", e_n = \\left(\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "A vector $x = \\left(\n",
    "\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{array}\n",
    "\\right)$ can be written as the following linear combination of the canonical vectors\n",
    "\n",
    "$$x = \\left(\n",
    "\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{array}\n",
    "\\right)\n",
    "= x_1\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right)\n",
    "+\n",
    "x_2\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right) + \\cdots +\n",
    "x_n\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right) = x_1 e_1 + x_2 e_2 + \\cdots + x_n e_n,$$\n",
    "\n",
    "Observe that the components of the vector are exactly the coordinates of the vector in the canonical basis.\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WpvinRQDwLP"
   },
   "source": [
    "**Exercise**<br>\n",
    "What are the coordinates of the vector $x = \\left(\n",
    "\\begin{array}{c}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right)$ in the basis given by the vectors $u = \\left(\n",
    "\\begin{array}{c}\n",
    "4 \\\\\n",
    "3\n",
    "\\end{array}\n",
    "\\right)$ and $v = \\left(\n",
    "\\begin{array}{c}\n",
    "-3 \\\\\n",
    "4\n",
    "\\end{array}\n",
    "\\right)$?\n",
    "\n",
    "Can you give a formula suited to calculate the coordinates / components of an arbitrary vector in the new basis of $\\mathbb{R}^2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2grclB93DwLP"
   },
   "source": [
    "## 2.3. Matrices\n",
    "\n",
    "------------\n",
    "**Definition of matrices, matrix addition, multiplication by a scalar of matrices**\n",
    "\n",
    "$n \\times m$-dimensional matrices are elements of the set $\\mathbb{R}^{n \\times m}$.\n",
    "\n",
    "We organise the elements of an $n \\times m$-dimensional matrix in $n$ rows and $m$ columns.\n",
    "\n",
    "For the notation of matrices we use often capital letters of the alphabet.\n",
    "\n",
    "For a matrix $X \\in \\mathbb{R}^{n \\times m}$ let us denote the element at the intersection of the $i$th row and $j$th column by $x_{i,j}$. Then we can define the matrix by its compenents in the following way\n",
    "\n",
    "$$X = \\left(\n",
    "\\begin{array}{cccc}\n",
    "x_{1,1} & x_{1,2} & \\cdots & x_{1,m}\\\\\n",
    "x_{2,1} & x_{2,2} & \\cdots & x_{2,m}\\\\\n",
    "\\vdots\\\\\n",
    "x_{n,1} & x_{n,2} & \\cdots & x_{n,m}\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "**Matrix addition** and **multiplication by a scalar** happens component-wise, exactly as in the case of vectors.\n",
    "The sum of the matrices $X = (x_{i,j})_{i=\\overline{1,n}, j = \\overline{1,m}} \\in \\mathbb{R}^{n \\times m}$ and $Y = (y_{i,j})_{i=\\overline{1,n}, j = \\overline{1,m}} \\in \\mathbb{R}^{n \\times m}$ is the matrix\n",
    "\n",
    "$$X+Y = (x_{i,j} + y_{i,j})_{i=\\overline{1,n}, j = \\overline{1,m}},$$\n",
    "\n",
    "that is\n",
    "\n",
    "$$X+Y =\n",
    "\\left(\n",
    "\\begin{array}{cccc}\n",
    "x_{1,1} + y_{1,1} & x_{1,2} + y_{1,2} & \\cdots & x_{1,m} + y_{1,m}\\\\\n",
    "x_{2,1} + y_{2,1} & x_{2,2} + y_{2,2} & \\cdots & x_{2,m} + y_{2,m}\\\\\n",
    "\\vdots\\\\\n",
    "x_{n,1} + y_{n,1} & x_{n,2} + y_{n,2} & \\cdots & x_{n,m} + y_{n,m}\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "For $\\lambda \\in \\mathbb{R}$  and $X = (x_{i,j})_{i=\\overline{1,n}, j = \\overline{1,m}} \\in \\mathbb{R}^{n \\times m}$ we define the mutiplication of $X$ by the scalar $\\lambda$ as the matrix\n",
    "\n",
    "$$\\lambda X = (\\lambda x_{i,j})_{i=\\overline{1,n}, j = \\overline{1,m}}$$\n",
    "\n",
    "that is\n",
    "\n",
    "$$\\lambda X = \\left(\n",
    "\\begin{array}{cccc}\n",
    "\\lambda x_{1,1} & \\lambda x_{1,2} & \\cdots & \\lambda x_{1,m}\\\\\n",
    "\\lambda x_{2,1} & \\lambda x_{2,2} & \\cdots & \\lambda x_{2,m}\\\\\n",
    "\\vdots\\\\\n",
    "\\lambda x_{n,1} & \\lambda x_{n,2} & \\cdots & \\lambda x_{n,m}\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "-----------------\n",
    "**Remark**<br>\n",
    "The set $\\mathbb{R}^{n\\times m}$ with the field $\\mathbb{R}$ and the two, above defined operations (matrix addition and multiplication by a scalar) is a vector space. To check the necessary properties, the calculation happen in the same way as in case of the vectors.\n",
    "\n",
    "**Notation**<br>\n",
    "We use the notation $\\mathbf{x_{\\cdot, j}}$ to refer to the $j$th column of a matrix.<br>\n",
    "We use the notation $\\mathbf{x_{i, \\cdot}}$ to refer to the $i$th row of a matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCY1oR3TDwLQ",
    "tags": []
   },
   "source": [
    "### 2.3.1. Matrix multiplication\n",
    "\n",
    "----------\n",
    "**Definition of matrix multiplication**\n",
    "\n",
    "The product of the matrices $A =(a_{i,j})_{i,j} \\in \\mathbb{R}^{n \\times m}$ and $B = (b_{i,j})_{i,j} \\in \\mathbb{R}^{m \\times l}$ is the matrix $A\\cdot B = \\left(c_{i,j}\\right)_{i,j} \\in \\mathbb{R}^{n \\times l}$, where\n",
    "\n",
    "$$c_{i,j} = \\sum_{k = 1}^m a_{i,k}\\cdot b_{k,j} = \\langle a_{i, \\cdot}, b_{\\cdot, j} \\rangle$$\n",
    "\n",
    "----------\n",
    "**Remark**<br>\n",
    "Observe that in the product matrix $A \\cdot B$ the element at the intersection of the $i$th row and $j$th column of the matrix can be calculated as the dot product of the $i$th row of $A$ and the $j$th column of $B$, that is\n",
    "$\\langle a_{i,\\cdot}, b_{\\cdot,j}\\rangle$ and thus <br>\n",
    "\n",
    "$$A \\cdot B = \\left(\n",
    "\\begin{array}{cccc}\n",
    "\\langle a_{1,\\cdot}, b_{\\cdot, 1} \\rangle &  \\langle a_{1,\\cdot}, b_{\\cdot, 2} \\rangle & \\cdots & \\langle a_{1,\\cdot}, b_{\\cdot, l} \\rangle\\\\\n",
    "\\langle a_{2,\\cdot}, b_{\\cdot, 1} \\rangle &  \\langle a_{2,\\cdot}, b_{\\cdot, 2} \\rangle & \\cdots & \\langle a_{2,\\cdot}, b_{\\cdot, l} \\rangle\\\\\n",
    "\\vdots\\\\\n",
    "\\langle a_{n,\\cdot}, b_{\\cdot, 1} \\rangle &  \\langle a_{n,\\cdot}, b_{\\cdot, 2} \\rangle & \\cdots & \\langle a_{n,\\cdot}, b_{\\cdot, l} \\rangle\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qte21Jo_DwLQ"
   },
   "source": [
    "**Question**\n",
    "\n",
    "a) We have seen that the dot product is bilinear and it is commutative. Are these preserved by the matrix product?\n",
    "\n",
    "b) What other properties does the matrix product have?\n",
    "\n",
    "\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiaY4ThYDwLQ"
   },
   "source": [
    "**Answer**\n",
    "\n",
    "a) The matrix product is also bilinear. That is\n",
    "\n",
    "$$(\\lambda A) \\cdot B = A \\cdot (\\lambda B) = \\lambda(A\\cdot B),$$\n",
    "\n",
    "for any matrices of matching dimension and a scalar $\\lambda$. Furthermore,\n",
    "\n",
    "$$(A+B) \\cdot C = A \\cdot C + B \\cdot C \\quad \\mbox{and} \\quad A \\cdot (B + C) = A \\cdot B + A \\cdot C$$\n",
    "\n",
    "for any matrices of matching dimension.\n",
    "\n",
    "Now we check, whether the matrix product is commutative.\n",
    "\n",
    "If matrix $A$ is $n \\times m$ dimensional and $B$ is $p \\times q$ dimensional then\n",
    "- $A \\cdot B$ is well defined if and only if $p = m$.\n",
    "- $B \\cdot A$ is well defined if and only if $q = n$.\n",
    "\n",
    "Assuming that both products $A \\cdot B$ and $B \\cdot A$ are well defined,\n",
    "- the dimension of the product matrix $A \\cdot B$ will be $n \\times n$,\n",
    "- the dimension of the product matrix $B \\cdot A$ will be $m \\times m$.\n",
    "\n",
    "The two product matrices are clearly different if $n \\neq m$. But let's assume $n = m$ also holds, that is $A$ and $B$ are both $n\\times n$ dimensional matrices. Let us compare the element on the position 1, 1 in the two product matrices\n",
    "- in $A \\cdot B$ on position 1, 1, we have the value $\\langle a_{1,\\cdot}, b_{\\cdot, 1} \\rangle$,\n",
    "- in $B \\cdot A$ on position 1, 1, we have the value $\\langle b_{1,\\cdot}, a_{\\cdot, 1} \\rangle = \\langle a_{\\cdot, 1}, b_{1, \\cdot} \\rangle$.\n",
    "\n",
    "As in general $\\langle a_{1,\\cdot}, b_{\\cdot, 1} \\rangle \\neq \\langle a_{\\cdot, 1}, b_{1, \\cdot} \\rangle$,\n",
    "we cannot use as general calculation rule that $A \\cdot B = B \\cdot A$.\n",
    "\n",
    "b) The associativity of the matrix product is an important property, that is\n",
    "\n",
    "$$A \\cdot (B \\cdot C) = (A \\cdot B) \\cdot C$$\n",
    "\n",
    "for any matrices of matching dimensions $A$, $B$ and $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWyl6cRUDwLQ"
   },
   "source": [
    "-------\n",
    "**Remark**\n",
    "\n",
    "The formulas defining the matrix addition and matrix multiplication can be completely derived from the assumption that\n",
    "* for a matrix $A = (a_{i,j})_{i,j} \\in \\mathbb{R}^{n\\times m}$ and a column vector $x = \\left(\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_m\n",
    "\\end{array}\\right) \\in \\mathbb{R}^{m}$\n",
    "\n",
    "$$\\begin{align*}\n",
    "A \\cdot x &= \\left(\n",
    "\\begin{array}{cccc}\n",
    "a_{1,1} & a_{1,2} & \\ldots & a_{1,m}\\\\\n",
    "a_{2,1} & a_{2,2} & \\ldots & a_{2,m}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{n,1} & a_{n,2} & \\ldots & a_{n,m}\n",
    "\\end{array}\\right) \\cdot \\left(\n",
    "\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\n",
    "\\right) = \\left(\n",
    "\\begin{array}{c}\n",
    "a_{1,1}\\cdot x_1 + a_{1,2}\\cdot x_2 + \\cdots + a_{1,m}\\cdot x_m\\\\\n",
    "a_{2,1}\\cdot x_1 + a_{2,2}\\cdot x_2 + \\cdots + a_{2,m}\\cdot x_m\\\\\n",
    "\\vdots\\\\\n",
    "a_{n,1}\\cdot x_1 + a_{n,2}\\cdot x_2 + \\cdots + a_{n,m}\\cdot x_m\n",
    "\\end{array}\\right) \\\\[0.5em]\n",
    "&=\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "a_{1,1}\\\\\n",
    "a_{2,1}\\\\\n",
    "\\vdots\\\\\n",
    "a_{n,1}\n",
    "\\end{array}\n",
    "\\right) \\cdot x_1 + \\left(\n",
    "\\begin{array}{c}\n",
    "a_{1,2}\\\\\n",
    "a_{2,2}\\\\\n",
    "\\vdots\\\\\n",
    "a_{n,2}\n",
    "\\end{array}\n",
    "\\right) \\cdot x_2 + \\cdots + \\left(\n",
    "\\begin{array}{c}\n",
    "a_{1,m}\\\\\n",
    "a_{2,m}\\\\\n",
    "\\vdots\\\\\n",
    "a_{n,m}\n",
    "\\end{array}\n",
    "\\right) \\cdot x_m = a_{\\cdot, 1} x_1 + a_{\\cdot, 2} x_2 + \\cdots + a_{\\cdot, m} x_m\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and the following naturally desirable properties of matrix addition and multiplication:\n",
    "* distributivity: $(A+B)\\cdot C = A\\cdot C+B\\cdot C$ and $A \\cdot (B + C) = A \\cdot B + A \\cdot C$,\n",
    "* associativity: $A \\cdot (B \\cdot C) = (A \\cdot B) \\cdot C$.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gE9P_JHDwLQ"
   },
   "source": [
    "**Explanation for the formula of matrix multiplication**\n",
    "\n",
    "If you wonder how did mathematicians come up with the above formula for matrix multiplication, you can find some reasoning here for it.\n",
    "\n",
    "Our assumption about the matrix multiplication that we would like to construct is, that it is a linear, associativity also holds and the rescaling of a matrix satisfies the following property\n",
    "$$A \\cdot (\\lambda B) = \\lambda (A \\cdot B)$$\n",
    "where $\\lambda \\in \\mathbb{R}$.\n",
    "\n",
    "Motivated by the fact that a linear system $$\\left\\{\n",
    "\\begin{array}{c}\n",
    "a_{1,1}\\cdot x_1 + a_{1,2}\\cdot x_2 + \\cdots + a_{1,m}\\cdot x_m = b_1\\\\\n",
    "a_{2,1}\\cdot x_1 + a_{2,2}\\cdot x_2 + \\cdots + a_{2,m}\\cdot x_m = b_2\\\\\n",
    "\\vdots\\\\\n",
    "a_{n,1}\\cdot x_1 + a_{n,2}\\cdot x_2 + \\cdots + a_{n,m}\\cdot x_m = b_n\n",
    "\\end{array}\\right.$$\n",
    "can be rewritten as $$\\left(\n",
    "\\begin{array}{c}\n",
    "a_{1,1}\\\\\n",
    "a_{2,1}\\\\\n",
    "\\vdots\\\\\n",
    "a_{n,1}\n",
    "\\end{array}\n",
    "\\right) \\cdot x_1 + \\left(\n",
    "\\begin{array}{c}\n",
    "a_{1,2}\\\\\n",
    "a_{2,2}\\\\\n",
    "\\vdots\\\\\n",
    "a_{n,2}\n",
    "\\end{array}\n",
    "\\right) \\cdot x_2 + \\cdots + \\left(\n",
    "\\begin{array}{c}\n",
    "a_{1,m}\\\\\n",
    "a_{2,m}\\\\\n",
    "\\vdots\\\\\n",
    "a_{n,m}\n",
    "\\end{array}\n",
    "\\right) \\cdot x_m = \\left(\n",
    "\\begin{array}{c}\n",
    "b_1\\\\\n",
    "b_2\\\\\n",
    "\\vdots\\\\\n",
    "b_n\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "or shorter as\n",
    "$$a_{\\cdot, 1} x_1 + a_{\\cdot, 2} x_2 + \\cdots + a_{\\cdot, m} x_m = b$$\n",
    "and we would like to be able to rewrite the left hand side of this matrix equation in the matrix form $A \\cdot x$, we define $A \\cdot x$ as the linear combination of the columns of $A$ with weights $x_1, x_2, \\ldots, x_m$, i.e.\n",
    "$$A \\cdot x = x_1 a_{\\cdot, 1} + x_2 a_{\\cdot, 2} + \\cdots + x_m a_{\\cdot, m}. \\qquad \\qquad (1)$$\n",
    "This definition can be written also in the following form\n",
    "$$\\begin{eqnarray}\n",
    "A \\cdot x &=& \\left(\n",
    "\\begin{array}{c}\n",
    "a_{1,1}\\\\\n",
    "a_{2,1}\\\\\n",
    "\\vdots\\\\\n",
    "a_{n,1}\n",
    "\\end{array}\n",
    "\\right) \\cdot x_1 + \\left(\n",
    "\\begin{array}{c}\n",
    "a_{1,2}\\\\\n",
    "a_{2,2}\\\\\n",
    "\\vdots\\\\\n",
    "a_{n,2}\n",
    "\\end{array}\n",
    "\\right) \\cdot x_2 + \\cdots + \\left(\n",
    "\\begin{array}{c}\n",
    "a_{1,m}\\\\\n",
    "a_{2,m}\\\\\n",
    "\\vdots\\\\\n",
    "a_{n,m}\n",
    "\\end{array}\n",
    "\\right) \\cdot x_m \\\\[1em]\n",
    "A \\cdot x &=& \\left(\\begin{array}{c}a_{1,1}\\cdot x_1 + a_{1,2}\\cdot x_2 + \\cdots + a_{1,m}\\cdot x_m\\\\\n",
    "a_{2,1}\\cdot x_1 + a_{2,2}\\cdot x_2 + \\cdots + a_{2,m}\\cdot x_m\\\\\n",
    "\\vdots\\\\\n",
    "a_{n,1}\\cdot x_1 + a_{n,2}\\cdot x_2 + \\cdots + a_{n,m}\\cdot x_m\n",
    "\\end{array}\\right)\\\\[1em]\n",
    "A \\cdot x &=& \\left(\\begin{array}{c}\\langle a_{1, \\cdot}, x\\rangle\\\\ \\langle a_{2, \\cdot}, x\\rangle\\\\ \\vdots\\\\ \\langle a_{n, \\cdot}, x\\rangle \\end{array}\\right). \\qquad \\qquad (2)\n",
    "\\end{eqnarray}$$\n",
    "Depending on the case we will use the form (1) or (2) of the above definition for $A \\cdot x$.\n",
    "\n",
    "By this we have already defined the matrix multiplication for the particular case when in $A \\cdot B$ the matrix $B$ is a column vector. Observe that this definition satisfies our previous assumptions about the matrix product. We will extend this definition also for the case, when $B$ is not necessarily a column vector.\n",
    "\n",
    "For simplicity we assume, that $A \\in \\mathbb{R}^{2 \\times 2}$ and $B \\in \\mathbb{R}^{2 \\times 3}$. We will derive the formula for the matrix multiplication $A \\cdot B$.\n",
    "\n",
    "<!---\n",
    "\n",
    "In $\\mathbb{R}^2$ every vector $v = \\left(\\begin{array}{c} v_1\\\\v_2 \\end{array}\\right)$ can be exprressed as a linear combination of the canonical basis, i.e.\n",
    "$$v = \\left(\\begin{array}{c} v_1\\\\v_2 \\end{array}\\right) = v_1 \\left(\\begin{array}{c} 1\\\\ 0 \\end{array}\\right) + v_2 \\left(\\begin{array}{c} 0\\\\ 1 \\end{array}\\right) = v_1 e_1 + v_2 e_2.$$\n",
    "\n",
    "Using the linearity assumption, one obtains\n",
    "$$ A \\cdot v = A\\cdot (v_1 e_1 + v_2 e_2).$$\n",
    "By the assumed distributivity, it yields\n",
    "$$ A \\cdot v A \\cdot (v_1e_1) + A \\cdot (v_2e_2).$$\n",
    "By the assumption that we made about rescaling it holds, that\n",
    "$$ A \\cdot v = v_1 (A \\cdot e_1) + v_2 (A \\cdot e_2).$$\n",
    "\n",
    "Observe that on the right hand side of the equation there is a linear combination.-->\n",
    "\n",
    "Let $v = \\left(\\begin{array}{c}v_1\\\\ v_2\\\\ v_3 \\end{array}\\right) \\in \\mathbb{R}^{3}$ be a column vector. By formula (1),\n",
    "$$B \\cdot v = v_1 b_{\\cdot, 1} + v_2 b_{\\cdot, 2} + v_3 b_{\\cdot, 3} \\qquad \\qquad (3)$$\n",
    "\n",
    "By the distributivity\n",
    "$$\\begin{eqnarray}\n",
    "(A \\cdot B) \\cdot v &=& A \\cdot (B \\cdot v)\\\\[1em]\n",
    "&\\stackrel{\\hbox{(3)}}{=}& A \\cdot (v_1 b_{\\cdot, 1} + v_2 b_{\\cdot, 2} + v_3 b_{\\cdot, 3})\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "By the linearity\n",
    "$$\\begin{eqnarray}\n",
    "(A \\cdot B) \\cdot v &=& A \\cdot (v_1 b_{\\cdot, 1}) + A \\cdot (v_2 b_{\\cdot, 2}) + (v_3 b_{\\cdot, 3})\\end{eqnarray},$$\n",
    "\n",
    "and\n",
    "$$\\begin{eqnarray}\n",
    "(A \\cdot B) \\cdot v &=& v_1 A \\cdot b_{\\cdot, 1} + v_2 A \\cdot b_{\\cdot, 2} + v_3 A \\cdot b_{\\cdot, 3}\\\\[1em]\n",
    "&=& \\left(\\begin{array}{ccc} \\uparrow & \\uparrow & \\uparrow\\\\ A \\cdot b_{\\cdot, 1} & A \\cdot b_{\\cdot, 2} & A \\cdot b_{\\cdot, 3}\\\\ \\downarrow & \\downarrow & \\downarrow \\end{array}\\right) \\cdot \\left( \\begin{array}{c}v_1\\\\v_2\\\\v_3\\end{array}\\right)\n",
    "\\\\[1em]\n",
    "&\\stackrel{\\hbox{(2)}}{=}& \\left(\\begin{array}{ccc} \\langle a_{1,\\cdot}, b_{\\cdot, 1}\\rangle & \\langle a_{1,\\cdot}, b_{\\cdot, 2}\\rangle  & \\langle a_{1,\\cdot}, b_{\\cdot, 3}\\rangle \\\\ \\langle a_{2,\\cdot}, b_{\\cdot, 1}\\rangle  & \\langle a_{2,\\cdot}, b_{\\cdot, 2}\\rangle  & \\langle a_{2,\\cdot}, b_{\\cdot, 3}\\rangle \\end{array}\\right) \\cdot \\left( \\begin{array}{c}v_1\\\\v_2\\\\v_3\\end{array}\\right).\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "This is the way how we can come to the idea of defining $A \\cdot B$ as $\\left(\\begin{array}{ccc} \\langle a_{1,\\cdot}, b_{\\cdot, 1}\\rangle & \\langle a_{1,\\cdot}, b_{\\cdot, 2}\\rangle  & \\langle a_{1,\\cdot}, b_{\\cdot, 3}\\rangle \\\\ \\langle a_{2,\\cdot}, b_{\\cdot, 1}\\rangle  & \\langle a_{2,\\cdot}, b_{\\cdot, 2}\\rangle  & \\langle a_{2,\\cdot}, b_{\\cdot, 3}\\rangle \\end{array}\\right)$.\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hyr-RIrXDwLR"
   },
   "source": [
    "### 2.3.2. A matrix as a linear transformation and the change of basis formula\n",
    "\n",
    "We can think of a matrix $A \\in \\mathbb{R}^{n\\times m}$ as the linear transformation $T_A$ mapping a vector $v\\in \\mathbb{R}^m$ to the image vector $A \\cdot v \\in \\mathbb{R}^n$, or shortly\n",
    "\n",
    "$$T_A: v \\to A \\cdot v$$\n",
    "\n",
    "Observe that the canonical basis vectors $e_1, e_2,\\ldots, e_m$ of $\\mathbb{R}^m$ are mapped by the above transformation to the vectors\n",
    "\n",
    "$$A\\cdot e_1 = A \\cdot \\left(\n",
    "\\begin{array}{c}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right) = a_{\\cdot, 1},  \\quad\n",
    "A \\cdot e_2 = A \\cdot \\left(\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right) = a_{\\cdot, 2}, \\quad \\ldots \\quad\n",
    "A \\cdot e_m = A \\cdot \\left(\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right) = a_{\\cdot, m}.$$\n",
    "\n",
    "Let us consider an arbitrarily chosen vector $v \\in \\mathbb{R}^m$, which with respect to the canonical basis $E = \\{e_1, e_2, \\ldots, e_m\\}$ has the coordinates $v_1, v_2, \\ldots, v_m$, i.e.\n",
    "\n",
    "$$[v]_{E} =\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots \\\\\n",
    "v_m\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "By the considered matrix transformation $T_A$ this will be mapped to the vector $v'$, which w.r.t. the canonical basis has as coordinates the elements of the vector $A\\cdot v$, i.e. <br>\n",
    "\n",
    "$$\\begin{align*}\n",
    "[v']_E = A \\cdot v = A \\cdot \\left(\n",
    "\\begin{array}{c}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots \\\\\n",
    "v_m\n",
    "\\end{array}\n",
    "\\right) &= A \\cdot (v_1e_1+v_2e_2+\\cdots+v_me_m)\\\\ \\\\\n",
    "&= A \\cdot v_1e_1 + A \\cdot v_2e_2 + \\cdots + A \\cdot v_me_m \\\\ \\\\\n",
    "&= v_1A \\cdot e_1 + v_2A\\cdot e_2 + \\cdots + v_m A\\cdot e_m \\\\ \\\\\n",
    "&= v_1a_{\\cdot, 1} + v_2a_{\\cdot, 2}+ \\cdots+ v_ma_{\\cdot, m},\n",
    "\\end{align*}$$\n",
    "<br>\n",
    "\n",
    "where in the last step we used our previous observation that $A \\cdot e_j = a_{\\cdot, j}$ for any index $j \\in \\{1, 2, \\ldots m\\}$. By the definition of the span,<br>\n",
    "\n",
    "$$[v']_E = v_1a_{\\cdot, 1} + v_2a_{\\cdot, 2}+ \\cdots+ v_ma_{\\cdot, m} \\in span(\\{a_{\\cdot, 1}, a_{\\cdot, 2}, \\ldots, a_{\\cdot, m}\\})$$\n",
    "\n",
    "This shows that every element of the image space can be written as a linear combination of the vectors $a_{\\cdot, 1}, a_{\\cdot, 2}, \\ldots, a_{\\cdot, m}$, thus $\\{a_{\\cdot, 1}, a_{\\cdot, 2}, \\ldots, a_{\\cdot, m}\\}$ is a generator set of the images of the matrix transformation. If the column vectors $a_{\\cdot, 1}, a_{\\cdot, 2}, \\ldots, a_{\\cdot, m}$ are also linearly inedependent, then they will form also a basis of the image space. Let us assume that this is the case and by the transformation the canonical basis gets mapped into the basis of the image space, the elments of which are the columns of $A$.\n",
    "\n",
    "Observe also that the matrix transformation associates to $v$ the vector $v'$, i.e.\n",
    "$$[v]_E = \\left(\n",
    "\\begin{array}{c}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots \\\\\n",
    "v_m\n",
    "\\end{array}\n",
    "\\right) \\mapsto [v']_{\\{a_{\\cdot, 1}, a_{\\cdot, 2}, \\ldots, a_{\\cdot, m}\\}} = \\left(\n",
    "\\begin{array}{c}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots \\\\\n",
    "v_m\n",
    "\\end{array}\n",
    "\\right) $$\n",
    "\n",
    "We derive the change of basis formula from the observation that if the coordinates of a vector w.r.t. the basis $\\{a_{\\cdot, 1}, a_{\\cdot, 2}, \\ldots, a_{\\cdot, m}\\}$ are $v_1, v_2, \\ldots, v_m$, then the canonical coordinates of this vector are $a_{\\cdot, 1} v_1 + a_{\\cdot, 2} v_2 +  \\cdots a_{\\cdot, 1} v_m = A \\cdot \\left(\n",
    "\\begin{array}{c}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots \\\\\n",
    "v_m\n",
    "\\end{array}\n",
    "\\right) = A \\cdot v$, that is\n",
    "\n",
    "$$ \\mathbf{[v']_E} = a_{\\cdot, 1} v_1 + a_{\\cdot, 2} v_2 +  \\cdots a_{\\cdot, 1} v_m = A \\cdot \\left(\n",
    "\\begin{array}{c}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots \\\\\n",
    "v_m\n",
    "\\end{array}\n",
    "\\right) = \\mathbf{A \\cdot [v']_{\\{a_{\\cdot, 1}, a_{\\cdot, 2}, \\ldots, a_{\\cdot, m}\\}}}$$\n",
    "\n",
    "Focusing just on the emphasised parts of the above equality, we obtain\n",
    "\n",
    "$$[v']_E = A \\cdot [v']_{\\{a_{\\cdot, 1}, a_{\\cdot, 2}, \\ldots, a_{\\cdot, m}\\}}$$\n",
    "\n",
    "This formula is a relation between the coordinates of the same vector in the two bases, which are linked by the matrix $A$. That's why this formula is called the **change of basis formula** and the matrix $A$ is called the **change of basis matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BIjwpAmDwLR"
   },
   "source": [
    "As the basis of the original vector space is mapped into the basis of the image space, to visualise geometrically the transformation produced by $T_1$, we often take a look at how does the canonical basis changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "AORXPExrDwLR",
    "outputId": "d94621da-aee9-4dbe-addd-427b1018747c"
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.geogebra.org/m/VVdWf2fe\", 600, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joxZxZ9TDwLS"
   },
   "source": [
    "**Question**\n",
    "\n",
    "- What did you observe in the above video?\n",
    "- What is the image of a line after a linear transformation?\n",
    "- What features do change with the transformation?\n",
    "- What kind of features stay unchanged?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hP88vT5EDwLS"
   },
   "source": [
    "<!--Let $b^{(1)}, b^{(2)}, \\ldots b^{(n)}$ be the new basis of $\\mathbb{R}^m$. If a vector $v$ is given in this basis by the coordinates $x_1, x_2, \\ldots, x_n$, then this vector can be written in the canonical basis as\n",
    "\n",
    "$$ v = x_1 b_1 + x_2 b_2 + \\cdots + x_n b_n =  \n",
    "\\left(\n",
    "\\begin{array}{cccc}\n",
    "\\uparrow & \\uparrow & \\cdots & \\uparrow\\\\\n",
    "b^{(1)} & b^{(2)} & \\cdots & b^{(n)}\\\\\n",
    "\\downarrow & \\downarrow & \\cdots & \\downarrow\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\cdot \\left(\n",
    "\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "and this equals to a column vector, where the components are the coordinates of the vector in the canonical basis.\n",
    "\n",
    "$$\\left(\n",
    "\\begin{array}{cccc}\n",
    "\\uparrow & \\uparrow & \\cdots & \\uparrow\\\\\n",
    "b^{(1)} & b^{(2)} & \\cdots & b^{(n)}\\\\\n",
    "\\downarrow & \\downarrow & \\cdots & \\downarrow\n",
    "\\end{array}\n",
    "\\right) \\cdot v_{B =\\{b_1, b_2,\\ldots, b_n\\}} = v_{E =\\{e_1, e_2,\\ldots, e_n\\}} $$\n",
    "-->\n",
    "\n",
    "In the below interactive window you can see the effect of transforming the elements of $\\mathbb{R}^2$ by multiplying them from the left by the marix $m$. The canonical basis $e_1$, $e_2$ is transformed into the basis $e_1'$, $e_2'$. You can change the transformation by moving the tips of $e_1'$, $e_2'$. Meanwhile you also see how does the vector $w$ get transformed. You can also change the vector $w$ by moving its tip.\n",
    "\n",
    "**Question**\n",
    "\n",
    "Try to create some degenerate transformation. How is $m$ in such cases?  \n",
    "\n",
    "**Answer**\n",
    "\n",
    "When the new basis vectors $e_1'$ and $e_2'$ are parallel, then the image space is reduced to a one dimensional space, as both canonical basis elements $e_1$ and $e_2$ are mapped into the same vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "UFXbJoRdDwLS",
    "outputId": "70ff391c-15b7-4a8d-9bb8-8e97e40abc16"
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/ncp2peap\", 1000, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-wCMu4PDwLS"
   },
   "source": [
    "### 2.3.3. Gaussian elimination to solve a system of linear equations\n",
    "\n",
    "---------------\n",
    "**Exercise 1**<br>\n",
    "Calculate the solution of the following linear system\n",
    "$$\\left\\{\n",
    "\\begin{align*}\n",
    "x + 2y &= 19\\\\\n",
    "3y &= 6\n",
    "\\end{align*}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "---------------\n",
    "\n",
    "If we have to solve a linear equation system like<br>\n",
    "\n",
    "$$\\left\\{\n",
    "\\begin{align*}\n",
    "2x + 3y &= 19\\\\\n",
    "x - 6y &= -28\n",
    "\\end{align*}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "one of the first techniques that we learn is the elimination of one of the variables from one equation. We can achieve this by adding a proper mutiple of the other equation to the current one.\n",
    "\n",
    "Let's eliminate the variable $y$ from the second equation. For this we mutiply the first equation by $2$ and add the so resulting equation to the second one.<br>\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{align*}\n",
    "{\\rm Eq}_1: \\quad 2x + 3y &= 19\\\\\n",
    "2{\\rm Eq}_1 + {\\rm Eq}_2:\\quad 5x &= 10\n",
    "\\end{align*}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Now we have obtained a system, which has a triangular form. Such systems are easy to solve by calculating the second variable, then sustituting its value in the first equation.\n",
    "\n",
    "**Exercise 2**<br>\n",
    "Calculate the solution of the following linear system\n",
    "$$\\left\\{\n",
    "\\begin{align*}\n",
    "x - 2y + 3z &= 9\\\\\n",
    " 2y + z &= 0\\\\\n",
    " -3z &= -6\n",
    "\\end{align*}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "The same approach as by the previous system works also here:\n",
    "- first we determine the value of $z$ from the last equation,\n",
    "- then we substitute $z$ into the previous two equations,\n",
    "- from the second equation now we can determine $y$,\n",
    "- we substitue the value of $y$ also into the first equation,\n",
    "- from the first equation we can calculate $x$.\n",
    "\n",
    "This approach can be extended to arbitrarily large systems, as well.\n",
    "\n",
    "**Exercise 3**<br>\n",
    "Calculate the solution of the following linear system\n",
    "$$\\left\\{\n",
    "\\begin{align*}\n",
    "x + 2y + 3z = 8\\\\\n",
    "3x + 2y + z = 4\\\\\n",
    "2x + 3y + z = 5\n",
    "\\end{align*}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "We are going to solve this system by applying row transformations to it, which help to transform the system equivalently into one of a triangular or diagonal form. Then we can apply the procedure from before ( in case of triangular form) or just read the solution (in case of diagonal form). This method of solving the system is called Gaussian elimination.\n",
    "\n",
    "You can learn the steps of the Gaussian elimination by practicing it with the test tool made you available on Ilias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOl4xJPJDwLS"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Calculate the solution of the previous croissant, coffe, juice problem\n",
    "\n",
    "$$\\left\\{\\begin{align}\n",
    "&5\\cdot x_1 + 4 \\cdot x_2 + 3 \\cdot x_3 = 32.3 \\\\\n",
    "&4\\cdot x_1 + 5 \\cdot x_2 + 3 \\cdot x_3 = 32.5 \\\\\n",
    "&6\\cdot x_1 + 5 \\cdot x_2 + 2 \\cdot x_3 = 31\n",
    "\\end{align}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7Zo_3Q7DwLS"
   },
   "source": [
    "\n",
    "\n",
    "### 2.3.4. Inverse of a matrix\n",
    "\n",
    "---------\n",
    "\n",
    "**Question**\n",
    "What is the product of the matrices\n",
    "$I = \\left(\n",
    "\\begin{array}{cc}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{array}\n",
    "\\right)$ and $A =\n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "2 & 3\\\\\n",
    "4 & 5\n",
    "\\end{array}\n",
    "\\right)?\n",
    "$\n",
    "\n",
    "Our experience from the above exercise is generalised in the following definition.\n",
    "\n",
    "----------\n",
    "**Definition of the unit matrix**\n",
    "\n",
    "The square matrix $I_n \\in \\mathbb{R}^{n \\times n}$ having ones just on the diagonal and all other elements being equal to $0$ has the property that\n",
    "\n",
    "$$I_n \\cdot A = A \\cdot I_n = A$$\n",
    "\n",
    "for any $A \\in \\mathbb{R}^{n \\times n}$.\n",
    "\n",
    "-------------\n",
    "**Definition of the inverse matrix**\n",
    "\n",
    "A square matrix $A \\in \\mathbb{R}^{n \\times n}$ is invertible if there exists a square matrix $A^{-1}\\in \\mathbb{R}^{n\\times n}$ such that\n",
    "\n",
    "$$A \\cdot A^{-1} = A^{-1} \\cdot A = I_n.$$\n",
    "\n",
    "The matrix $A^{-1}$ with the above properties is the inverse matrix of $A$.\n",
    "\n",
    "--------------\n",
    "\n",
    "**Question**\n",
    "\n",
    "Do all matrices have an inverse?\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "**Remark**<br>\n",
    "Once we have introduced the notion of inverse matrix, another possibility to solve a linear equation of the form\n",
    "\n",
    "$$Ax = b$$\n",
    "\n",
    "is to multiply from the left by the inverse of the matrix $A$ and obtain\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A^{-1} \\cdot (A \\cdot x) &= A^{-1}\\cdot b\\\\\n",
    "(A^{-1} \\cdot A) \\cdot x &= A^{-1} \\cdot b\\\\\n",
    "I_n \\cdot x& = A^{-1} \\cdot b\\\\\n",
    "x &= A^{-1}\\cdot b\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH6viOTdDwLT"
   },
   "source": [
    "----------------\n",
    "**The inverse caculated by Gaussian elimination**\n",
    "\n",
    "Determine the inverse of the matrix\n",
    "\n",
    "$$A =\n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "1 & 2 & 3\\\\\n",
    "3 & 2 & 1\\\\\n",
    "2 & 3 & 1\n",
    "\\end{array}\n",
    "\\right).$$\n",
    "\n",
    "The matrix we are looking for satisfies the following equation\n",
    "$$A \\cdot A^{-1} = I_3.$$\n",
    "\n",
    "Let us denote the columns of the inverse matrix $A^{-1}$ by $x,y,z \\in \\mathbb{R}^3$. We are going to determine them from the relation\n",
    "\n",
    "$$A \\cdot\n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "\\uparrow & \\uparrow & \\uparrow\\\\\n",
    "x & y & z\\\\\n",
    "\\downarrow & \\downarrow & \\downarrow\n",
    "\\end{array}\n",
    "\\right)\n",
    "= \\left(\n",
    "\\begin{array}{ccc}\n",
    "1 & 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Observe that the above relationship is equivalent to\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "\\uparrow & \\uparrow & \\uparrow\\\\\n",
    "A\\cdot x & A\\cdot y & A \\cdot z\\\\\n",
    "\\downarrow & \\downarrow & \\downarrow\n",
    "\\end{array}\n",
    "\\right) = \\left(\n",
    "\\begin{array}{ccc}\n",
    "\\uparrow & \\uparrow & \\uparrow\\\\\n",
    "e_1 & e_2 & e_3\\\\\n",
    "\\downarrow & \\downarrow & \\downarrow\n",
    "\\end{array}\n",
    "\\right)\\\\\n",
    "\\\\\n",
    "A\\cdot x = e_1\\quad \\mbox{and} \\quad A \\cdot y = e_2 \\quad \\mbox{and} \\quad A \\cdot z = e_3\n",
    "\\end{align*},$$\n",
    "\n",
    "where we have used the fact that the $i$th column ($i \\in \\{1, 2, 3\\}$) of the product matrix is obtained by multiplying the first matrix just by the $i$th column of the second matrix.\n",
    "\n",
    "Observe that we can solve the above three equations and calculate the vectors $x$, $y$ and $z$ following the very same procedure of Gaussian elimination. Due to the fact that the coefficient matrix $A$ is the same for all three equations, we can perform these three Gaussian eliminations in parallel.\n",
    "\n",
    "<!--To see how does this happen step by step solve the practice test on Ilias.-->\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChV-FxdvDwLT"
   },
   "source": [
    "### 2.3.5. The determinant\n",
    "\n",
    "---------\n",
    "**Definition of the determinant (for a $2\\times 2$ matrix)**\n",
    "\n",
    "The determinant of the matrix\n",
    "\n",
    "$$A = \\left(\n",
    "\\begin{array}{cc}\n",
    "a & b\\\\\n",
    "c & d\n",
    "\\end{array}\n",
    "\\right) \\in \\mathbb{R}^{2 \\times 2}$$\n",
    "\n",
    "is denoted by $\\det(A)$ and can be calculated as $a\\cdot d - b \\cdot c$.\n",
    "\n",
    "-------------------\n",
    "-------------------\n",
    "**Exercise**\n",
    "\n",
    "Playing with the below animation discover the geometric interpretation of the determinant.\n",
    "\n",
    "You can change the vectors $b_1$ and $b_2$ by dragging their end points to some new position.\n",
    "\n",
    "a) What changes occur in the yellow area, in the matrix $M$ and in $\\det(M)$ for the following set-ups?\n",
    "\n",
    "- $b_1 = (4,0), b_2 = (0,1)$.\n",
    "- $b_1 = (1,0), b_2 = (0,3)$.\n",
    "- $b_1 = (4,0), b_2 = (0,3)$.\n",
    "- $b_1 = (4,0), b_2 = (1,3)$.\n",
    "- $b_1 = (4,2), b_2 = (1,3)$.\n",
    "\n",
    " What is the relationship between the yellow area and the determinant of $M$?\n",
    "    \n",
    "b) What do you observe for $b_1 = (-1,0), b_2 = (0,2)$? Based on your observation do you recommend any adjustment in the claim relating the yellow area to the determinant of $M$?\n",
    "\n",
    "c) What is the relationship between the grey and the yellow area? Can you give another interpretation to the determinant using this new observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "M1VicLaTDwLT",
    "outputId": "3c79a8ae-e317-460a-e869-87ba81deed64"
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/tkqwvxnk\", 1000, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2Z7yhufDwLT"
   },
   "source": [
    "**The absolut value of the determinant and the area of the parallelepiped**\n",
    "\n",
    "Observe that the above determinant is exactly the area of the parallelogram spanned by the vectors\n",
    "$\\left(\n",
    "\\begin{array}{c}\n",
    "a\\\\\n",
    "c\n",
    "\\end{array}\n",
    "\\right)$ and $\\left(\n",
    "\\begin{array}{c}\n",
    "b\\\\\n",
    "d\n",
    "\\end{array}\n",
    "\\right)$\n",
    "if the second vector is situated into a positive direction w.r.t. the first one. Otherwise the determinant has a negative sign, but its absolute value is still equal to the area.<br><br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/KingaS03/Mathematics-for-Machine-Learning-and-Data-Science/blob/master/Images/Determinant.png?raw=1\" width=\"200\">\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "The exact definition of the determinant for higher dimensional matrices is overwhelming. For our calculations it is enough to know that the absolute value of it is always equal to the volume of the parallelepiped spanned by the column vectors of the matrix.\n",
    "\n",
    "---------\n",
    "**Characterisation of linear dependence with the help of the determininant**\n",
    "\n",
    "A reformulation of the definition of linear dependency says that the vectors $v_1, v_2, \\ldots, v_n \\in \\mathbb{R}^n$ are linearly dependent if they do not span the whole space $\\mathbb{R}^n$. This situation is characterised by the fact that the parallelepiped spanned by these vectors is degenerate and its volume is 0.\n",
    "\n",
    "Let us denote by $V$ the matrix containing on its columns the vectors $v_1, v_2, \\ldots, v_n$. By the relation between the volume of the parallelepiped mentioned before and $\\det(V)$, we can claim the following\n",
    "\n",
    "\"The vectors $v_1, v_2, \\ldots, v_n \\in \\mathbb{R}^n$ are linearly dependent if and only if $\\det(V) = 0$, where $V$ denotes the matrix containing on its column the vectors $v_1, v_2, \\ldots, v_n$.\"\n",
    "\n",
    "Furthermore, equivalently\n",
    "\n",
    "\"The vectors $v_1, v_2, \\ldots, v_n \\in \\mathbb{R}^n$ are linearly independent is and only if $\\det(V) \\neq 0$, where $V$ is the matrix containing on its column the vectors $v_1, v_2, \\ldots, v_n$.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UqNgv7yDwLT"
   },
   "source": [
    "<!--**Exercise**\n",
    "\n",
    "Solve the linear system\n",
    "\n",
    "$$\\left\\{\\begin{align}\n",
    "x + y &= 1 \\\\\n",
    "2\\cdot x - 3 \\cdot y - 5 \\cdot z &= -1 \\\\\n",
    "4\\cdot x - y -5 \\cdot z &=0\n",
    "\\end{align}\\right.$$-->\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Let us consider a $2 \\times 2$ dimensional real matrix with non-zero determinant given as $A = \\left(\n",
    "\\begin{array}{cc}\n",
    "a & b\\\\\n",
    "c & d\n",
    "\\end{array}\n",
    "\\right)$\n",
    "\n",
    "Show that\n",
    "\n",
    "$$A^{-1} = \\frac{1}{ad-bc}\\left(\n",
    "\\begin{array}{cc}\n",
    "d & -b\\\\\n",
    "-c & a\n",
    "\\end{array}\n",
    "\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSsSGOA9DwLT"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "The reference vectors (basis vectors) used by our friend to express vectors of $\\mathbb{R}^2$ are different from the canonical ones: $u = \\left( \\begin{array}{c}\n",
    "2\\\\\n",
    "3\n",
    "\\end{array}\n",
    "\\right)$, $v = \\left(\\begin{array}{c}\n",
    "3\\\\\n",
    "4\n",
    "\\end{array}\n",
    "\\right)$. He would like to perform a rotation by $60^$ in his coordinate system.\n",
    "\n",
    "We know that in the canonical coordinate system a rotation can be achieved by mutiplying the vector from left by the rotation matrix<br>\n",
    "\n",
    "$$R_{60^} = \\left(\n",
    "\\begin{array}{cc}\n",
    "\\cos(60^) & -\\sin(60^)\\\\\n",
    "\\sin(60^) & \\cos(60^)\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "Can you give a formula that would perform the rotation in the basis of our friend?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "We know the transformation matrix describing the phenomenon in the canonical basis:\n",
    "\n",
    "$$R_{60^} = \\left(\n",
    "\\begin{array}{cc}\n",
    "\\cos(60^) & -\\sin(60^)\\\\\n",
    "\\sin(60^) & \\cos(60^)\n",
    "\\end{array}\n",
    "\\right) = \\left(\n",
    "\\begin{array}{cc}\n",
    "\\frac{1}{2} & -\\frac{\\sqrt{3}}{2}\\\\\n",
    "\\frac{\\sqrt{3}}{2} & \\frac{1}{2}\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "We need to rewrite it in the basis determined by the vectors $u$ and $v$. Namely we would like to provide the matrix transformation that maps a vector given in the basis of the friend into the image vector (which is just the rotation by $60^$ of the starting vector) expressed also in the basis of the friend.\n",
    "\n",
    "The recipe is the following:\n",
    "- We start with a vector $x$ expressed in the basis of the friend as\n",
    "$\\left(\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\\right)$, i.e. $[x]_{\\{u,v\\}} = \\left(\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\\right)$.\n",
    "- We express its coordinates w.r.t. the canonical basis by the change of basis formula\n",
    "\n",
    "$$[x]_{\\{e_1,e_2\\}} = \\left(\\begin{array}{cc}\n",
    "2 & 3\\\\\n",
    "3 & 4\n",
    "\\end{array}\\right) \\cdot [x]_{\\{u,v\\}} = \\left(\\begin{array}{cc}\n",
    "2 & 3\\\\\n",
    "3 & 4\n",
    "\\end{array}\\right) \\cdot\n",
    "\\left(\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "- We apply the transformation matrix, which describes the rotation in the canonical basis. The coordinates of the rotated vector in the canonical basis will be\n",
    "\n",
    "$$\\left(\n",
    "\\begin{array}{cc}\n",
    "\\frac{1}{2} & -\\frac{\\sqrt{3}}{2}\\\\\n",
    "\\frac{\\sqrt{3}}{2} & \\frac{1}{2}\n",
    "\\end{array}\n",
    "\\right) \\cdot \\left(\\begin{array}{cc}\n",
    "2 & 3\\\\\n",
    "3 & 4\n",
    "\\end{array}\\right) \\cdot\n",
    "\\left(\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "- Lastly we express the rotated vector in the basis of the friend. As the transformation from the coordinates of the friend to the canonical ones is described by the matrix $\\left(\\begin{array}{cc}\n",
    "2 & 3\\\\\n",
    "3 & 4\n",
    "\\end{array}\\right)$, the inverse mapping transforming the canonical coordinates of a vector to the coordinates w.r.t. the basis of the friend is described by the inverse of this mapping, that is the inverse of the matrix $\\left(\\begin{array}{cc}\n",
    "2 & 3\\\\\n",
    "3 & 4\n",
    "\\end{array}\\right)$. Thus the result vector expressed in the coordinates of the friend is\n",
    "\n",
    "$$\\left(\\begin{array}{cc}\n",
    "2 & 3\\\\\n",
    "3 & 4\n",
    "\\end{array}\\right)^{-1} \\cdot \\left(\n",
    "\\begin{array}{cc}\n",
    "\\frac{1}{2} & -\\frac{\\sqrt{3}}{2}\\\\\n",
    "\\frac{\\sqrt{3}}{2} & \\frac{1}{2}\n",
    "\\end{array}\n",
    "\\right) \\cdot \\left(\\begin{array}{cc}\n",
    "2 & 3\\\\\n",
    "3 & 4\n",
    "\\end{array}\\right) \\cdot\n",
    "\\left(\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "Finally the mapping\n",
    "\n",
    "$$\\left(\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\\right) \\mapsto \\left(\\begin{array}{cc}\n",
    "2 & 3\\\\\n",
    "3 & 4\n",
    "\\end{array}\\right)^{-1} \\cdot \\left(\n",
    "\\begin{array}{cc}\n",
    "\\frac{1}{2} & -\\frac{\\sqrt{3}}{2}\\\\\n",
    "\\frac{\\sqrt{3}}{2} & \\frac{1}{2}\n",
    "\\end{array}\n",
    "\\right) \\cdot \\left(\\begin{array}{cc}\n",
    "2 & 3\\\\\n",
    "3 & 4\n",
    "\\end{array}\\right) \\cdot\n",
    "\\left(\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "is the linear mapping given by the matrix\n",
    "\n",
    "$$\\left(\\begin{array}{cc}\n",
    "2 & 3\\\\\n",
    "3 & 4\n",
    "\\end{array}\\right)^{-1} \\cdot \\left(\n",
    "\\begin{array}{cc}\n",
    "\\frac{1}{2} & -\\frac{\\sqrt{3}}{2}\\\\\n",
    "\\frac{\\sqrt{3}}{2} & \\frac{1}{2}\n",
    "\\end{array}\n",
    "\\right) \\cdot \\left(\\begin{array}{cc}\n",
    "2 & 3\\\\\n",
    "3 & 4\n",
    "\\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUGuv8zbDwLT"
   },
   "source": [
    "In general, if $C$ is the change of basis matrix describing the tranformation from the coordinates given in the basis $\\{c_{\\cdot, 1}, \\ldots, c_{\\cdot, m}\\}$ to the canonical coordinates, i.e.\n",
    "\n",
    "$$C\\cdot [x]_{\\{c_{\\cdot, 1}, \\ldots, c_{\\cdot, m}\\}} = [x]_{\\{e_{\\cdot, 1}, \\ldots, e_{\\cdot, m}\\}}$$\n",
    "\n",
    "and $T$ is the matrix which describes the desired transformation in the canonical basis, then the same transformation w.r.t. the basis $\\{c_{\\cdot, 1}, \\ldots, c_{\\cdot, m}\\}$ is described by the matrix\n",
    "\n",
    "$$C^{-1} \\cdot T \\cdot C.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpG8WAS4DwLU"
   },
   "source": [
    "### 2.3.6. Orthogonal matrices and Gram-Schmidt orthogonalisation\n",
    "\n",
    "--------\n",
    "**Definition of orthogonal matrices**\n",
    "\n",
    "A square matrix $Q \\in \\mathbb{R}^{n\\times n}$ is said to be orthogonal if\n",
    "\n",
    "$$Q \\cdot Q^T = Q^T \\cdot Q= I_n.$$\n",
    "\n",
    "This is equivalent to the fact that the rows of the matrix $Q$ form an orthogonal basis of $\\mathbb{R}^n$.\n",
    "\n",
    "Furthermore, this is also equivalent to the fact that the columns of the matrix $Q$ form an orthogonal basis of $\\mathbb{R}^n$.\n",
    "\n",
    "---------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_FR_nbZDwLU"
   },
   "source": [
    "**Remark**\n",
    "\n",
    "Orthogonal matrices have the nice property that their transposed is their inverse. That's why we like to work with them more than with other matrices. Consequently we prefer also the orthonormal bases to the other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPAkKrNvDwLU"
   },
   "source": [
    "---\n",
    "**Definition of orthogonal basis and orthonormal basis**\n",
    "\n",
    "A basis $\\{b_1, b_2, \\ldots, b_n\\}$ of a vector space $V$ is called **orthogonal** if the basis vectors are pairwise orthogonal to each other, i.e. $\\langle b_i, b_j \\rangle = 0$ for any $i \\neq j$.\n",
    "\n",
    "If in addition each basis vector has norm 1, then the basis is called **orthonormal**.\n",
    "\n",
    "------\n",
    "\n",
    "How can we transform a basis into an orthonormal basis?\n",
    "\n",
    "The below video shows us exactly such an orthogonalisation.\n",
    "\n",
    "**Question**\n",
    "\n",
    "What do you observe? Identify the steps that are performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "Baz9WvqbDwLU",
    "outputId": "b068c1fb-0b38-418f-da6b-638381c2d976"
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/cyvs2mur\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyvwulYXDwLU"
   },
   "source": [
    "The process you could observe in the video is called Gram-Schmidt orthogonalisation. In the forthcoming we explain the steps in detail.\n",
    "\n",
    "------------------------\n",
    "\n",
    "**Gram-Schmidt orthogonalisation**\n",
    "\n",
    "Let's see how can we transform a basis to an orthonormal basis in $\\mathbb{R}^n$.\n",
    "\n",
    "Let us consider a vector space $V$ with a basis given by the vectors $x_1, x_2, \\ldots x_n$. We will modify the basis iteratively, changing at each step just one of the elements of the basis.\n",
    "\n",
    "- Step 1: We change the basis element $x_1$ by normalising it. The new basis is now compound of\n",
    "\n",
    " $$u_1 = \\frac{x_1}{||x_1||}, x_2, \\ldots, x_n$$\n",
    "\n",
    "- Step 2: We modify the basis element $x_2$. We would like that this new basis element $v_2$ is orthogonal to the first one and ${\\rm span}(\\{u_1, v_2\\}) = {\\rm span}(\\{u_1, x_2\\})$. From this condition we get that $v_2$ should be the projection of $x_2$ to the orthogonal complement of $\\{x_1\\}$ or equivalently $v_2 = x_2 - \\pi_{u_1}{x_2}$\n",
    "\n",
    "The new basis is now compound of\n",
    "\n",
    " $$u_1 = \\frac{x_1}{||x_1||}, v_2 = x_2 - \\pi_{u_1}{x_2}, x_3 \\ldots, x_n$$\n",
    "\n",
    "- Step 3: We normalise $v_2$. The new basis is now compound of\n",
    "\n",
    " $$u_1, u_2 = \\frac{v_2}{||v_2||}, x_3 \\ldots, x_n$$\n",
    "\n",
    "- Step 4: We modify the basis element $x_3$. We would like that this new basis element $v_3$ is orthogonal to the previous two ones $u_1$ and $u_2$ and ${\\rm span}(\\{u_1, u_2, v_3\\}) = {\\rm span}(\\{u_1, u_2, x_3\\})$. From this condition we get that\n",
    "\n",
    "$$v_3 = x_3 - \\pi_{u_1}(x_3) - \\pi_{u_2}(x_3)$$\n",
    "\n",
    "The new basis is compound of\n",
    "\n",
    " $$u_1, u_2, v_3 = x_3 - \\pi_{u_1}(x_3) - \\pi_{u_2}(x_3), x_4 \\ldots, x_n$$\n",
    "\n",
    "- Step 5: We normalise $v_3$. The new basis is\n",
    "\n",
    " $$u_1, u_2, u_3 = \\frac{v_3}{||v_3||}, x_4 \\ldots, x_n$$\n",
    "\n",
    " We continue this process until all the basis vectors are exchanged.\n",
    "\n",
    "**Remark**<br>\n",
    "Observe that the above transformations are all such linear transformations that preserve the linear independence of the vectors of the basis in each step. This is crutial to make sure that we end up also with a basis.\n",
    "\n",
    "**Remark**<br>\n",
    "If we start to orthogonalise a set of vectors about which we don't know whether they are linearly independent, then during the iterative transformation it might happen that some of the new vectors will be $\\mathbf{0}$. We realise this latest when we would like to normalise this vector and it turns out that its norm is 0. If we encounter this situation, this means that the original vectors that we started with were not linearly independent.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Determine the transformation matrix, which is describing the reflection w.r.t. the vectors\n",
    "$\\left(\n",
    "\\begin{array}{c}\n",
    "1\\\\\n",
    "0\\\\\n",
    "2\n",
    "\\end{array}\n",
    "\\right)$ and $\\left(\n",
    "\\begin{array}{c}\n",
    "0\\\\\n",
    "2\\\\\n",
    "2\n",
    "\\end{array}\n",
    "\\right)$.\n",
    "\n",
    "<!--To practice the Gram-Schmidt orthogonalisation you can solve the test provided on Ilias.-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPcLwcmRDwLU"
   },
   "source": [
    "### 2.3.7. Eigenvectors, eigenvalues\n",
    "<!--\"eigen\" = \"characteristic\"-->\n",
    "\n",
    "Eigenvectors of a matrix $A$ are the vectors, which are just scaled by a factor when applying the matrix transformation imposed by the matrix $A$ on them. The eigenvalues are the corresponding factors.\n",
    "\n",
    "Use this informal definition of eigenvalues and eigenvectors to answer the following exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Quzp-ocODwLU"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Associate the following transformations in $\\mathbb{R}^2$ to the number of eigenvectors that they have:\n",
    "1. Rotation in $\\mathbb{R}^2$\n",
    "2. Scaling along one axis\n",
    "3. Scaling along 2 axes\n",
    "4. Multiplication by the identity matrix\n",
    "\n",
    "Number of eigenvectors:<br>\n",
    "a. every vector is an eigenvector<br>\n",
    "b. the transformation has exactly 2 eigenvectors<br>\n",
    "c. the transformation has exactly one eigenvector<br>\n",
    "d. the transformation can have none or two eigenvectors\n",
    "\n",
    "<!--**Exercise**\n",
    "\n",
    "Diagonalisation by changing the basis to the eigenvectors\n",
    "Application to calculating the $n$th power of a matrix. We can be interested in this question when $T$ is s transition matrix encorporating the change that happens in one time unit. Then $T^n$ shows the change happening in $n$ time units. If we can find a basis, where the matrix is diagonal, then calculate its $n$th power and afterwards transform it back, it is easier than calculating the $n$th power of the original matrix.-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7MP0cTBDwLU"
   },
   "source": [
    "It can be handy to use the below tool for visualising the eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "O8wFHYR3DwLV",
    "outputId": "7ec45976-b5ed-45c2-e0e4-2827f9e6dfc2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/ncp2peap\", 1000, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbM-0zaZDwLV"
   },
   "source": [
    "------------\n",
    "**Formal definition of eigenvectors and eigenvalues**\n",
    "\n",
    "Let us consider a square matrix $A \\in \\mathbb{R}^{n \\times n}$. We say that the non-zero vector $v \\in \\mathbb{R}^n$ is an eigenvector of $A$ and $\\lambda$ is the associated eigenvalue if\n",
    "\n",
    "$$A \\cdot v = \\lambda v.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6F5vVZxfDwLV"
   },
   "source": [
    "**Remark**<br>\n",
    "As a consequence of the above definition an eigenvalue $\\lambda$ should satisfy\n",
    "\n",
    "$$A\\cdot v-\\lambda v = \\mathbf{0}$$\n",
    "\n",
    "or equivalently<br>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A \\cdot v - \\lambda I_n \\cdot v = \\mathbf{0}\\\\\n",
    "\\\\\n",
    "(A - \\lambda I_n) \\cdot v = \\mathbf{0}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "As the vector $(A - \\lambda I_n) \\cdot v$ is a linear combination of the columns of $A - \\lambda I_n$ with not all zero coefficients, the above equation tells that the columns of the matrix $A - \\lambda I_n$ are linearly dependent. Using the characterisation of linear dependency with the help of determinants, this is equivalent to the fact that\n",
    "\n",
    "$$\\det(A-\\lambda I_n) = 0$$\n",
    "\n",
    "For our setting the expression on the left hand side will be a polynomial of order $n$ in $\\lambda$, which is called the characteristic polynomial of $A$. The roots of the characteristic polynomial will be our eigenvalues.\n",
    "\n",
    "After determining an eigenvalue $\\lambda^*$, we can calculate the corresponding eigenvector(s), by solving the equation\n",
    "\n",
    "$$(A - \\lambda^* I_n) \\cdot v = \\mathbf{0}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7PqYThmDwLV"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "As an exercise let us calculate the eigenvectors and eigenvalues of the matrix describing the 2-fold stretching along the $y$-axis.\n",
    "\n",
    "The matrix of this transformation is $A = \\left( \\begin{array}{cc}\n",
    "1 & 0\\\\\n",
    "0 & 2\n",
    "\\end{array}\n",
    "\\right)$.\n",
    "\n",
    "Due to the above remark any eigenvalue $\\lambda$ should solve the equation\n",
    "\n",
    "$$\\det(A-\\lambda I_2) = \\mathbf{0}$$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\det\\left(\n",
    "\\begin{array}{cc}\n",
    "1-\\lambda & 0\\\\\n",
    "0 & 2-\\lambda\n",
    "\\end{array}\n",
    "\\right) = 0\\\\\n",
    "(1- \\lambda) \\cdot (2-\\lambda) = 0\n",
    "\\end{align*}$$\n",
    "\n",
    "This latter implies that the two eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = 2$.\n",
    "\n",
    "Any eigenvector $v = \\left(\\begin{array}{c}\n",
    "v_1\\\\\n",
    "v_2\n",
    "\\end{array}\\right)$ corresponding to the eigenvalue $\\lambda_1 = 1$ satisfies the equation\n",
    "\n",
    "$$(A-1 I_2)v = \\mathbf{0}$$\n",
    "\n",
    "that is\n",
    "\n",
    "$$\\left(\n",
    "\\begin{array}{cc}\n",
    "0 & 0\\\\\n",
    "0 & 1\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\left(\\begin{array}{c}\n",
    "v_1\\\\\n",
    "v_2\n",
    "\\end{array}\\right)\n",
    "= \\mathbf{0}$$\n",
    "\n",
    "This reduces to $0 \\cdot v_1 + 1 \\cdot v_2 = 0 \\Leftrightarrow v_2 = 0$. We can conclude that the eigenvectors corresponding to the eigenvalue $\\lambda_1 =1 $ have the form $\\left(\\begin{array}{c}\n",
    "t\\\\\n",
    "0\n",
    "\\end{array}\\right)$, where $t$ can be any real number.\n",
    "\n",
    "Similarly, any eigenvector $v$ corresponding to the eigenvalue $\\lambda_2 = 2$ satisfies the equation\n",
    "\n",
    "$$\\begin{align*}\n",
    "(A-2I_2)v = \\mathbf{0}\\\\\n",
    "\\\\\n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "-1 & 0\\\\\n",
    "0 & 0\n",
    "\\end{array}\n",
    "\\right) \\left(\\begin{array}{c}\n",
    "v_1\\\\\n",
    "v_2\n",
    "\\end{array}\\right) = \\mathbf{0}\\\\\n",
    "\\\\\n",
    "-v_1 = 0\n",
    "\\end{align*}$$\n",
    "\n",
    "Summing up, the eigenvectors corresponding to the eigenvalue $\\lambda_2 = 2$ are of the form $\\left(\\begin{array}{c}\n",
    "0\\\\\n",
    "t\n",
    "\\end{array}\\right)$, where $t$ can be any real number.\n",
    "\n",
    "**Remark**<br>\n",
    "Observe that the two eigenvectors are the two coordinate axes and this is corresponding to what you could observe in the geometrical visualisation of the eigenvectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfnMzNHWDwLV"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Calculate with the above presented method the eigenvalues and eigenvectors of the matrices describing the\n",
    "    \n",
    "1. rotation in $\\mathbb{R}^2$\n",
    "2. scaling along 2 axes\n",
    "3. identity transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-wbfPSJDwLV"
   },
   "source": [
    "**Definition**\n",
    "\n",
    "A square matrix $A \\in \\mathbb{R}^n$ is called **diagonalisable** if there exists an invertible matrix $P \\in \\mathbb{R}^n$ such that $P^{-1} \\cdot A \\cdot P$ is a diagonal matrix (i.e. a matrix having zero values everywhere except on its diagonal).\n",
    "\n",
    "Or equivalently, consider the linear transformation defined as $T_A: v \\in \\mathbb{R}^n \\mapsto A \\cdot v$ w.r.t. the canonical basis of both the domain and image space. $A$ is diagonalisable if there exists a basis of $\\mathbb{R}^n$ w.r.t. which the transformation can be described by a diagonal matrix.\n",
    "\n",
    "**Remark**\n",
    "\n",
    "1. One can prove that a matrix $A \\in \\mathbb{R}^n$ is diagonalisable if and only if $\\mathbb{R}^n$ has a basis formed of the eigenvectors of $A$. Then we can take for the matrix $P$ the matrix containing on its columns the eigenvectors forming a basis.\n",
    "\n",
    "2. Not all matrices are diagonalisable, but even for the ones without a diagonal form there still exists the Jordan form.\n",
    "\n",
    "3. Let us consider the following setting: The starting position of a tracked point of interest is $\\left(\\begin{array}{c}\n",
    "x_0\\\\\n",
    "y_0\n",
    "\\end{array}\n",
    "\\right)$. In every next time point its new position can determined by the formula\n",
    "$$\\left(\\begin{array}{c}\n",
    "x_{t+1}\\\\\n",
    "y_{t+1}\n",
    "\\end{array}\n",
    "\\right) = A \\cdot \\left(\\begin{array}{c}\n",
    "x_t\\\\\n",
    "y_t\n",
    "\\end{array}\n",
    "\\right),$$\n",
    "where $A \\in \\mathbb{R}^2$. Then applying iteratively this recursive formula\n",
    "$$\\left(\\begin{array}{c}\n",
    "x_{1}\\\\\n",
    "y_{1}\n",
    "\\end{array}\n",
    "\\right) = A \\cdot \\left(\\begin{array}{c}\n",
    "x_0\\\\\n",
    "y_0\n",
    "\\end{array}\n",
    "\\right), \\quad \\left(\\begin{array}{c}\n",
    "x_{2}\\\\\n",
    "y_{2}\n",
    "\\end{array}\n",
    "\\right) = A \\cdot \\left(\\begin{array}{c}\n",
    "x_1\\\\\n",
    "y_1\n",
    "\\end{array}\n",
    "\\right) = A \\cdot A \\cdot \\left(\\begin{array}{c}\n",
    "x_0\\\\\n",
    "y_0\n",
    "\\end{array}\n",
    "\\right) = A^2 \\cdot \\left(\\begin{array}{c}\n",
    "x_0\\\\\n",
    "y_0\n",
    "\\end{array}\n",
    "\\right), \\quad \\left(\\begin{array}{c}\n",
    "x_{3}\\\\\n",
    "y_{3}\n",
    "\\end{array}\n",
    "\\right) = A \\cdot \\left(\\begin{array}{c}\n",
    "x_2\\\\\n",
    "y_2\n",
    "\\end{array}\n",
    "\\right) = A \\cdot A^2 \\cdot \\left(\\begin{array}{c}\n",
    "x_0\\\\\n",
    "y_0\n",
    "\\end{array}\n",
    "\\right) = A^3 \\cdot \\left(\\begin{array}{c}\n",
    "x_0\\\\\n",
    "y_0\n",
    "\\end{array}\n",
    "\\right),$$\n",
    "and in general\n",
    "$$\\left(\\begin{array}{c}\n",
    "x_{t}\\\\\n",
    "y_{t}\n",
    "\\end{array}\n",
    "\\right) = A^t \\cdot \\left(\\begin{array}{c}\n",
    "x_0\\\\\n",
    "y_0\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "Calculating the powers of a matrix is easier if the matrix is diagonal. If we know that $A$ is diagonalisable, then we can do the following trick to calculate $A^t$:\n",
    "* Transform $A$ to a diagonal form: $P^{-1} \\cdot A \\cdot P = D$\n",
    "* Calculate $D^t$: <br></br>\n",
    "$D^2 = P^{-1} \\cdot A \\cdot P \\cdot P^{-1} \\cdot A \\cdot P = P^{-1} \\cdot A \\cdot A \\cdot P = P^{-1} \\cdot A^2 \\cdot P,$<br></br>\n",
    "$D^3 = D^2 \\cdot D = P^{-1} \\cdot A^2 \\cdot P \\cdot P^{-1} \\cdot A \\cdot P = P^{-1} A^3 \\cdot P$,<br></br>\n",
    "in general $D^t = P^{-1} \\cdot A^t \\cdot P$.\n",
    "* Express $A^t$ from the previous equation as $P \\cdot D^t \\cdot P^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYpWVqu7DwLV"
   },
   "source": [
    "### 2.3.8. The power method to calculate the largest eigenvalue and the corresponding eigenvector\n",
    "\n",
    "Consider an $n \\times n$ matrix $A$ with $n$ linearly independent eigenvectors $v^1, v^2, \\ldots, v^n$ of unit length. Assume that the corresponding eigenvalues can be ordered decreasingly in the following way\n",
    "\n",
    "$$|\\lambda_1| > |\\lambda_2| \\geq |\\lambda_3| \\geq \\cdots \\geq |\\lambda_n|.$$\n",
    "\n",
    "Let $x^{(0)}$ be a vector of $\\mathbb{R}^{n \\times 1}$ which written w.r.t. the basis $\\{v^1, v^2, \\ldots, v^n\\}$ has the components $c_1, c_2, \\ldots c_n$, where $c_1$ is not $0$.\n",
    "\n",
    "Let us consider the sequence $x^{(k)} = A^k x^{(0)}$ for $k \\in \\mathbb{N}$.\n",
    "\n",
    "The sequence of the normalised vectors $\\frac{x^{(k)}}{||x^{(k)}||}$ converges to $v^1$, i.e.\n",
    "\n",
    "$$\\lim\\limits_{k \\to \\infty} \\frac{x^{(k)}}{||x^{(k)}||} = v^1$$\n",
    "\n",
    "Furthermore,\n",
    "\n",
    "$$\\lim\\limits_{k \\to \\infty} \\frac{x_j^{(k+1)}}{x_j^{(k)}} = \\lambda_1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTQEX_-KDwLV"
   },
   "source": [
    "### 2.3.9. Applications of the eigenvalues and eigenvectors\n",
    "\n",
    "PageRank algorithm - The eigenvector of the probability transition matrix of the websites corresponding to the largest eigenvalue will be strictly positive and its components can be interpreted as a ranking of the sites.\n",
    "\n",
    "Football ranking - The eigenvector of the interaction matrix belonging to the largest eigenvalue is the only one having just positive components. This can be used as a ranking for the teams.\n",
    "\n",
    "Epidemic - The largest eigenvalue of the adjacency matrix describing the population is inverse proportional to the transmissibility rate of the epidemic. To stop the pandemic the goal is to cut down edges in the adjacency matrix (which is equivavent to not meeting certain people) in such a way to achieve the largest possible reduction in the value of the largest eigenvalue.\n",
    "\n",
    "Spectral clustering - To split the nodes of a graph in two categories, the Fiedler eigenvector (the eigenvector corresponding to the smallest eigenvalue) of the Laplacian matrix should be calculated. The sign of the components of the Fiedler vector can be used to define the two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvKx6xHGDwLV"
   },
   "source": [
    "## 2.4 Homework\n",
    "Read through the linear algebra part of this course and summarise the relevant concepts (and / or formulas) on a cheat sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZLxk7LVDwLW"
   },
   "source": [
    "<[ Introduction ](Introduction.ipynb)|[ Calculus ](Calculus.ipynb)>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
