{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/KingaS03/Introduction-to-Python-2020-June/master)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KingaS03/Introduction-to-Python-2020-June)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculus\n",
    "\n",
    "Agenda\n",
    "- differentiation of univariate functions\n",
    "- rules of differentiation\n",
    "- differentiation of multivariate functions (the Jacobian, the Hessian)\n",
    "- chain rule for univariate and multivariate functions \n",
    "- the Taylor approximation\n",
    "- the Newton-Raphson method\n",
    "- gradient descent method\n",
    "- backpropagation\n",
    "\n",
    "\n",
    "## 3.1. Motivation\n",
    "Find the optimal value of the model parameters of a neuronal network.\n",
    "\n",
    "## 3.2. Functions\n",
    "A function $f:A \\to B$ associates to each element of the set $A$ an element of the set $B$.\n",
    "\n",
    "For our future context $A = \\mathbb{R}^n$ and $B = \\mathbb{R}^m$ for some natural numbers $n$ and $m$.\n",
    "\n",
    "### 3.2.1. Differentiation of a function (univariate case)\n",
    "For a function $f:\\mathbb{R} \\to \\mathbb{R}$ we would like characterise its local linear behavior. Therefore we take two points $x$ and $x+\\Delta x$ and their corresponding values $f(x)$ and $f(x+\\Delta x)$. We are going to connect these points by a line and we will calculate the gradient of this line\n",
    "\n",
    "$$m = \\frac{\\Delta f}{\\Delta x} = \\frac{f(x+\\Delta x)-f(x)}{(x+\\Delta x) - x} = \\frac{f(x+\\Delta x)-f(x)}{\\Delta x}$$\n",
    "\n",
    "Now we are going to take smaller and smaller values for the increment $\\Delta x$. We define the derivative of $f$ in point $x$ as the value of the above quotient when $\\Delta x$ is getting infinitesimally small.\n",
    "\n",
    "The mathematically exact formula for the derivative is\n",
    "\n",
    "$$f'(x) = \\lim\\limits_{\\Delta x \\to 0} \\frac{f(x+\\Delta x)-f(x)}{\\Delta x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1100\"\n",
       "            height=\"900\"\n",
       "            src=\"https://www.geogebra.org/classic/enyhcvgw\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10f9ec940>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://www.geogebra.org/classic/enyhcvgw\", 1100, 900)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Differentiation rules \n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"Images/DifferentiationRules.png\" width=\"500\"> \n",
    "</center>\n",
    "\n",
    "### 3.2.3. Differentiation of a function (multivariate case)\n",
    "When the function $f:\\mathbb{R}^n \\to \\mathbb{R}$ depends on more variables $x_1, x_2, \\ldots, x_n$ and it is nice enough, we can calculate its partial derivatives w.r.t. each variable. The partial derivative of the function $f$ in a point $x^* =(x_1^*, x_2*, \\ldots, x_n^*)$ w.r.t. the variable $x_1$ can be calculated by fixing the values of the other parameters to be equal to $x_2^*, \\ldots, x_n^*$ and derivating the so resulting function by its only parameter $x_1$.\n",
    "\n",
    "To describe the formula in a mathematical exact way let us consider the function $g: \\mathbb{R} \\to \\mathbb{R}$ defined by the formula \n",
    "\n",
    "$$g(x_1) = f(x_1, x_2^*, \\ldots, x_n^*)$$\n",
    "\n",
    "Then the partial derivative of $f$ w.r.t. $x_1$ is denoted by $\\frac{\\partial f}{\\partial x_1}$ and is equal to the derivative of $g$ in the point $x_1*$, that is\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_1}(x_1^*,, x_2^*, \\ldots, x_n^*) = g'(x_1)$$\n",
    "\n",
    "Alternatively we can use for this partial derivative also other notations like the shorter \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_1}(x^*) \\quad \\mbox{or} \\quad \\partial_{x_1} f(x*)$$\n",
    "\n",
    "When it clear that we are performing our calculations in the point $x^*$ and there is no source for confusion, we can omit $x^*$ also and work just with \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_1} \\quad \\mbox{or} \\quad \\partial_{x_1} f$$\n",
    "\n",
    "We can proceed similarly in the case of the other variables to calculate all partial derivatives\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_2}(x^*), \\quad \\frac{\\partial f}{\\partial x_3}(x^*), \\quad \\ldots \\quad, \\frac{\\partial f}{\\partial x_n}(x^*)$$\n",
    "\n",
    "The row vector of all partial derivatives is called the **gradient** of the function or the **Jacobian** of it, that is\n",
    "$$ \\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots \\frac{\\partial f}{\\partial x_n}\\right)$$\n",
    "\n",
    "The gradient or Jacobian of the function $f$ has the following two properties, which are crutial for our forthcoming applications:\n",
    "- in a fixed point $x* =(x_1^*, x_2*, \\ldots, x_n*)$ the gradient/ Jacobian $\\nabla f$ points up the hill along the steepest direction\n",
    "- its length is proportional to the steepness.\n",
    "\n",
    "**Further generalisation**<br>\n",
    "For a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ having also a multivariate output, we can take each output and calculate its partial derivatives w.r.t. each input variable $x_1, x_2, \\ldots, x_n$. For the first output we will have $n$ partial derivatives,i.e.\n",
    "\n",
    "$$\\frac{\\partial f_1}{\\partial x_1}(x^*), \\quad \\frac{\\partial f_1}{\\partial x_1}(x^*), \\quad \\ldots \\quad, \\frac{\\partial f_1}{\\partial x_n}(x^*)$$\n",
    "\n",
    "And for each output the same will happen. We will organise these partial derivatives in a matrix in such a way that in the $i$th row the derivatives of $f_i$ will be enlisted, and at the intersection of $i$th row and $j$th column the derivative \n",
    "\n",
    "$$\\frac{\\partial f_i}{\\partial x_j}$$\n",
    "\n",
    "will be stored.\n",
    "\n",
    "This way we obtain the matrix\n",
    "\n",
    "$$\\nabla f = \\left(\n",
    "\\begin{array}{cccc}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "This matrix is called the Jacobian of the function $f$. \n",
    "\n",
    "### 3.2.4. Higher order differentials (uni- and multivariate case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a function $f: \\mathbb{R}\\to \\mathbb{R}$ we can calculate its derivative in each point, this means that the derivative $f'$ of the function is again a function mapping each point $x \\in \\mathbb{R}$ to the derivative $f'(x)$.\n",
    "\n",
    "Now we could differentiate again each the first order derivative $f'$ and as such we get to the second order derivative, i.e.\n",
    "\n",
    "$$f''(x) = \\lim\\limits_{\\Delta x \\to 0}\\frac{f'(x+\\Delta x) - f'(x)}{\\Delta x}$$\n",
    "\n",
    "The second order derivative can be again derivated and this way we obtain the $3$rd order derivative of a function.\n",
    "\n",
    "-------------------\n",
    "**Multivariate case**\n",
    "\n",
    "We extend the notion of second order derivative to a function $f: \\mathbb{R}^n \\to \\mathbb{R}$.\n",
    "\n",
    "Consider as starting point the Jacobian of the function (which corresponds to the derivative from the univariate case). Let us calculate all partial derivatives of the first order partial derivatives from \n",
    "\n",
    "$$\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots \\frac{\\partial f}{\\partial x_n}\\right),$$\n",
    "\n",
    "and organize them in the following way in a matrix\n",
    "\n",
    "$$\\nabla^2 f = \\left(\n",
    "\\begin{array}{cccc}\n",
    "\\frac{\\partial^2 f}{\\partial x_1\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1\\partial x_n} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_n\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n\\partial x_n} \n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "then the resulting matrix is called the **Hessian matrix**.\n",
    "\n",
    "The value of the Hessian matrix can be used \n",
    "\n",
    "- to derive better local approximation for a function than the linear one,\n",
    "- to find out whether a critical point is a minimum or maximim point or saddle point (exacly as the second order derivative helps us determine whether a critical point is an extreme point of the function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Applications of the differentials\n",
    "\n",
    "### 3.3.1. The Taylor series approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"800\"\n",
       "            src=\"https://www.geogebra.org/classic/kc2umqak\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10fc7e390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/kc2umqak\", 1000, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "**Taylor polynomial of order $n$**\n",
    "\n",
    "The Taylor polynomial of order $n$ of a smooth enough function $f: \\mathbb{R} \\to \\mathbb{R}$ around the point $x = x_0$ is given by the following formula\n",
    "\n",
    "$$p(x) = \\frac{f(x_0)}{0!} + \\frac{f'(x_0)}{1!}(x-x_0) + \\frac{f''(x_0)}{2!} + \\cdot + \\frac{f^{(n)}(x_0)}{n!} (x-x_0)^n$$\n",
    "\n",
    "Where $0! = 0$ by convention.\n",
    "\n",
    "If the function is nice enough, then the approximation error:\n",
    "\n",
    "$f(x) - p(x)$ is of magnitude $(x-x_0)^n$.\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**<br>\n",
    "The above polinomial has the property that the function value and the first $n$ derivatives of the original function $f$ and the polynomial $p$ are exactly the same in the point $x=x_0$. This polynomial is uniquely defined.\n",
    "\n",
    "--------------------\n",
    "**The Taylor approximation of a multivariate function**\n",
    "For a function $f: \\mathbb{R}^n \\to \\mathbb{R}$ the Taylor approximation of order 1 is\n",
    "\n",
    "$$l(x) = \\frac{f(x_0)}{0!} + \\frac{\\nabla f(x_0)}{1!}\\cdot (x-x_0),$$\n",
    "\n",
    "where $\\nabla f(x_0)$ denotes the Jacobian of the function in point $x_0$ and this row vector is multiplied by the column vector $x-x_0$ in the above formula.\n",
    "\n",
    "For a function $f: \\mathbb{R}^n \\to \\mathbb{R}$ the Taylor approximation of order 2 is\n",
    "\n",
    "$$q(x) = \\frac{f(x_0)}{0!} + \\frac{1}{1!}\\nabla f(x_0)\\cdot (x-x_0) + \\frac{1}{2} (x-x_0)^T \\cdot \\nabla^2 f(x_0) \\cdot (x-x_0),$$\n",
    "\n",
    "where $\\nabla^2 f(x_0)$ denotes the Hessian of the function in point $x_0$ and this matrix  is multiplied from left by the row vector $(x-x_0)^T$ nd from the right by the column vector $x-x_0$ in the above formula.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. The Newton-Raphson method \n",
    "\n",
    "The Newton-Raphson method is used to find the approximate a root of a function.\n",
    "\n",
    "Observe how does it work and identify the steps of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.geogebra.org/classic/mm9xvyxr\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10fd609b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/mm9xvyxr\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Raphson method is an iterative method. \n",
    "\n",
    "We cosider a function $f:\\mathbb{R} \\to \\mathbb{R}$\n",
    "    \n",
    "The purpose of this method is to approximate roots of the function, i.e. such $x$ values for which $f(x) = 0$.\n",
    "\n",
    "Let us assume that we know the value of the function in a point $x_0$, i.e we know $f(x_0)$. We approximate the behaviour of the function by the tangent line\n",
    "\n",
    "$$f(x) \\simeq l(x) = f(x_0) + f'(x_0)\\cdot (x-x_0)$$\n",
    "\n",
    "and we solve the equation \n",
    "\n",
    "$$l(x) = 0$$\n",
    "\n",
    "The solution of this will be denoted by $x_1$ and by solving the above linear equation we obtain that\n",
    "\n",
    "$$x_1 = x_0 -  \\frac{f(x_0)}{f'(x_0)}$$\n",
    "\n",
    "$x_1$ is our second approximation for a root of $f$. \n",
    "\n",
    "If we continue the process now by constructing the tangent line in $x_1$ and defining the next point as an intersection of the tangent with the $x$-axis, then \n",
    "\n",
    "$$x_2 = x_1 - \\frac{f(x_0)}{f'(x_0)}$$\n",
    "\n",
    "will be our third approximation for the root.\n",
    "\n",
    "If the function is nice enough, then this method converges to a root of the function.\n",
    "\n",
    "<!--The method can generalised for functions of type $f: \\mathbb{R}^n \\to \\mathbb{R}$. In this setting we start by choosing an $x_0$ and the next value of the sequence that we construct can be determined as\n",
    "\n",
    "$$x_{n+1} = x_{n} - \\lambda \\nabla f(x_n) .$$-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. Gradient descent method\n",
    " \n",
    "The gradient descent method is similar to the Newton-Raphson one in the sense that we perform an iterative step in the steepest direction. The difference is that the goal of this process is to minimise a cost function $C: \\mathbb{R} \\to \\mathbb{R}$ (or $C: \\mathbb{R} \\to \\mathbb{R}$ in the multivariate case). We update the gradient in every iterative step and we move along the steepest gradient downwards, i.e.\n",
    "\n",
    "$$x_{n+1} = x_{n} - \\lambda \\nabla f(x_n).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.geogebra.org/classic/xfa7y3wc\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10fd80860>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.geogebra.org/classic/xfa7y3wc\", 800, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3.4. Backpropagation\n",
    "\n",
    "See the blackboard\n",
    "\n",
    "\n",
    "<!--\n",
    "Speed vs time, tangent -> acceleration\n",
    "Is the speed as function the derivative of some other function? Related to the integral, antiderivative. Distance vs time.\n",
    "\n",
    "Geometrical def. of the derivative: \"rise over run\" gradient - for two points on the graph of the function\n",
    "\n",
    "the gradient of the tangent line\n",
    "\n",
    "Formal definition of the derivative with $\\Delta x$, $f$ and $\\lim$.\n",
    "\n",
    "Ex. Calculate the derivative of a linear function.\n",
    "Ex. Calculate the derivative of a parabolic function / polynom of grade 2.\n",
    "\n",
    "Sum rule, power rule.\n",
    "\n",
    "Special functions and their derivatives: 1/x, e^x (the only function being equal to its derivative), \n",
    "\n",
    "Product rule. Quotient rule can be derived also from the product rule and whenever the quotient rule should be used, one can use equivalently the product rule as well.\n",
    "\n",
    "## Chain law / rule\n",
    "\n",
    "## Multivariate differentiation\n",
    "dependent and independent variables, how do we select them? speed can be depicted as the function of time, but not the other way around.\n",
    "\n",
    "partial differentiation - fix all the variables except one constant and calc. the derivative w.r.t. the remaining variable\n",
    "\n",
    "Chain rule for the multivariate setup.\n",
    "\n",
    "## The Jacobian \n",
    "The Jacobian vector of a mutivariate function. - the vector pointing to the steepest slope. Contour lines with Jacobian directions.\n",
    "\n",
    "The Jacobian for vector valued functions.\n",
    "\n",
    "## Looking for extremal values of a function. \n",
    "Context: y = f(x), z = f(x,y)\n",
    "\n",
    "Sandpit. Find the deepest point by Jacobians, by the depth of the pit.\n",
    "\n",
    "## The Hessian - the Jacobian of the Jacobian\n",
    "Hessian - shows whether the found stationary point is a min or max point.\n",
    "\n",
    "## In reality we don't know the function\n",
    "We estimate also the Jacobians. What should be the stepsize? We calculate the approx. of the Jac. for different step sizes and we average out.-->\n",
    "\n",
    "<!--## Total derivative\n",
    "When our function depends on n variables and the variables depend on the same parameter t. \n",
    "$$\\frac{{\\rm d}f}{{\\rm d}t} = \\frac{\\partial f}{\\partial x} \\cdot \\frac{{\\rm d} x}{{\\rm d} t} = J_f \\frac{{\\rm d} x}{{\\rm d} t}$$-->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
