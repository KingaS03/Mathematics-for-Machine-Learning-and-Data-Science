{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/KingaS03/Mathematics-for-Machine-Learning-and-Data-Science/master)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KingaS03/Mathematics-for-Machine-Learning-and-Data-Science)\n",
    "\n",
    "<[ Calculus ](Calculus.ipynb)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Calculus & Statistics\n",
    "\n",
    "Agenda:\n",
    "* Mean, variance, covariance for one-dimensionsal data sets\n",
    "* Covariance matrix for higher dimensional data sets\n",
    "* Inner products\n",
    "* Projections\n",
    "* PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Basic concepts of descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------\n",
    "### 4.1.1 Mean, variance, standard deviation\n",
    "Instead of working directly with the data many times we want to characterise it by its mean, variance and correlation between the observed features.\n",
    "\n",
    "For a set of data points $X = \\{x_1, x_2, \\ldots, x_n\\}$ the mean or expected value of this data set is:\n",
    "$$E(X) = \\frac{x_1 + x_2 + \\cdots + x_n}{n} \\quad \\mbox{or shorter} \\quad E(X) = \\frac{1}{n}\\sum\\limits_{i=1}^n x_i.$$\n",
    "\n",
    "<!---Calculate the mean value of the next two data points: ..... . \n",
    "\n",
    "If you have calculated correctly, they have the same mean value. To differentiate among them, we can calculate their variance, which is a measure of average square distance from the mean value.-->\n",
    "\n",
    "The average square distance from the mean is\n",
    "$$\\frac{(x_1 - \\mu)^2 + (x_2-\\mu)^2 + \\cdots + (x_n - \\mu)^2}{n} = \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i - \\mu)^2, \\quad \\mbox{where} \\quad \\mu = E(X).$$\n",
    "\n",
    "This value above is denoted by ${\\rm Var}(X)$ and it is called the variance of the data points $X = \\{x_1, x_2, \\ldots, x_n\\}$. A common notation for it is also $\\sigma^2$.\n",
    "\n",
    "<!---Calculate the variance for the above two data points.\n",
    "\n",
    "One can conclude that the spread of ..... data points is larger than the spread of the ....... data points.-->\n",
    "\n",
    "If the data is measured in units, then observe that the measurement unit for the variance is square units. \n",
    "\n",
    "The standard deviation is obtained from the variance by calculating the square root of it, i.e.\n",
    "$$\\sigma = \\sqrt{{\\rm Var}(X)}.$$\n",
    "\n",
    "The above formula explains also the $\\sigma^2$ notation in case of the variance.\n",
    "\n",
    "**Remarks:** \n",
    "* If the data $X = \\{x_1, x_2, \\ldots, x_n\\}$ is centered (i.e. $E(X) = \\mu = 0$), then the variance can be calculated as\n",
    "\n",
    "$${\\rm Var}(X) = \\frac{x_1^2 + x_2^2 + \\cdots + x_n^2}{n}$$\n",
    "\n",
    "or if $x = \\left(\\begin{array}{c}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right)$ is the column vector containing all the observations, then $${\\rm Var}(X) = \\frac{1}{n}\\left(\\begin{array}{cccc}\n",
    "x_1 &\n",
    "x_2 & \n",
    "\\ldots &\n",
    "x_n\n",
    "\\end{array}\\right) \\cdot \n",
    "\\left(\\begin{array}{c}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right)  = \\frac{1}{n}x^T \\cdot x.$$\n",
    "\n",
    "* Any data $X = \\{x_1, x_2, \\ldots, x_n\\}$ can be centered by extracting the mean of the data from each element, i.e. the data $X' = \\{x'_1 = x_1 - \\mu,x'_2 = x_2 - \\mu, \\ldots, x'_n = x_n - \\mu\\}$ is centered. \n",
    "* For not centered data organised into a column vector $x = \\left(\\begin{array}{c}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right)$ the variance can be calculated in the following way by matrix operations: \n",
    "\n",
    "$${\\rm Var}(X) = \\left(\\begin{array}{cccc}\n",
    "x_1-\\mu &\n",
    "x_2-\\mu & \n",
    "\\ldots &\n",
    "x_n-\\mu\n",
    "\\end{array}\\right) \\cdot \n",
    "\\left(\\begin{array}{c}\n",
    "x_1-\\mu\\\\ \n",
    "x_2-\\mu\\\\ \n",
    "\\vdots\\\\\n",
    "x_n-\\mu\n",
    "\\end{array}\\right) = \\frac{1}{n}(x-\\mu)^T \\cdot (x-\\mu), \\quad \\mbox{where} \\quad \\mu = E(X).$$\n",
    "\n",
    "-------------\n",
    "\n",
    "### 4.1.2. Higher dimensional variance, covariance\n",
    "The generalisation of the variance to higher dimensional datasets.\n",
    "\n",
    "If we observe more features at the same time (i.e. we deal with higher dimensional data), then we can calculate the variance along each dimension.\n",
    "\n",
    "<!---Consider the following two datasets and calculate for both of them the mean and variance along $X$ and the variance along $Y$.\n",
    "\n",
    "If you have calculated correctly, then the two datasets have the same variance and same mean values along both features. However, if me plot them, we can see that they are different. To capture a difference we can calculate their covariance, as well. -->\n",
    "\n",
    "--------\n",
    "**Definition**\n",
    "\n",
    "The covariance of two observed features $X$ and $Y$ based on the observed value pairs $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$ is calulated by the following formula:\n",
    "\n",
    "$${\\rm Cov}(X,Y) = E\\left((X - \\mu_x)(Y-\\mu_y)\\right) = \\frac{(x_1-\\mu_x)(y_1-\\mu_y) + (x_2-\\mu_x)(y_2-\\mu_y) + \\cdots + (x_n-\\mu_x)(y_n-\\mu_y)}{n} = \\frac{1}{n} \\sum\\limits_{i=1}^n (x_i-\\mu_x)(y_i-\\mu_y),$$\n",
    "\n",
    "where $\\mu_x = E(X)$ and $\\mu_y = E(Y)$.\n",
    "\n",
    "--------------\n",
    "\n",
    "**Remark**\n",
    "\n",
    "1. The covariance of $X$ and $Y$ with matrix operations is\n",
    "\n",
    "$${\\rm Cov}(X,Y) = \\frac{1}{n}(X-\\mu_x)^T \\cdot (Y - \\mu_y), \\quad \\mbox{where} \\quad x = \\left(\\begin{array}{c}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right), y = \\left(\\begin{array}{c}\n",
    "y_1\\\\ \n",
    "y_2\\\\ \n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{array}\\right), \\mu_x = E(X) \\mbox{ and }\\mu_y = E(Y)$$\n",
    "\n",
    "\n",
    "2. What is the relationship between ${\\rm Cov}(X,Y)$ and ${\\rm Cov}(Y,X)$? \n",
    "$${\\rm Cov}(X,Y)\\ ???\\ {\\rm Cov}(Y,X)$$\n",
    "\n",
    "\n",
    "--------------\n",
    "\n",
    "**Interpretation of the covariance**\n",
    "* If ${\\rm Cov}(X,Y) > 0$, then the values of $Y$ increase on average when the values of $X$ increase.\n",
    "* If ${\\rm Cov}(X,Y) < 0$, then the values of $Y$ decrease on average when the values of $X$ increase.\n",
    "* If ${\\rm Cov}(X,Y) = 0$, then there is no observable linear trend between $X$ and $Y$.\n",
    "\n",
    "-----------\n",
    "\n",
    "**Definition**\n",
    "\n",
    "For a two-dimensional data $D = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}$ we can calculate the following 4 values: ${\\rm Var}(X),{\\rm Var}(Y),{\\rm Cov}(X,Y)$ and ${\\rm Cov}(Y,X)$. The covariance matrix of this two-dimensional data is:\n",
    "\n",
    "$${\\rm Var}(D) = \\left(\\begin{array}{cc}\n",
    "{\\rm Var}(X) & {\\rm Cov}(X,Y)\\\\\n",
    "{\\rm Cov(Y,X)} & {\\rm Var(Y)}\n",
    "\\end{array}\\right).$$\n",
    "\n",
    "---------------\n",
    "**Flash questions**\n",
    "\n",
    "<p>\n",
    "1. How does the covariance matrix of a three-dimensional data $D = \\{(x_1, y_1, z_1), (x_2, y_2, z_2), \\ldots, (x_n, y_n, z_n)\\}$ look like?\n",
    "\n",
    "<!--\n",
    "It is the following $3 x 3$ matrix: ${\\rm Var}(D) = \\left(\n",
    "\\begin{array}{ccc}\n",
    "{\\rm Var}(X) & {\\rm Cov(X,Y)} & {\\rm Cov(X,Y)}\\\\\n",
    "{\\rm Cov}(Y,X) & {\\rm Var(Y)} & {\\rm Cov(Y,Z)}\\\\\n",
    "{\\rm Cov}(Z,X) & {\\rm Cov(Z,Y)} & {\\rm Var(Z)}\n",
    "\\end{array}\\right)$\n",
    "</p>\n",
    "-->\n",
    "  \n",
    "<p>  \n",
    "2. How can one calculate the covariance matrix with the help of matrix operations?\n",
    "\n",
    "<!--\n",
    "Due to the fact that the variance and covariance can be calculated as \n",
    "<p type='vertical-padding:0.5cm'>\n",
    "    $${\\rm Var}(X) = (X-\\mu_x)^T \\cdot (X-\\mu_x) = \\langle X-\\mu_x, X-\\mu_x \\rangle, \\qquad {\\rm Cov}(X,Y) = (X-\\mu_x)^T \\cdot (Y-\\mu_y) = \\langle X-\\mu_x, Y-\\mu_y \\rangle,$$\n",
    "</p>\n",
    "    \n",
    "the variance of the 3 dimensional dataset $D$ from the previous exercise can be expressed as\n",
    "<p type='vertical-padding:0.5cm'>\n",
    "$${\\rm Var}(D) = \\left(\n",
    "\\begin{array}{ccc}\n",
    "{\\rm Var}(X) & {\\rm Cov(X,Y)} & {\\rm Cov(X,Y)}\\\\\n",
    "{\\rm Cov}(Y,X) & {\\rm Var(Y)} & {\\rm Cov(Y,Z)}\\\\\n",
    "{\\rm Cov}(Z,X) & {\\rm Cov(Z,Y)} & {\\rm Var(Z)}\n",
    "\\end{array}\\right) = \n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "\\langle X-\\mu_x, X-\\mu_x \\rangle & \\langle X-\\mu_x, Y-\\mu_y \\rangle & \\langle X-\\mu_x, Z-\\mu_z \\rangle\\\\\n",
    "\\langle Y-\\mu_y, X-\\mu_x \\rangle & \\langle Y-\\mu_y, Y-\\mu_y \\rangle & \\langle Y-\\mu_y, Z-\\mu_z \\rangle\\\\\n",
    "\\langle Z-\\mu_z, X-\\mu_x \\rangle & \\langle Z-\\mu_z, Y-\\mu_y \\rangle & \\langle Z-\\mu_z, Z-\\mu_z \\rangle\n",
    "\\end{array}\\right),$$\n",
    "</p>\n",
    "\n",
    "which by the definition of the matrix product is equal to \n",
    "<p type='vertical-padding:0.5cm'>\n",
    "$$\\left(\n",
    "\\begin{array}{ccc}\n",
    "\\leftarrow & X-\\mu_x & \\rightarrow\\\\\n",
    "\\leftarrow & Y-\\mu_y & \\rightarrow\\\\\n",
    "\\leftarrow & Z-\\mu_x & \\rightarrow\\\\\n",
    "\\end{array}\\right) \\cdot \\left(\\begin{array}{ccc}\n",
    "\\uparrow & \\uparrow & \\uparrow\\\\\n",
    "X-\\mu_x & Y-\\mu_y & Z-\\mu_x\\\\\n",
    "\\downarrow & \\downarrow & \\downarrow\\\\\n",
    "\\end{array}\\right)$$\n",
    "<p>\n",
    "-->\n",
    "<p>\n",
    "\n",
    "------------\n",
    "**Remark**\n",
    "\n",
    "The covariance is measured in ${\\rm unit}_1 \\cdot {\\rm unit}_2$ if the measurement unit of the values in data set $X$ \n",
    "is ${\\rm unit}_1$ and the measurement unit of the values in data set $Y$ is ${\\rm unit}_2$. Depending on our choice of measuremeant units, the covariance is going to be larger or smaller. For example if both sets of our measurements are expressed in $m$ and the covariance of them is $25$, then changing our measurement unit to $cm$, would yield a covariance of $25*100^2 = 250.000$. Both covariances are characterising the same dataset. How to interpret them? How to compare two covariances if they come from different data sets?\n",
    "\n",
    "The correlation is an answer to all these questions:\n",
    "\n",
    "-------------\n",
    "\n",
    "**Definition**\n",
    "\n",
    "The correlation of two observed features $X$ and $Y$ based on the observed value pairs $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$ is calulated by the following formula:\n",
    "\n",
    "$${\\rm Corr}(X,Y) = \\frac{{\\rm Cov}(X,Y)}{\\sigma_X \\cdot \\sigma_Y}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "### 4.1.3. The effect of linear transformations on the mean and of the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As start, please take a short quiz by running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone \"https://github.com/KingaS03/Mathematics-for-Machine-Learning-and-Data-Science.git\"\n",
    "cd \"Mathematics-for-Machine-Learning-and-Data-Science\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"questions/questions1.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we generalise what we have observed?\n",
    "\n",
    "\\begin{align*}\n",
    "E(X + c) = c+ E(X) \\\\[0.5em]\n",
    "E(cX) = c E(X)\\\\[0.5em]\n",
    "E(aX + b) = aE(X)+b\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the test in the below field!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"questions/questions2.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we say about the variance of a one-dimensional data set $X$ in general?\n",
    "\n",
    "\\begin{align*}\n",
    "{\\rm Var}(X + c) = {\\rm Var}(X)\\\\[0.5em]\n",
    "{\\rm Var}(cX) = c^2{\\rm Var}(X)\\\\[0.5em]\n",
    "{\\rm Var}(aX + b) = a^2{\\rm Var}(X)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the covariance matrix?\n",
    "\n",
    "\\begin{align*}\n",
    "{\\rm Var}(AD + c) = A \\cdot {\\rm Var}(D) \\cdot A^T,\n",
    "\\end{align*}\n",
    "\n",
    "where $D = \\{(x_1, x_2, \\ldots, x_d)^T\\}$, $A \\in \\mathbb{R}^{dxd}$ and $c \\in \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "## 4.2 Descriptive statistics and linear algebra\n",
    "\n",
    "### 4.2.1 The inner product\n",
    "\n",
    "The inner product is a generalisation of the dot product or Euclidean scalar product, that we have introduced in Chapter 2. Once we have an inner product, we are able to calculate lengths (distances) and angles.\n",
    "\n",
    "-----------\n",
    "**Definition**\n",
    "\n",
    "On a vector space $V$ the inner product is a **symmetric, positive definite, bilinear** mapping $\\langle \\cdot, \\cdot \\rangle: V \\times V \\mapsto \\mathbb{R}$.\n",
    "\n",
    "* symmetry holds if $\\langle x, y\\rangle = \\langle y, x \\rangle$ for all $x, y \\in V$\n",
    "* positive definiteness holds if $\\langle x, x \\rangle > 0$ for all $x \\in V$, where $x \\neq \\mathbf{0}$\n",
    "* bilinearity holds if \n",
    "\\begin{align*}\n",
    "\\langle c_1 \\cdot x^{(1)} + c_2 \\cdot x^{(2)}, y \\rangle = c_1 \\langle x^{(1)}, y \\rangle + c_2 \\langle x^{(2)}, y \\rangle, \\quad \\mbox{for all} \\quad c_1, c_2 \\in \\mathbb{R}, x^{(1)}, x^{(2)}, y \\in V\\\\\n",
    "\\langle x, c_1 \\cdot y^{(1)} + c_2 \\cdot y^{(2)} \\rangle = c_1 \\langle x, y^{(1)} \\rangle + c_2 \\langle x, y^{(2)} \\rangle \\quad \\mbox{for all} \\quad c_1, c_2 \\in \\mathbb{R}, x, y^{(1)}, y^{(2)} \\in V\n",
    "\\end{align*}\n",
    "\n",
    "-------------\n",
    "**Definition**\n",
    "A matrix $A \\in \\mathbb{R}^{n \\times n}$ is positive definite if $x^T \\cdot A \\cdot x >0$ for all $x \\in \\mathbb{R}$.\n",
    "\n",
    "---------------\n",
    "**Sylvester's criterion**\n",
    "A symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is positive definite iff all the leading principal minors are positive, i.e. if all the following matrices have positive determinants:\n",
    "* the upper left $1\\times 1$ corner of $A$,\n",
    "* the upper left $2\\times 2$ corner of $A$,\n",
    "* the upper left $3\\times 3$ corner of $A$,\n",
    "* ...\n",
    "* $A$ itself.\n",
    "\n",
    "-----------\n",
    "**Remarks**\n",
    "\n",
    "1. For our context it suffices to consider for $V$ the $n$-dimensional real vector space $\\mathbb{R}^n$. \n",
    "2. On $V = \\mathbb{R}^n$ for every inner product $\\langle \\cdot, \\cdot \\rangle$ defined on $V \\times V$ there exists a symmetric, positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$ such that \n",
    "$$\\langle x, y \\rangle = x^T \\cdot A \\cdot y, \\quad \\mbox{for every} \\quad x,y \\in \\mathbb{R}^n.$$\n",
    "3. Observe that the dot product introduced in Chapter 2 satisfies the general definition of an inner product, therefore the inner product can be seen as a generalisation of the dot product.\n",
    "\n",
    "The dot product can be written as\n",
    "$$\\langle x, y \\rangle = x^T \\cdot I_n \\cdot y,$$\n",
    "where $I_n$ is the $n \\times n$-dimensional identity matrix.\n",
    "\n",
    "--------------\n",
    "**Definitions**\n",
    "\n",
    "1. We say that two vectors $x, y \\in \\mathbb{R}^n$ are orthogonal to each other w.r.t. the considered inner product, if their inner product $\\langle x, y \\rangle$ is $0$. \n",
    "2. In a right triangle the side opposite to the right angle is called hypotenuse.\n",
    "3. The cosine of an angle in a right triangle is the ratio of the leg next to the angle and of the hypotenuse. Furthermore, $\\cos(\\alpha) = - \\cos(\\pi-\\alpha)$.\n",
    "4. For a vector $x \\in \\mathbb{R}^n$ we introduce the $||x||^2$ notation for the value $\\langle x, x\\rangle$ and we call the value $||x|| = \\sqrt{\\langle x, y \\rangle}$ the length of the  vector $x$.\n",
    "5. For two vectors $x, y \\in \\mathbb{R}^n$ we introduce the notation $d(x,y)$ for the value $||x-y||$. The properties of the inner product assure that the so defined function $d: \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}_{\\geq 0}$ is a distance (or with other words metric) in the mathematical sense, i.e. the following properties are satisfied by it for all $x,y, z \\in \\mathbb{R}^n$:\n",
    "\n",
    "    \\begin{align*}\n",
    "    & (1)\\ \\mbox{identity of indiscernibles:} \\quad d(x,y) = 0 \\ \\Leftrightarrow \\ x = y\\\\[0.5em]\n",
    "    & (2)\\ \\mbox{symmetry:} \\quad d(x,y) = d(y,x)\\\\[0.5em]\n",
    "    & (3)\\ \\mbox{triangle inequality:} \\quad d(x,y) \\leq d(x,z) + d(z,y)\n",
    "    \\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"https://www.geogebra.org/classic/qhxyf5cj\", 900, 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Remarks**\n",
    "\n",
    "\n",
    "1. The above defined inner product can also be interpreted as the inner product of functions defined on a finite set of points. The notion of inner product can be extended also to continuous functions. For example for two continuous functions $f: [a,b] \\to \\mathbb{R}$ and $g: [a,b] \\to \\mathbb{R}$ their inner product can be defined as:\n",
    "\n",
    "$$\\langle f, g \\rangle = \\int_{a}^{b} f(x)g(x)\\, dx.$$\n",
    "\n",
    "In this space of functions an integrable function $f$ defined on the interval $[-\\pi,\\pi]$ can be approximated by its projection to a subspace spanned by the orthonormal functions $1, \\cos(x), \\cos(2x), \\ldots, \\cos(nx), \\sin(x), \\sin(2x), \\ldots, \\sin(nx)$. \n",
    "\n",
    "The projection of $f$ to ${\\rm span}\\{1, \\cos(x), \\cos(2x), \\ldots, \\cos(nx), \\sin(x), \\sin(2x), \\ldots, \\sin(nx)\\}$ is the $n$-th Fourier series.\n",
    "\n",
    "2. With the above definition of orthogonality, in a right triangle the Pythagorean theorem and the more general law of cosines also hold, i.e.\n",
    "\\begin{align*}\n",
    "    &||x-y||^2 = ||x||^2 + ||y||^2, \\quad \\mbox{if} \\quad \\langle x,y \\rangle = 0\\\\[0.5em]\n",
    "    &||x-y||^2 = ||x||^2 + ||y||^2 - 2\\cdot\\cos(\\theta_{x,y})\\cdot||x||\\cdot||y||,\n",
    "\\end{align*}\n",
    "where $\\theta_{x,y}$ denotes the angle between the vectors $x$ and $y$. \n",
    "\n",
    "As we did in the linear algebra part, we can conclude, that\n",
    "\n",
    "$$\\cos(\\theta_{x,y}) = \\frac{\\langle x, y\\rangle}{||x|| \\cdot ||y||}$$\n",
    "\n",
    "3. Observe that if you think of a 1-dimensional centered data set $X = \\{x_1, x_2, \\ldots, x_n\\}$ as a column vector \n",
    "\n",
    "$$x = \\left(\n",
    "\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\n",
    "\\right)\\in \\mathbb{R},$$ \n",
    "\n",
    "then \n",
    "\n",
    "$${\\rm Var}(X) = \\frac{1}{n}x^T \\cdot x = \\frac{1}{n}\\langle x, x \\rangle = \\frac{1}{n}||x||^2 \\quad \\mbox{and} \\quad \\sigma_X = \\sqrt{{\\rm Var}(X)} = ||x||$$\n",
    "\n",
    "\n",
    "For a 2-dimensional centered data set $\\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}$ it holds that \n",
    "\n",
    "\\begin{align*}\n",
    "&{\\rm Cov}(X,Y) = x^T \\cdot y = \\langle x, y \\rangle\\\\[0.5em]\n",
    "&{\\rm Corr}(X,Y) = \\frac{{\\rm Cov}(X,Y)}{\\sigma_X \\cdot \\sigma_Y} = \\frac{\\langle x, y \\rangle}{||x|| \\cdot ||y||} = {\\rm cos}(X,Y)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Applications \n",
    "\n",
    "------------------------\n",
    "\n",
    "We load the images of digits from the well-known MNIST data set and we create a function to calculate distances between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.2-rc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "#from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = train_images.astype(np.double)\n",
    "labels = train_labels.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the following code do? Modify it in such a way to plot out a 7 instead of a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[labels==1].reshape(-1, 28, 28)[0], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and plot the average digit of $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.average(images[labels==1].reshape(-1, 28, 28),0), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider our images as vectors and calculate their distance and angle, by using the Euclidean metric on the space of image vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x0, x1):\n",
    "    \"\"\"Compute distance between two vectors x0, x1 using the dot product\"\"\"\n",
    "    distance = np.sum((x1-x0)*(x1-x0))**0.5 # <-- EDIT THIS to compute the distance between x0 and x1\n",
    "    return distance\n",
    "\n",
    "def angle(x0, x1):\n",
    "    \"\"\"Compute the angle between two vectors x0, x1 using the dot product\"\"\"\n",
    "    angle = np.arccos(np.sum(x0*x1)/(distance(x0,0)*distance(x1,0))) # <-- EDIT THIS to compute angle between x0 and x1\n",
    "    return angle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an image of a digit. Plot it. Find the one, which is closest to it among the next 1000 images. Plot this second image, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch cell\n",
    "mindist = 100000\n",
    "first = 2\n",
    "second2remember = 0\n",
    "for second in range(1000):\n",
    "    f = images[first].reshape(28, 28)\n",
    "    s = images[second].reshape(28, 28)\n",
    "    d = distance(f.ravel(), s.ravel())\n",
    "    if 0 < d < mindist:\n",
    "        mindist = d\n",
    "        second2remember = second\n",
    "print(second2remember)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0].reshape(28, 28), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[832].reshape(28, 28), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Projections\n",
    "\n",
    "----------------\n",
    "\n",
    "### 4.4.1 Projection to a multi-dimensional space\n",
    "\n",
    "**Definition**\n",
    "In the real vector space $\\mathbb{R}^n$ the projection of a vector $x \\in\\mathbb{R}^n$ to a subspace with basis $B = \\{b_1, b_2, \\ldots, b_m\\} \\subset \\mathbb{R}^n$ is defined to be that element $\\pi_B(x) \\in {\\rm span}(B)$, for which $||x - \\pi_B(x)||$ is minimal. \n",
    "\n",
    "--------\n",
    "**Remark**\n",
    "By definition the projection of $x \\in \\mathbb{R}^n$ to $B = \\{b_1, b_2, \\ldots, b_m\\} \\subset \\mathbb{R}^n$ is the best approximation of the vector that can be spanned by $B$.\n",
    "\n",
    "----------------\n",
    "\n",
    "**Claim**\n",
    "For the above defined projection it holds that \n",
    "$$ x - \\pi_B(x)\\ \\bot\\ {\\rm span}(B)$$\n",
    "\n",
    "*Proof*\n",
    "\n",
    "Let $y \\in {\\rm span}(B)$ such that $x - y \\ \\bot\\ {\\rm span}(B)$. First we show that under these assumptions $y$ is the best approximation of the vector $x$, that can be spanned by $B$. \n",
    "\n",
    "For any $b \\in {\\rm span}(B)$ it holds, that\n",
    "\n",
    "\\begin{align*}\n",
    "    ||x - b||^2 &= ||(x - y) + (y - b)||^2\\\\[0.5em]\n",
    "    & = ||x-y||^2 + ||y-b||^2 + 2 \\underbrace{\\langle x-y, y-b\\rangle}_{ 0, \\mbox{ as } y-b\\ \\in\\ {\\rm span}(B)}\\\\[0.5em]\n",
    "    &\\geq ||x-y||^2\n",
    "\\end{align*}\n",
    "\n",
    "To complete the proof, we will show that the conditions\n",
    "\\begin{align*}\n",
    "    &(1) \\quad  y \\in {\\rm span}(B)\\\\\n",
    "    &(2) \\quad x-y \\ \\bot \\ {\\rm span}(B)\n",
    "\\end{align*}\n",
    "do uniquely determine the vector $y$.\n",
    "\n",
    "Condition (1) means that there exist the coefficients $\\lambda_1, \\lambda_2, \\ldots, \\lambda_m$ such that \n",
    "\n",
    "$$y = \\lambda_1 b_1 + \\lambda_2 b_2 +  \\cdots + \\lambda_m b_m$$\n",
    "\n",
    "Condition (2) means that the vector $x- y$ is perpendicular to every $b \\in {\\rm span}(B)$, so particularly also to the elements $b_i$ of the basis $B$, i.e.\n",
    "\n",
    "$$\\langle x-y, b_i \\rangle = 0, \\quad \\forall i \\in \\{1,2, \\ldots, m\\}.$$\n",
    "\n",
    "Let us substitute in the above formula $y = \\lambda_1 b_1 + \\lambda_2 b_2 +  \\cdots + \\lambda_m b_m$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle b_i, x - (\\lambda_1 b_1 + \\lambda_2 b_2 +  \\cdots + \\lambda_m b_m) \\rangle= 0\\\\[0.5em]\n",
    "\\langle b_i, x - B\\cdot \\lambda \\rangle = 0, \\quad \\forall i \\in \\{1,2, \\ldots, m\\},\n",
    "\\end{align*}\n",
    "\n",
    "where by $B$ we denote the matrix containing on its columns the basis vectors $b_1, b_2, \\ldots b_m$, i.e. $B = [b_1|b_2|\\ldots|b_m]$ and $\\lambda = \\left(\n",
    "\\begin{array}{c}\n",
    "\\lambda_1\\\\\n",
    "\\lambda_2\\\\\n",
    "\\ldots\\\\\n",
    "\\lambda_m\n",
    "\\end{array}\n",
    "\\right)$.\n",
    "\n",
    "Thus the last equation can be rewritten equivalently as\n",
    "\n",
    "\\begin{align*}\n",
    "b_i^T \\cdot (x - B \\cdot \\lambda) = 0, \\quad \\forall i \\in \\{1,2, \\ldots, m\\}\n",
    "\\end{align*}\n",
    "\n",
    "These $m$ equations can be merged together and written in the following form\n",
    "\n",
    "\\begin{align*}\n",
    "&B^T \\cdot (x - B\\cdot \\lambda) = \\mathbf{0}\\\\[0.5em]\n",
    "&B^T \\cdot x - B^T \\cdot B \\cdot \\lambda = \\mathbf{0}\\\\[0.5em]\n",
    "&B^T \\cdot B \\cdot \\lambda = B^T \\cdot x\\\\[0.5em]\n",
    "&\\lambda = \\left(B^T\\cdot B\\right)^{-1} B^T \\cdot x \\quad \\Rightarrow \\quad y = B \\cdot \\underbrace{\\left(B^T\\cdot B\\right)^{-1} B^T \\cdot x}_{\\lambda}\n",
    "\\end{align*}\n",
    "\n",
    "-------------------------\n",
    "\n",
    "**Remark**\n",
    "\n",
    "<p>\n",
    "1. In the proof of the above claim we have also derived the projection formula\n",
    "\n",
    "$$\\pi_{B}(x) = B \\cdot \\left(B^T\\cdot B\\right)^{-1} B^T \\cdot x.$$\n",
    "\n",
    "Observe that this is an extension of the formula that we have obtained in Chapter 2 for projecting on a single line $b$:\n",
    "\n",
    "$$\\pi_{b}(x) = \\frac{b \\cdot b^T}{||b||^2} \\cdot x.$$\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "2. In the special case when $b_1, b_2, \\ldots, b_m$ form an orthonomal set of vectors, then the $B^T \\cdot B$ matrix appearing in the projection formula will be the unit matrix $I_m$. Indeed, the element on the position $(i,j)$ of the product matrix $B^T \\cdot B$ can be calculated as $\\langle b_i, b_j \\rangle = b_i^T \\cdot b_j$, which is $1$ for $i = j$ and is $0$ otherwise, due to orthonormality of the set of vectors $\\{b_1, b_2, \\ldots, b_m\\}$.\n",
    "\n",
    "Thus in such a case $\\pi_B(x) = B \\cdot \\underbrace{B^T \\cdot x}_{\\lambda}$.\n",
    "\n",
    "When particularly $B = \\{b\\}$ and $||b|| = 1$, then $\\pi_B(x) = \\pi_b(x)= b \\cdot \\underbrace{b^T \\cdot x}_{\\lambda}$.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "3. Observe furthermore that when $\\{b_1, b_2, \\ldots, b_m\\}$ is an orthonomal set of vectors, then \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{\\pi_B(x)} &= B \\cdot \\underbrace{B^T \\cdot x}_{\\lambda} \\\\\n",
    "&= \\left(\n",
    "\\begin{array}{cccc}\n",
    "\\uparrow & \\uparrow & \\ldots & \\uparrow\\\\\n",
    "b_1 & b_2 & \\ldots & b_m\\\\\n",
    "\\downarrow & \\downarrow & \\ldots & \\downarrow\n",
    "\\end{array}\\right) \\cdot \\left( \\begin{array}{c}\n",
    "b_1^T \\cdot x\\\\\n",
    "b_2^T \\cdot x\\\\\n",
    "\\vdots\\\\\n",
    "b_m^T \\cdot x\n",
    "\\end{array}\\right)\\\\[0.5em] \n",
    "&= \\mathbf{b_1 \\cdot b_1^T \\cdot x + b_2 \\cdot b_2^T \\cdot x + \\cdots + b_m \\cdot b_m^T \\cdot x}\\\\[0.5em]\n",
    "&= \\mathbf{\\pi_{b_1}(x) + \\pi_{b_2}(x) + \\cdots +\\pi_{b_m}(x)}\n",
    "\\end{align*}\n",
    "\n",
    "In particular if $m = n$, then $x \\in {\\rm span}B$, i.e. $\\mathbf{\\pi_B(x)} = x$.\n",
    "<!--To show this, let us denote the components of $x$ w.r.t. the basis $B$ by $\\beta_i$ ($i \\in \\{1, 2, \\ldots, n\\}$), i.e.\n",
    "\n",
    "$$x = \\beta_1 \\cdot b_1 + \\beta_2 \\cdot b_2 +  \\cdots + \\beta_n \\cdot b_n.$$\n",
    "\n",
    "Multiplying the above equality from the left by $b_1^T$ yields\n",
    "\n",
    "$$b_1^T \\cdot x = \\beta_1 \\cdot \\underbrace{b_1^T \\cdot b_1}_{ = 1} + \\beta_2 \\cdot \\underbrace{b_1^T \\cdot b_2}_{ = 0} + \\cdots + \\beta_n \\cdot \\underbrace{b_1^T \\cdot b_n}_{= 0}$$ \n",
    "\n",
    "Thus $\\beta_1 = b_1^T \\cdot x$ and $\\beta_1 \\cdot b_1 = \\underbrace{b_1^T \\cdot x}_{\\in \\mathbb{R}} \\cdot b_1 = b_1 \\cdot b_1^T \\cdot x = \\pi_{b_1}(x).$-->\n",
    "\n",
    "<!--One can prove similarly for all $i \\in \\{1, 2, \\ldots, n\\}$ that \n",
    "\n",
    "$$\\beta_i = b_i^T \\cdot x \\quad \\mbox{and} \\quad \\beta_i \\cdot b_i = b_i \\cdot b_i^T \\cdot x = \\pi_{b_i}(x).$$\n",
    "\n",
    "Thus $x = b_1 \\cdot b_1^T \\cdot x + b_2 \\cdot b_2^T \\cdot x + \\cdots + b_n \\cdot b_n^T \\cdot x = \\pi_B(x)$.\n",
    "</p>-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 PCA\n",
    "\n",
    "**Step 1**\n",
    "\n",
    "Let us assume that we have chosen a sample of $n$ individuals, and for each of them we have recorded $d$ features. The features describing the $i$th individual we store in the vector denoted by $x_i \\in \\mathbb{R}^d$.\n",
    "\n",
    "For any orthonormal basis $\\{b_1, b_2, \\ldots, b_d\\} \\subset \\mathbb{R}^d$ of $\\mathbb{R}^d$ any element $x$ of $\\mathbb{R}^d$ can be written as:\n",
    "\n",
    "$$x = \\underbrace{b_1 \\cdot b_1^T \\cdot x + b_2 \\cdot b_2^T \\cdot x + \\cdots + b_m \\cdot b_m^T \\cdot x}_{\\pi_{B'}(x)} + b_{m+1} \\cdot b_{m+1}^T \\cdot x + \\cdots + b_d \\cdot b_d^T \\cdot x$$\n",
    "\n",
    "Our goal is to express the $x_1, x_2, \\ldots, x_n$ vectors in a lower dimensional space, therefore from the above exact representation of $x$ we want to keep just the sum of the first $m$ terms, which is exactly $\\pi_{B'}(x)$, where $B'$ denotes the smaller set of orthonormal vectors $\\{b_1, b_2, \\ldots, b_m\\}$.\n",
    "\n",
    "We will measure by the mean squared reconstruction error $f(b_1, b_2, \\ldots, b_m)$, namely how well can our data points $x_1, x_2,\\ldots, x_n$ be represented in the subspace spanned by $B' = \\{b_1, b_2, \\ldots, b_m\\}$: \n",
    "\n",
    "\\begin{align*}\n",
    "f(b_1, b_2, \\ldots, b_d) &= \\frac{1}{n} \\sum_{i=1}^n ||x_i - \\pi_{\\{b_1, \\ldots, b_m\\}}(x_i)||^2\\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^n ||\\pi_{\\{b_{m+1}, \\ldots, b_d\\}}(x_i)||^2\\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n || b_{m+1} \\cdot b_{m+1}^T \\cdot x_i + \\cdots + b_d \\cdot b_d^T \\cdot x_i||^2\\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n || b_{m+1} \\cdot (b_{m+1}^T \\cdot x_i) + \\cdots + b_d \\cdot (b_d^T \\cdot x_i)||^2\\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n (b_{m+1}^T \\cdot x_i)^2 + \\cdots + (b_{d}^T \\cdot x_i)^2\n",
    "\\end{align*}\n",
    "\n",
    "$f(b_1, b_2, \\ldots, b_d)$ is our function subject to minimisation. But before proceeding to determine the optimal \n",
    "orthonormal vectors $\\{b_1, b_2, \\ldots, b_d\\}$ we give a statistical interpretation to the above sum.\n",
    "\n",
    "<!---\n",
    "**Step 2**\n",
    "\n",
    "If we think of an $x \\in \\mathbb{R}^d$ as a vector represented w.r.t. $\\{b_1, b_2, \\ldots, b_d\\}$, i.e. $x = b_1 \\cdot (b_1^T \\cdot x) + b_2 \\cdot (b_2^T \\cdot x) + \\cdots b_d \\cdot (b_d^T \\cdot x)$ and we denote by $X$ the set of data points with values $b_1^T \\cdot x, b_2^T \\cdot x, \\ldots, b_d^T \\cdot x$, then\n",
    "\n",
    "$${\\rm Var}(X) = \\frac{1}{d}\\left((b_{1}^T \\cdot x)^2 + (b_{2}^T \\cdot x)^2 \\cdots + (b_{d}^T \\cdot x)^2\\right).$$\n",
    "\n",
    "If $X_i$ denotes the associated dataset to $x_i$, then\n",
    "$${\\rm Var}(X_i) = \\frac{1}{d}\\left((b_{1}^T \\cdot x_i)^2 + (b_{2}^T \\cdot x_i)^2 \\cdots + (b_{d}^T \\cdot x_i)^2\\right).$$\n",
    "\n",
    "$\\pi_{\\{b_{m+1}, \\ldots, b_d\\}}(x_i) \\in {\\rm span}\\{b_1, b_2, \\ldots, b_m\\} \\subset {\\rm span}\\{b_1, b_2, \\ldots, b_d\\}$. \n",
    "\n",
    "$$\\pi_{\\{b_{m+1}, \\ldots, b_d\\}}(x_i) = 0 \\cdot (b_1^T \\cdot x_i) + 0 \\cdot (b_2^T \\cdot x_i) + \\cdots + 0 \\cdot (b_m^T \\cdot x_i) + b_{m+1} \\cdot (b_{m+1}^T \\cdot x_i) + \\cdots + b_d \\cdot (b_d^T \\cdot x_i).$$ \n",
    "\n",
    "Analogously to how we proceeded before we can associate to the above projection also the set of data points $\\underbrace{0, 0, \\ldots, 0}_{m \\mbox{ times}}, b_{m+1}^T \\cdot x_i, \\ldots, b_{d}^T \\cdot x_i$, which we denote by $\\Pi_{\\{b_{m+1}, \\ldots, b_d\\}}(x_i)$. Then\n",
    "\n",
    "$${\\rm Var}\\left(\\Pi_{\\{b_{m+1}, \\ldots, b_d\\}}(x_i)\\right) = \\frac{1}{d}\\left((b_{m+1}^T \\cdot x_i)^2 + \\cdots + (b_{d}^T \\cdot x_i)^2\\right).$$\n",
    "\n",
    "By all the above reasoning $f(b_1, b_2, \\ldots, b_m) = \\frac{1}{n} \\sum_{i=1}^n (b_{m+1}^T \\cdot x_i)^2 + \\cdots + (b_{d}^T \\cdot x_i)^2$ is the average variance of the part of $x$, which is encoded in ${\\rm span}\\{b_{m+1}, \\ldots, b_d\\}$.-->\n",
    "\n",
    "**Step 2**\n",
    "\n",
    "We rewrite $f(b_1, b_2, \\ldots, b_m)$ as follows\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "f(b_1, b_2, \\ldots b_m) &=& \\frac{1}{n} \\sum_{i=1}^n \\sum_{k=m+1}^d (b_k^T \\cdot x_i)^2\\\\\n",
    "&=& \\frac{1}{n} \\sum_{i=1}^n \\sum_{k=m+1}^d b_k^T \\cdot x_i \\cdot x_i^T \\cdot b_k\\\\\n",
    "&=& \\frac{1}{n} \\sum_{k=m+1}^d b_k^T\\left(\\frac{1}{n} \\sum_{i=1}^n x_i \\cdot x_i^T \\right) b_k\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Observe that $\\displaystyle\\frac{1}{n}\\sum\\limits_{i=1}^n x_i \\cdot x_i^T = \\frac{1}{n} X \\cdot X^T = {\\rm var(X)}$, where $X$ is the data matrix, the columns of which are the observations $x_1, x_2, \\ldots, x_n$. \n",
    "\n",
    "Therefore, $\\displaystyle S = \\frac{1}{n} \\sum\\limits_{i=1}^n x_i \\cdot x_i^T$ is the covariance of the $d$ features and $\\displaystyle f(b_1, b_2, \\ldots b_d) = \\sum_{k=m+1}^d b_k^T \\cdot S \\cdot b_k = {\\rm Trace}\\left(\\sum_{k=m+1}^d (b_k \\cdot b_k^T) \\cdot S\\right)$ is the total variance of the projected data to the complement of the principal space. Our goal is to find such an orthonormal basis $\\{b_1, b_2, \\ldots, b_d\\}$ for which the average reconstruction error $f(b_1, b_2, \\ldots, b_d)$ is minimal.\n",
    "\n",
    "**Step 3**\n",
    "\n",
    "We will minimize $\\displaystyle f(b_1, b_2, \\ldots b_d) = \\sum\\limits_{k=m+1}^d b_k^T \\cdot S \\cdot b_k$ under the constraints that $b_{k}^T \\cdot b_k = 1$ for all $k \\in \\{m+1, m+2, \\ldots, d\\}$.\n",
    "\n",
    "We construct the Lagrange function:\n",
    "\n",
    "$$L = \\sum_{k=m+1}^d b_k^T \\cdot S \\cdot b_k - \\sum_{k=m+1}^d \\lambda_k (b_{k}^T \\cdot b_k-1),$$\n",
    "\n",
    "where $\\lambda_k \\in \\mathbb{R}$ for all $k \\in \\{m+1, \\ldots, d\\}$.\n",
    "\n",
    "We calculate the gradients of $f$ w.r.t. $b_{m+1}, \\ldots, b_d$ and we obtain:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial b_{m+1}} = 2 b_{m+1}^T\\cdot S - 2\\lambda_{m+1}b_{m+1}^T \\quad \\ldots \\quad \\frac{\\partial f}{\\partial b_{d}} = 2 b_{d}^T\\cdot S - 2\\lambda_{d}b_{d}^T.$$\n",
    "\n",
    "Setting all gradients to be $0$ and transposing the above equalities yields, that \n",
    "\n",
    "$$S \\cdot b_{m+1} = \\lambda_{m+1}b_{m+1} \\quad \\ldots \\quad S \\cdot b_{d} = \\lambda_{d}b_{d}.$$\n",
    "\n",
    "Therefore $b_{m+1}, \\ldots, b_{d}$ should be chose to be eigenvectors of $S$ corresponding to the eigenvalues $\\lambda_{m+1}, \\ldots, \\lambda_d$, respectively.\n",
    "\n",
    "In this case the value of the average reconstruction error is \n",
    "\n",
    "$$f(b_1, b_2, \\ldots, b_d) = \\sum_{k=m+1}^d b_k^T \\cdot \\underbrace{S \\cdot b_k}_{\\lambda_k b_k} = \\sum_{k=m+1}^d \\lambda_k,$$\n",
    "\n",
    "which will be minimal if we choose for $\\lambda_{m+1}, \\ldots, \\lambda_{d}$ the smallest eigenvalues of $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Logistic regression\n",
    "In case of logistic regression the output of the approximation model can be interpreted as one or several probabilities, by which a probability distribution can be defined.\n",
    "\n",
    "In the simplest case our goal is to approximate the probability of belonging to a predefined category. For example we would like to construct a model that returns the probability that an apartment will be sold in the first month after it is put to the market. If we denote this probability by $p$, then $p$ associated to the success and $1-p$ associated to the failure define a probability distribution (Bernoulli distribution). \n",
    "\n",
    "When we want to approximate the probability of belonging to several predefined categories. For example we would like to construct a model that returns the probabilities that an apartment is sold in the first month, in the second months,..., in the 6th month after it is put to the market, then with the help of these 6 probabilities $p_1, p_2, \\ldots, p_6$ we can define a probability measure $p$ on the set of potential outcomes in the following way:\n",
    "\n",
    "$$\\begin{array}{c}\n",
    "p(S_1) = p_1, p(S_2) = p_2, p(S_3) = p_3, p(S_4) = p_4, p(S_5) = p_5, p(S_6) = p_6, \n",
    "p(S_{>6}) = 1-(p_1+p_2+\\cdots+p_6),\n",
    "\\end{array}$$\n",
    "\n",
    "where $S_i$ denotes the event, that the apartment is sold in month $i$ (for $i \\in \\{1,2,\\ldots, 6\\}$), and $S_{>6}$ denotes the event, that the aptment is sold after month $6$.\n",
    "\n",
    "Thus, when constructing the loss function (measuring the goodness of fit), we can work with such tools, that are able to compare probability distributions with each other or \"measure the proximity\" of two probability distributions. Such a tool is for example the cross-entropy.\n",
    "\n",
    "The logistic function / sigmoid function: $\\sigma(t) = \\frac{e^t}{1+e^t} = \\frac{1}{1+e^{-t}}.$\n",
    "\n",
    "The below code snippet produces the plot of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "y = 1/(1+np.exp(-x))\n",
    "\n",
    "plt.plot(x, y, '-') \n",
    "\n",
    "# legend, title, and labels.\n",
    "plt.title('The sigmoid function', size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can observe that this function can take as argument any real value and outputs a value that is always between $0$ and $1$. Practically this means that if we apply this function to any real value, it is able to transform it into another value, that can be interpreted as a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1. One explanatory variable, two categories \n",
    "**Explanatory variable:** $x$.\n",
    "\n",
    "**Two categories** (actually just one category and the complementary category): probability $p$ of belonging to a specific category needs to be approximated. $1-p$ will be the probability of belonging to the complementary category.\n",
    "\n",
    "We obtain an approximation for $p$ by first applying a linear function on $x$ and turning the output of this function into a probability by applying the sigmoid function to it.\n",
    "\n",
    "$$x \\quad \\to \\quad b+w \\cdot x \\quad \\to \\quad \\sigma(b+ w \\cdot x) = \\underbrace{\\frac{1}{1+e^{-(b+w \\cdot x)}}}_{f(x)} \\simeq p.$$\n",
    "\n",
    "For determining the goodness of fit the cross-entropy can be used.\n",
    "\n",
    "\n",
    "### Case 2. Many explanatory variables, two categories\n",
    "**Many explanatory variables:** $x_1, x_2, \\ldots, x_d$.\n",
    "\n",
    "**Two categories** (actually just one category and the complementary category): probability $p$ of belonging to a specific category needs to be approximated. $1-p$ will be the probability of belonging to the complementary category.\n",
    "\n",
    "We obtain an approximation for $p$ by first applying a linear function of $x_1, x_2,\\ldots,x_d$ and turning the output of this function into a probability by applying the sigmoid function to it.\n",
    "\n",
    "$$x \\quad \\to \\quad b+\\sum_{i=0}^d w_i \\cdot x_i = b + \\mathbf{w}^T \\cdot \\mathbf{x} \\quad \\to \\quad \\sigma(b+ \\mathbf{w}^T \\cdot \\mathbf{x}) = \\underbrace{\\frac{1}{1+e^{-\\mathbf{w}^T \\cdot \\mathbf{x}}}}_{f(x_1, x_2, \\ldots, x_d) = f(\\mathbf{x})} \\simeq p,$$\n",
    "where $\\mathbf{w} = \\left(\\begin{array}{c}\n",
    "b\\\\\n",
    "w_1\\\\\n",
    "w_2\\\\\n",
    "\\vdots\\\\\n",
    "w_d\n",
    "\\end{array}\\right)$ and $\\mathbf{x} = \\left(\\begin{array}{c}\n",
    "1\\\\\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_d\n",
    "\\end{array}\\right)$.\n",
    "\n",
    "For determining the goodness of fit the cross-entropy can be used.\n",
    "\n",
    "### Case 3. Multinomial logistic regression. Many explanatory variables, many categories\n",
    "**Many explanatory variables:** $x_1, x_2, \\ldots, x_d$.\n",
    "\n",
    "**Many categories**: probabilities $p_1, p_2, \\ldots, p_n$ of belonging to category $1, 2, \\ldots, n$ need to be approximated. $1-(p_1+p_2+\\cdots+p_n)$ will be the probability of not belonging to any of the previous categories.\n",
    "\n",
    "To create an approximation for the probaility of belonging to each of the $n$ categories we need $n$ separete linear models that we apply to the original variables $x_1, x_2, \\ldots, x_d$, which in matrix notation can be summarised as:\n",
    "\n",
    "$$\\begin{array}{c}\n",
    "\\mathbf{x} = (x_1, x_2, \\ldots, x_d) \\quad \\to \\quad \\mathbf{w^{(1)}} \\cdot \\mathbf{x}\\\\\n",
    "\\mathbf{x} = (x_1, x_2, \\ldots, x_d) \\quad \\to \\quad \\mathbf{w^{(2)}} \\cdot \\mathbf{x}\\\\\n",
    "\\vdots\\\\\n",
    "\\mathbf{x} = (x_1, x_2, \\ldots, x_d) \\quad \\to \\quad \\mathbf{w^{(n)}} \\cdot \\mathbf{x}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We can combine these together to form $n$ probabilities, which sum up altogether to less than $1$, by the following formulas:\n",
    "\n",
    "$$\\displaystyle \\begin{array}{c}\n",
    "\\displaystyle p_1 \\simeq \\hat{p}_1 = \\frac{e^{\\mathbf{w^{(1)}} \\cdot \\mathbf{x}}}{1+ \\sum_{i=0}^n e^{\\mathbf{w^{(i)}} \\cdot \\mathbf{x}}},\\\\\n",
    "\\displaystyle p_2 \\simeq \\hat{p}_2 = \\frac{e^{\\mathbf{w^{(2)}} \\cdot \\mathbf{x}}}{1+ \\sum_{i=0}^n e^{\\mathbf{w^{(i)}} \\cdot \\mathbf{x}}},\\\\\n",
    "\\vdots\\\\\n",
    "\\displaystyle p_n \\simeq \\hat{p}_n = \\frac{e^{\\mathbf{w^{(n)}} \\cdot \\mathbf{x}}}{1+ \\sum_{i=0}^n e^{\\mathbf{w^{(i)}} \\cdot \\mathbf{x}}}.\n",
    "\\end{array}$$\n",
    "\n",
    "The probability of belonging to neither of the $n$ categories can be approximated by $1 - (\\hat{p}_1 + \\hat{p}_2 + \\cdots + \\hat{p}_d)$.\n",
    "\n",
    "\n",
    "## 4.7. Multivariate linear regression and further regressions derived from it\n",
    "\n",
    "### Multivariate linear regression\n",
    "In case of a multivariate linear regression one has instead of one explanatory variable $d$ variables denoted by $x_1, x_2, \\ldots, x_d$ and one response variable denoted by $y$. The observed $y$ values are approximated by this model through a multivariate linear of the form\n",
    "\n",
    "$$f(x_1,x_2, \\ldots, x_d) = b + w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\cdots + w_d \\cdot x_d = \n",
    "\\underbrace{(\\begin{array}{ccccc}\n",
    "b & w_1 & w_2 & \\ldots & w_d\n",
    "\\end{array})}_{\\mathbf{w}^T} \\cdot \n",
    "\\underbrace{\\left(\\begin{array}{c}\n",
    "1\\\\\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_d\n",
    "\\end{array}\\right)}_{\\mathbf{x}} = \\mathbf{w} \\cdot \\mathbf{x}.$$\n",
    "\n",
    "The error of approximation is measured with MSE, which in this case is\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=0}^n \\left(y-(b+\\mathbf{w}^T \\cdot \\mathbf{x}_i\\right))^2,$$\n",
    "\n",
    "where $\\mathbf{x}_i$ denotes the column vector containing all features of observation $i$ and $\\mathbf{w} = \\left(\\begin{array}{c}\n",
    "b\\\\\n",
    "w_1\\\\\n",
    "w_2\\\\\n",
    "\\vdots\\\\\n",
    "w_d\n",
    "\\end{array}\\right)$.\n",
    "\n",
    "By letting the partial derivatives of $MSE$ w.r.t. all model parameters being equal to 0, i.e. \n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial b} = 0, \\frac{\\partial MSE}{\\partial w_1} = 0, \\frac{\\partial MSE}{\\partial w_2} = 0, \\ldots, \\frac{\\partial MSE}{\\partial w_d} = 0,$$\n",
    "\n",
    "one can obtain an explicit formula for the parameter values that minimise $MSE$.\n",
    "\n",
    "### Polynomial regression\n",
    "For a single explanatory variable $x$ and the response variable $y$ the polynomial regression model of order $d$ is described by the approximation function \n",
    "\n",
    "$$f(x) = b + w_1 \\cdot x + w_2 \\cdot x^2 + \\cdots + w_d \\cdot x^d,$$\n",
    "\n",
    "which with the introduction of notations $x_1 = x, x_2 = x^2, \\ldots, x_d = x^d$ reduces to the above multilinear regression model.\n",
    "\n",
    "In practice we calculate $x_1 = x, x_2 = x^2, \\ldots, x_d = x^d$ and we can use the formula from the multivariate linear regression model to determine the model parameters.\n",
    "\n",
    "### Log-linear regression\n",
    "In case of the log-linear model not the response variable $y$ is depending linearly on the explanatory variables $x_1, x_2, \\ldots, x_d$, but the logarithmus of $y$. Thus if we introduce the new response variable $y' = \\ln(y)$, then this can be approximated by a multilinear regression model of the explanatory variables $x_1, x_2, \\ldots, x_d$\n",
    "\n",
    "$$y' = \\ln(y) \\simeq f(x_1, x_2, \\ldots, x_d) = b + w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\cdots + w_d \\cdot x_d$$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$y \\simeq e^{f(x_1, x_2, \\ldots, x_d)} = e^{b + w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\cdots + w_d \\cdot x_d}.$$\n",
    "\n",
    "In practice we calculate $y' = \\ln(y)$ and we can use the formula from the multivariate linear regression model fitted to the independent variables $x_1, x_2, \\ldots, x_d$ and the dependent variable $y'$ to determine the model parameters.\n",
    "\n",
    "One can further generalise this model, if instead of the explanatory variables $x_1, x_2, \\ldots, x_d$ one places in the approximation formula functions of them, i.e. $f_1(x_1), f_2(x_2), \\ldots, f_d(x_d)$. In this case\n",
    "\n",
    "$$y \\simeq e^{f(x_1, x_2, \\ldots, x_d)} = e^{b + w_1 \\cdot f_1(x_1) + w_2 \\cdot f_2(x_2) + \\cdots + w_d \\cdot f_d(x_d)}.$$\n",
    "\n",
    "## 4.8 Further topics\n",
    "### 4.8.1 Entropy\n",
    "### 4.8.2 Kullback-Leibler divergence\n",
    "### 4.8.3 [Law of total probability](https://www.youtube.com/watch?v=8odFouBR2wE)\n",
    "### 4.8.4 [Bayes' theorem](https://www.youtube.com/watch?v=HZGCoVF3YvM)\n",
    "\n",
    "<!-- Universal approximation theorem, K-means, SVM -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<[ Calculus ](Calculus.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
